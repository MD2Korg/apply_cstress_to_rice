{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cerebralcortex.util.helper_methods import get_study_names\n",
    "# sn = get_study_names(\"/home/jupyter/cc3_conf/\")\n",
    "# print(sn)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, LongType, \\\n",
    "DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "ModuleMetadata\n",
    "import pandas as pd\n",
    "from cerebralcortex import Kernel\n",
    "study_name = 'rice'\n",
    "CC = Kernel(\"/home/jupyter/cc3_conf/\", study_name=study_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/pyspark/sql/pandas/group_ops.py:76: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_respiration_quality(data,\n",
    "                                window_size=3,\n",
    "                                outlier_threshold_high = 4000,\n",
    "                                outlier_threshold_low = 20,\n",
    "                                slope_threshold = 300,\n",
    "                                eck_threshold_band_loose = 175,\n",
    "                                eck_threshold_band_off = 20,\n",
    "                                minimum_expected_samples = 3*(0.33)*21.33,\n",
    "                                acceptable_outlier_percent = 34):\n",
    "    \n",
    "    def get_metadata(stream_name):\n",
    "        stream_metadata = Metadata()\n",
    "        stream_metadata.set_name(stream_name).set_description(\"Chest Respiration quality per sample 3 seconds\") \\\n",
    "            .add_dataDescriptor(\n",
    "            DataDescriptor().set_name(\"quality\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "            \"Respiration data quality\").set_attribute('Loose/Improper Attachment','Electrode Displacement').set_attribute('Sensor off Body',\\\n",
    "            'Autosense not worn').set_attribute('Battery down/Disconnected', \\\n",
    "            'No data is present - Can be due to battery down or sensor disconnection').set_attribute('Interittent Data Loss', \\\n",
    "             'Not enough samples are present').set_attribute('Acceptable','Good Quality')) \\\n",
    "            .add_dataDescriptor(\n",
    "            DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "            \"respiration sample value\")) \\\n",
    "            .add_module(\n",
    "            ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/rip_quality.ipynb\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "                \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "        return stream_metadata\n",
    "    \n",
    "    def get_quality(data):\n",
    "        data_quality_band_loose = 'Loose/Improper Attachment'\n",
    "        data_quality_not_worn = 'Sensor off Body'\n",
    "        data_quality_band_off = 'Battery down/Disconnected'\n",
    "        data_quality_missing = 'Interittent Data Loss' \n",
    "        data_quality_good = 'Acceptable'\n",
    "        if (len(data)== 0):\n",
    "            return data_quality_band_off\n",
    "        range_data = max(data)-min(data)\n",
    "        if range_data<=eck_threshold_band_off:\n",
    "            return data_quality_not_worn\n",
    "        if (len(data)<=minimum_expected_samples) :\n",
    "            return data_quality_missing\n",
    "        if range_data<=eck_threshold_band_loose:\n",
    "            return data_quality_band_loose\n",
    "\n",
    "        outlier_counts = 0 \n",
    "        for i in range(0,len(data)):\n",
    "            im,ip  = i,i\n",
    "            if i==0:\n",
    "                im = len(data)-1\n",
    "            else:\n",
    "                im = i-1\n",
    "            if i == len(data)-1:\n",
    "                ip = 0\n",
    "            else:\n",
    "                ip = ip+1\n",
    "            stuck = ((data[i]==data[im]) and (data[i]==data[ip]))\n",
    "            flip = ((abs(data[i]-data[im])>((int(outlier_threshold_high)))) or (abs(data[i]-data[ip])>((int(outlier_threshold_high)))))\n",
    "            disc = ((abs(data[i]-data[im])>((int(slope_threshold)))) and (abs(data[i]-data[ip])>((int(slope_threshold)))))\n",
    "            if disc:\n",
    "                outlier_counts += 1\n",
    "            elif stuck:\n",
    "                outlier_counts +=1\n",
    "            elif flip:\n",
    "                outlier_counts +=1\n",
    "            elif data[i] >= outlier_threshold_high:\n",
    "                outlier_counts +=1\n",
    "            elif data[i]<= outlier_threshold_low:\n",
    "                outlier_counts +=1\n",
    "        if (100*outlier_counts>acceptable_outlier_percent*len(data)):\n",
    "            return data_quality_band_loose\n",
    "        return data_quality_good\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType()),\n",
    "        StructField(\"localtime\", TimestampType()),\n",
    "        StructField(\"version\", IntegerType()),\n",
    "        StructField(\"user\", StringType()),\n",
    "        StructField(\"quality\", StringType()),\n",
    "        StructField(\"respiration\", DoubleType())\n",
    "    ])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def ecg_quality(key,data):\n",
    "        data['quality'] = ''\n",
    "        if data.shape[0]>0:\n",
    "            data = data.sort_values('timestamp')\n",
    "            data['quality'] = get_quality(list(data['respiration']))\n",
    "        return data\n",
    "\n",
    "    stream_name = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "    rip_quality_stream = rip.compute(ecg_quality,windowDuration=window_size,startTime='0 seconds')\n",
    "    data = rip_quality_stream._data\n",
    "    ds = DataStream(data=data,metadata=get_metadata(stream_name))\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rip_stream = 'respiration--org.md2k.autosense--autosense_chest--chest'\n",
    "rip = CC.get_stream(rip_stream)\n",
    "ds = compute_respiration_quality(rip)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1971347207"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.quality.per.sample').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/pyspark/sql/pandas/group_ops.py:76: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  \"more details.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- localtime: timestamp (nullable = true)\n",
      " |-- quality: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"quality\", StringType())\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def ecg_quality_60(key,data):\n",
    "    if data.shape[0]>0:\n",
    "        quals = list(data['quality'].values)\n",
    "        qual = Counter(quals).most_common()[0][0]\n",
    "        data = data[:1].reset_index(drop=True)\n",
    "        data['quality'] = [qual]\n",
    "        data['start'] = [key[2]['start']]\n",
    "        data['end'] = [key[2]['end']]\n",
    "        return data[['start','end','timestamp','localtime','version','user','quality']]\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['start','end','timestamp','localtime','version','user','quality'])\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.quality.60seconds'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Chest RIP quality 60 seconds\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"quality\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"ECG data quality\").set_attribute('Loose/Improper Attachment','Electrode Displacement').set_attribute('Sensor off Body',\\\n",
    "    'Autosense not worn').set_attribute('Battery down/Disconnected', \\\n",
    "    'No data is present - Can be due to battery down or sensor disconnection').set_attribute('Interittent Data Loss', \\\n",
    "     'Not enough samples are present').set_attribute('Acceptable','Good Quality')) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"window\").set_type(\"struct\").set_attribute(\"description\", \\\n",
    "    \"window start and end time in UTC\").set_attribute('start', \\\n",
    "    'start of 1 minute window').set_attribute('end','end of 1 minute window')) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "\n",
    "stream_metadata.is_valid()\n",
    "ecg_stream = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "ecg = CC.get_stream(ecg_stream)\n",
    "ecg_quality_stream = ecg.compute(ecg_quality_60,windowDuration=60,startTime='0 seconds')\n",
    "ecg_quality_stream = ecg_quality_stream.select('timestamp', F.struct('start', 'end').alias('window'),\n",
    "                                   'localtime','quality','user','version')\n",
    "ecg_quality_stream.printSchema()\n",
    "data = ecg_quality_stream._data\n",
    "ds = DataStream(data=data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1736319"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.quality.60seconds').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+----------+-----------+------------------------------------+-------+\n",
      "|timestamp              |localtime              |quality   |respiration|user                                |version|\n",
      "+-----------------------+-----------------------+----------+-----------+------------------------------------+-------+\n",
      "|2018-05-04 19:25:06.004|2018-05-04 13:25:06.004|Acceptable|2040.0     |0cb209c9-886f-3358-91c6-b7625552b813|1      |\n",
      "|2018-05-04 19:25:06.047|2018-05-04 13:25:06.047|Acceptable|2038.0     |0cb209c9-886f-3358-91c6-b7625552b813|1      |\n",
      "|2018-05-04 19:25:06.093|2018-05-04 13:25:06.093|Acceptable|2039.0     |0cb209c9-886f-3358-91c6-b7625552b813|1      |\n",
      "|2018-05-04 19:25:06.139|2018-05-04 13:25:06.139|Acceptable|2037.0     |0cb209c9-886f-3358-91c6-b7625552b813|1      |\n",
      "|2018-05-04 19:25:06.185|2018-05-04 13:25:06.185|Acceptable|2043.0     |0cb209c9-886f-3358-91c6-b7625552b813|1      |\n",
      "+-----------------------+-----------------------+----------+-----------+------------------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "# TODO: What is this?\n",
    "class Quality(Enum):\n",
    "    ACCEPTABLE = 1\n",
    "    UNACCEPTABLE = 0\n",
    "from operator import itemgetter\n",
    "\n",
    "def smooth(data:np.ndarray,\n",
    "           span: int = 5)-> np.ndarray:\n",
    "    \"\"\"\n",
    "\n",
    "    :rtype: object\n",
    "    :param data:\n",
    "    :param span:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return []\n",
    "    # plt.figure()\n",
    "    # plt.plot(data[:,1])\n",
    "    sample = data[:,1]\n",
    "    sample_middle = np.convolve(sample, np.ones(span, dtype=int), 'valid') / span\n",
    "    divisor = np.arange(1, span - 1, 2)\n",
    "    sample_start = np.cumsum(sample[:span - 1])[::2] / divisor\n",
    "    sample_end = (np.cumsum(sample[:-span:-1])[::2] / divisor)[::-1]\n",
    "    sample_smooth = np.concatenate((sample_start, sample_middle, sample_end))\n",
    "    data[:,1] = sample_smooth\n",
    "\n",
    "    # plt.plot(data[:,1])\n",
    "    # plt.show()\n",
    "    return data\n",
    "\n",
    "\n",
    "def moving_average_curve(data: np.ndarray,\n",
    "                         window_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Moving average curve from filtered (using moving average) samples.\n",
    "\n",
    "    :rtype: object\n",
    "    :return: mac\n",
    "    :param data:\n",
    "    :param window_length:\n",
    "    \"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return []\n",
    "    # plt.figure()\n",
    "    # plt.plot(data[:,1])\n",
    "    sample = data[:,1]\n",
    "    for i in range(window_length, len(sample) - (window_length + 1)):\n",
    "        sample_avg = np.mean(sample[i - window_length:i + window_length + 1])\n",
    "        data[i,1] = sample_avg\n",
    "\n",
    "    # plt.plot(data[:,1])\n",
    "    # plt.show()\n",
    "    return data[np.array(range(window_length, len(sample) - (window_length + 1)))]\n",
    "\n",
    "\n",
    "def up_down_intercepts(data: np.ndarray,\n",
    "                       mac: np.ndarray,\n",
    "                       data_start_time_to_index: dict) -> [np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns Up and Down Intercepts.\n",
    "    Moving Average Centerline curve intersects breath cycle twice. Once in the inhalation branch\n",
    "    (Up intercept) and in the exhalation branch (Down intercept).\n",
    "\n",
    "    :param data_start_time_to_index:\n",
    "    :rtype: object\n",
    "    :return up_intercepts, down_intercepts:\n",
    "    :param data:\n",
    "    :param mac:\n",
    "    \"\"\"\n",
    "\n",
    "    up_intercepts = []\n",
    "    down_intercepts = []\n",
    "\n",
    "    subsets = []\n",
    "    for i in range(len(mac)):\n",
    "        data_index = data_start_time_to_index[mac[i,0]]\n",
    "        subsets.append(data[data_index,1])\n",
    "    # plt.plot(data[:,0],data[:,1])\n",
    "    # plt.plot(mac[:,0],mac[:,1])\n",
    "    # # plt.plot(mac[:,0],subsets)\n",
    "    # plt.show()\n",
    "    if len(subsets) == len(mac):\n",
    "        for i in range(len(mac) - 1):\n",
    "            if subsets[i] <= mac[i,1] and mac[i + 1,1] <= subsets[i + 1]:\n",
    "                up_intercepts.append(mac[i + 1])\n",
    "            elif subsets[i] >= mac[i,1] and mac[i + 1,1] >= subsets[i + 1]:\n",
    "                down_intercepts.append(mac[i + 1])\n",
    "    else:\n",
    "        raise Exception(\"Data sample not found at Moving Average Curve.\")\n",
    "\n",
    "    return up_intercepts, down_intercepts\n",
    "\n",
    "def filter_intercept_outlier(up_intercepts: List,\n",
    "                             down_intercepts: List) -> [List,List]:\n",
    "    \"\"\"\n",
    "    Remove two or more consecutive up or down intercepts.\n",
    "\n",
    "    :rtype: object\n",
    "    :return up_intercepts_updated, down_intercepts_updated:\n",
    "    :param up_intercepts:\n",
    "    :param down_intercepts:\n",
    "    \"\"\"\n",
    "\n",
    "    up_intercepts_filtered = []\n",
    "    down_intercepts_filtered = []\n",
    "\n",
    "    for index in range(len(down_intercepts) - 1):\n",
    "        up_intercepts_between_down_intercepts = []\n",
    "        for ui in up_intercepts:\n",
    "            if down_intercepts[index][0] <= ui[0] <= down_intercepts[index + 1][0]:\n",
    "                up_intercepts_between_down_intercepts.append(ui)\n",
    "\n",
    "        if len(up_intercepts_between_down_intercepts) > 0:\n",
    "            up_intercepts_filtered.append(up_intercepts_between_down_intercepts[-1])\n",
    "\n",
    "    up_intercepts_after_down = [ui for ui in up_intercepts if ui[0] > down_intercepts[-1][0]]\n",
    "    if len(up_intercepts_after_down) > 0:\n",
    "        up_intercepts_filtered.append(up_intercepts_after_down[-1])\n",
    "\n",
    "    down_intercepts_before_up = [di for di in down_intercepts if di[0] < up_intercepts_filtered[0][0]]\n",
    "    if len(down_intercepts_before_up) > 0:\n",
    "        down_intercepts_filtered.append(down_intercepts_before_up[-1])\n",
    "\n",
    "    for index in range(len(up_intercepts_filtered) - 1):\n",
    "        down_intercepts_between_up_intercepts = []\n",
    "        for di in down_intercepts:\n",
    "            if up_intercepts_filtered[index][0] <= di[0] <= up_intercepts_filtered[index + 1][0]:\n",
    "                down_intercepts_between_up_intercepts.append(di)\n",
    "\n",
    "        if len(down_intercepts_between_up_intercepts) > 0:\n",
    "            down_intercepts_filtered.append(down_intercepts_between_up_intercepts[-1])\n",
    "\n",
    "    down_intercepts_after_up = [di for di in down_intercepts if di[0] > up_intercepts_filtered[-1][0]]\n",
    "    if len(down_intercepts_after_up) > 0:\n",
    "        down_intercepts_filtered.append(down_intercepts_after_up[-1])\n",
    "\n",
    "    up_intercepts_truncated = []\n",
    "    for ui in up_intercepts_filtered:\n",
    "        if ui[0] >= down_intercepts_filtered[0][0]:\n",
    "            up_intercepts_truncated.append(ui)\n",
    "\n",
    "    min_length = min(len(up_intercepts_truncated), len(down_intercepts_filtered))\n",
    "\n",
    "    up_intercepts_updated = up_intercepts_truncated[:min_length]\n",
    "    down_intercepts_updated = down_intercepts_filtered[:min_length]\n",
    "\n",
    "    return up_intercepts_updated, down_intercepts_updated\n",
    "\n",
    "\n",
    "\n",
    "def generate_peak_valley(up_intercepts: List,\n",
    "                         down_intercepts: List,\n",
    "                         data: np.ndarray) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Compute peak valley from up intercepts and down intercepts indices.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks, valleys:\n",
    "    :param up_intercepts:\n",
    "    :param down_intercepts:\n",
    "    :param data:\n",
    "    \"\"\"\n",
    "    peaks = []\n",
    "    valleys = []\n",
    "\n",
    "    last_iterated_index = 0\n",
    "    for i in range(len(down_intercepts) - 1):\n",
    "        peak = None\n",
    "        valley = None\n",
    "\n",
    "        for j in range(last_iterated_index, len(data)):\n",
    "            if down_intercepts[i][0] <= data[j][0] <= up_intercepts[i][0]:\n",
    "                if valley is None or data[j][1] < valley[1]:\n",
    "                    valley = data[j]\n",
    "            elif up_intercepts[i][0] <= data[j][0] <= down_intercepts[i + 1][0]:\n",
    "                if peak is None or data[j][1] > peak[1]:\n",
    "                    peak = data[j]\n",
    "            elif data[j][0] > down_intercepts[i + 1][0]:\n",
    "                last_iterated_index = j\n",
    "                break\n",
    "        if peak is None or valley is None:\n",
    "            continue\n",
    "\n",
    "        valleys.append(valley)\n",
    "        peaks.append(peak)\n",
    "\n",
    "    return peaks, valleys\n",
    "\n",
    "\n",
    "def correct_valley_position(peaks: List,\n",
    "                            valleys: List,\n",
    "                            up_intercepts: List,\n",
    "                            data: np.ndarray,\n",
    "                            data_start_time_to_index: dict) -> List:\n",
    "    \"\"\"\n",
    "    Correct Valley position by locating actual valley using maximum slope algorithm which is\n",
    "    located between current valley and following peak.\n",
    "\n",
    "    Algorithm - push valley towards right:\n",
    "    Search for points lies in between current valley and following Up intercept.\n",
    "    Calculate slopes at those points.\n",
    "    Ensure that valley resides at the begining of inhalation cycle where inhalation slope is maximum.\n",
    "\n",
    "    :rtype: object\n",
    "    :return valley_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param up_intercepts:\n",
    "    :param data:\n",
    "    :param data_start_time_to_index: hash table for data where start_time is key, index is value\n",
    "    \"\"\"\n",
    "    valley_updated = valleys.copy()\n",
    "    for i in range(len(valleys)):\n",
    "        if valleys[i][0] < up_intercepts[i][0] < peaks[i][0]:\n",
    "            up_intercept = up_intercepts[i]\n",
    "\n",
    "            if valleys[i][0] not in data_start_time_to_index or up_intercept[0] not in data_start_time_to_index:\n",
    "                exception_message = 'Data has no start time for valley or up intercept start time at index ' + str(i)\n",
    "                raise Exception(exception_message)\n",
    "            else:\n",
    "                valley_index = data_start_time_to_index[valleys[i][0]]\n",
    "                up_intercept_index = data_start_time_to_index[up_intercept[0]]\n",
    "                data_valley_to_ui = data[valley_index: up_intercept_index + 1]\n",
    "                sample_valley_to_ui = data_valley_to_ui[:,1]\n",
    "\n",
    "                slope_at_samples = np.diff(sample_valley_to_ui)\n",
    "\n",
    "                consecutive_positive_slopes = [-1] * len(slope_at_samples)\n",
    "\n",
    "                for j in range(len(slope_at_samples)):\n",
    "                    slopes_subset = slope_at_samples[j:]\n",
    "                    if all(slope > 0 for slope in slopes_subset):\n",
    "                        consecutive_positive_slopes[j] = len(slopes_subset)\n",
    "\n",
    "                if any(no_con_slope > 0 for no_con_slope in consecutive_positive_slopes):\n",
    "                    indices_max_pos_slope = []\n",
    "                    for k in range(len(consecutive_positive_slopes)):\n",
    "                        if consecutive_positive_slopes[k] == max(consecutive_positive_slopes):\n",
    "                            indices_max_pos_slope.append(k)\n",
    "                    valley_updated[i] = data_valley_to_ui[indices_max_pos_slope[-1]]\n",
    "\n",
    "        else:\n",
    "            # TODO: discuss whether raise exception or not\n",
    "            # Up intercept at index i is not between valley and peak at index i.\n",
    "            break\n",
    "\n",
    "    return valley_updated\n",
    "\n",
    "\n",
    "def correct_peak_position(peaks: List,\n",
    "                          valleys: List,\n",
    "                          up_intercepts: List,\n",
    "                          data: np.ndarray,\n",
    "                          max_amplitude_change_peak_correction: float,\n",
    "                          min_neg_slope_count_peak_correction: int,\n",
    "                          data_start_time_to_index: dict) -> List:\n",
    "    \"\"\"\n",
    "    Correct peak position by checking if there is a notch in the inspiration branch at left position.\n",
    "    If at least 60% inspiration is done at a notch point, assume that notch as an original peak.\n",
    "    Our hypothesis is most of breathing in done for that cycle. We assume insignificant amount of\n",
    "    breath is taken or some new cycle started after the notch.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param up_intercepts:\n",
    "    :param data:\n",
    "    :param max_amplitude_change_peak_correction:\n",
    "    :param min_neg_slope_count_peak_correction:\n",
    "    :param data_start_time_to_index: hash table for data where start_time is key, index is value\n",
    "\n",
    "    \"\"\"\n",
    "    for i, item in enumerate(peaks):\n",
    "        if valleys[i][0] < up_intercepts[i][0] < peaks[i][0]:\n",
    "            up_intercept = up_intercepts[i]\n",
    "            # points between current valley and UI.\n",
    "            if up_intercept[0] not in data_start_time_to_index or peaks[i][0] not in data_start_time_to_index:\n",
    "                exception_message = 'Data has no start time for peak or up intercept start time at index ' + str(i)\n",
    "#                 raise Exception(exception_message)\n",
    "                return np.array([])\n",
    "            else:\n",
    "                data_up_intercept_index = data_start_time_to_index[up_intercept[0]]\n",
    "                data_peak_index = data_start_time_to_index[peaks[i][0]]\n",
    "\n",
    "                data_ui_to_peak = data[data_up_intercept_index: data_peak_index + 1]\n",
    "\n",
    "                sample_ui_to_peak = data_ui_to_peak[:,1]\n",
    "                slope_at_samples = np.diff(sample_ui_to_peak)\n",
    "\n",
    "                if not all(j >= 0 for j in slope_at_samples):\n",
    "                    indices_neg_slope = [j for j in range(len(slope_at_samples)) if slope_at_samples[j] < 0]\n",
    "                    peak_new = data_ui_to_peak[indices_neg_slope[0]]\n",
    "                    valley_peak_dist_new = peak_new[1] - valleys[i][1]\n",
    "                    valley_peak_dist_prev = peaks[i][1] - valleys[i][1]\n",
    "                    if valley_peak_dist_new == 0:\n",
    "                        return np.array([])\n",
    "                    else:\n",
    "                        amplitude_change = (valley_peak_dist_prev - valley_peak_dist_new) / valley_peak_dist_new * 100.0\n",
    "\n",
    "                        if len(indices_neg_slope) >= min_neg_slope_count_peak_correction:\n",
    "                            if amplitude_change <= max_amplitude_change_peak_correction:\n",
    "                                peaks[i] = peak_new  # 60% inspiration is done at that point.\n",
    "\n",
    "        else:\n",
    "            # TODO: Discuss whether raise exception or not for this scenario.\n",
    "            break  # up intercept at i is not between valley and peak at i\n",
    "\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def remove_close_valley_peak_pair(peaks: List,\n",
    "                                  valleys: List,\n",
    "                                  minimum_peak_to_valley_time_diff: float = 0.31) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out too close valley peak pair.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param minimum_peak_to_valley_time_diff:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = []\n",
    "\n",
    "    for i, item in enumerate(peaks):\n",
    "        time_diff_valley_peak = peaks[i][0] - valleys[i][0]\n",
    "        if time_diff_valley_peak/1000 > minimum_peak_to_valley_time_diff:\n",
    "            peaks_updated.append(peaks[i])\n",
    "            valleys_updated.append(valleys[i])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "def filter_expiration_duration_outlier(peaks: List,\n",
    "                                       valleys: List,\n",
    "                                       threshold_expiration_duration: float) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair for which expiration duration is too small.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param threshold_expiration_duration:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = [valleys[0]]\n",
    "\n",
    "    for i, item in enumerate(peaks):\n",
    "        if i < len(peaks) - 1:\n",
    "            expiration_duration = valleys[i + 1][0] - peaks[i][0]\n",
    "            if expiration_duration/1000 > threshold_expiration_duration:\n",
    "                peaks_updated.append(peaks[i])\n",
    "                valleys_updated.append(valleys[i + 1])\n",
    "\n",
    "    peaks_updated.append(peaks[-1])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def filter_small_amp_expiration_peak_valley(peaks: List,\n",
    "                                            valleys: List,\n",
    "                                            expiration_amplitude_threshold_perc: float) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair if their expiration amplitude is less than or equal to 10% of\n",
    "    average expiration amplitude.\n",
    "\n",
    "    :rtype: object\n",
    "    :return: peaks_updated, valleys_updated:\n",
    "    :param: peaks:\n",
    "    :param: valleys:\n",
    "    :param: expiration_amplitude_threshold_perc:\n",
    "    \"\"\"\n",
    "\n",
    "    expiration_amplitudes = []\n",
    "    peaks_updated = []\n",
    "    valleys_updated = [valleys[0]]\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        if i < len(peaks) - 1:\n",
    "            expiration_amplitudes.append(abs(valleys[i + 1][1] - peak[1]))\n",
    "\n",
    "    mean_expiration_amplitude = np.mean(expiration_amplitudes)\n",
    "\n",
    "    for i, expiration_amplitude in enumerate(expiration_amplitudes):\n",
    "        if expiration_amplitude > expiration_amplitude_threshold_perc * mean_expiration_amplitude:\n",
    "            peaks_updated.append(peaks[i])\n",
    "            valleys_updated.append(valleys[i + 1])\n",
    "\n",
    "    peaks_updated.append(peaks[-1])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def filter_small_amp_inspiration_peak_valley(peaks: List,\n",
    "                                             valleys: List,\n",
    "                                             inspiration_amplitude_threshold_perc: float) -> [List,List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair if their inspiration amplitude is less than or to equal 10% of\n",
    "    average inspiration amplitude.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param inspiration_amplitude_threshold_perc:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = []\n",
    "\n",
    "    inspiration_amplitudes = [(peaks[i][1] - valleys[i][1]) for i, valley in enumerate(valleys)]\n",
    "    mean_inspiration_amplitude = np.mean(inspiration_amplitudes)\n",
    "\n",
    "    for i, inspiration_amplitude in enumerate(inspiration_amplitudes):\n",
    "        if inspiration_amplitude > inspiration_amplitude_threshold_perc * mean_inspiration_amplitude:\n",
    "            valleys_updated.append(valleys[i])\n",
    "            peaks_updated.append(peaks[i])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def compute_peak_valley(rip: np.ndarray,\n",
    "                        fs: float = 21.33,\n",
    "                        smoothing_factor: int = 5,\n",
    "                        time_window: int = 8,\n",
    "                        expiration_amplitude_threshold_perc: float = 0.10,\n",
    "                        threshold_expiration_duration: float = 0.312,\n",
    "                        inspiration_amplitude_threshold_perc: float = 0.10,\n",
    "                        max_amplitude_change_peak_correction: float = 30,\n",
    "                        min_neg_slope_count_peak_correction: int = 4,\n",
    "                        minimum_peak_to_valley_time_diff=0.31) -> [np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute peak and valley from rip data and filter peak and valley.\n",
    "\n",
    "    :rtype: object\n",
    "    :param minimum_peak_to_valley_time_diff:\n",
    "    :param inspiration_amplitude_threshold_perc:\n",
    "    :param smoothing_factor:\n",
    "    :return peak_datastream, valley_datastream:\n",
    "    :param rip:\n",
    "    :param rip_quality:\n",
    "    :param fs:\n",
    "    :param time_window:\n",
    "    :param expiration_amplitude_threshold_perc:\n",
    "    :param threshold_expiration_duration:\n",
    "    :param max_amplitude_change_peak_correction:\n",
    "    :param min_neg_slope_count_peak_correction:\n",
    "    \"\"\"\n",
    "\n",
    "    rip_filtered = rip\n",
    "#     print(rip_filtered.shape)\n",
    "    data_smooth = smooth(data=rip_filtered, span=smoothing_factor)\n",
    "#     print(data_smooth.shape)\n",
    "    window_length = int(round(time_window * fs))\n",
    "    # plt.figure()\n",
    "    # plt.plot(rip_filtered[:,0],rip_filtered[:,1])\n",
    "    # plt.plot(data_smooth[:,0],data_smooth[:,1])\n",
    "    # # plt.plot(data_mac[:,0],data_mac[:,1])\n",
    "    # plt.show()\n",
    "    data_mac = moving_average_curve(deepcopy(data_smooth), window_length=window_length)\n",
    "\n",
    "    data_smooth_start_time_to_index = {}\n",
    "    for index, data in enumerate(data_smooth):\n",
    "        data_smooth_start_time_to_index[data_smooth[index,0]] = index\n",
    "\n",
    "    up_intercepts, down_intercepts = up_down_intercepts(data=data_smooth,\n",
    "                                                        mac=data_mac,\n",
    "                                                        data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "    if len(up_intercepts)<3 or len(down_intercepts)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # print(up_intercepts,down_intercepts)\n",
    "    up_intercepts_filtered, down_intercepts_filtered = filter_intercept_outlier(up_intercepts=up_intercepts,\n",
    "                                                                                down_intercepts=down_intercepts)\n",
    "    if len(up_intercepts_filtered)<3 or len(down_intercepts_filtered)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    peaks, valleys = generate_peak_valley(up_intercepts=up_intercepts_filtered,\n",
    "                                          down_intercepts=down_intercepts_filtered,\n",
    "                                          data=data_smooth)\n",
    "    if len(peaks)<3 or len(valleys)<3:\n",
    "        return np.array([]),np.array([])\n",
    "\n",
    "    valleys_corrected = correct_valley_position(peaks=peaks,\n",
    "                                                valleys=valleys,\n",
    "                                                up_intercepts=up_intercepts_filtered,\n",
    "                                                data=data_smooth,\n",
    "                                                data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "\n",
    "    if len(valleys_corrected)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    peaks_corrected = correct_peak_position(peaks=peaks,\n",
    "                                            valleys=valleys_corrected,\n",
    "                                            up_intercepts=up_intercepts_filtered,\n",
    "                                            data=data_smooth,\n",
    "                                            max_amplitude_change_peak_correction=max_amplitude_change_peak_correction,\n",
    "                                            min_neg_slope_count_peak_correction=min_neg_slope_count_peak_correction,\n",
    "                                            data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "    if len(peaks_corrected)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # remove too close valley peak pair.\n",
    "    peaks_filtered_close, valleys_filtered_close = remove_close_valley_peak_pair(peaks=peaks_corrected,\n",
    "                                                                                 valleys=valleys_corrected,\n",
    "                                                                                 minimum_peak_to_valley_time_diff=minimum_peak_to_valley_time_diff)\n",
    "    if len(peaks_filtered_close)<3 or len(valleys_filtered_close)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # Remove small  Expiration duration < 0.31\n",
    "    peaks_filtered_exp_dur, valleys_filtered_exp_dur = filter_expiration_duration_outlier(peaks=peaks_filtered_close,\n",
    "                                                                                          valleys=valleys_filtered_close,\n",
    "                                                                                          threshold_expiration_duration=threshold_expiration_duration)\n",
    "    if len(peaks_filtered_exp_dur)<3 or len(valleys_filtered_exp_dur)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # filter out peak valley pair of inspiration of small amplitude.\n",
    "    peaks_filtered_insp_amp, valleys_filtered_insp_amp = filter_small_amp_inspiration_peak_valley(\n",
    "        peaks=peaks_filtered_exp_dur,\n",
    "        valleys=valleys_filtered_exp_dur,\n",
    "        inspiration_amplitude_threshold_perc=inspiration_amplitude_threshold_perc)\n",
    "    if len(peaks_filtered_insp_amp)<3 or len(valleys_filtered_insp_amp)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # filter out peak valley pair of expiration of small amplitude.\n",
    "    peaks_filtered_exp_amp, valleys_filtered_exp_amp = filter_small_amp_expiration_peak_valley(\n",
    "        peaks=peaks_filtered_insp_amp,\n",
    "        valleys=valleys_filtered_insp_amp,\n",
    "        expiration_amplitude_threshold_perc=expiration_amplitude_threshold_perc)\n",
    "    if len(peaks_filtered_exp_amp)<3 or len(valleys_filtered_exp_amp)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    \n",
    "    peaks_filtered_exp_amp, valleys_filtered_exp_amp = np.array(peaks_filtered_exp_amp), np.array(valleys_filtered_exp_amp)\n",
    "    # peaks_filtered_exp_amp = np.insert(peaks_filtered_exp_amp,\n",
    "    # peaks_filtered_exp_amp =  np.insert(peaks_filtered_exp_amp, 2, 5, axis=1)\n",
    "    # valleys_filtered_exp_amp =  np.insert(valleys_filtered_exp_amp, 2, 5, axis=1)\n",
    "    # peaks_filtered_exp_amp[:,2] = [data_smooth_start_time_to_index[i[0]] for i in peaks_filtered_exp_amp]\n",
    "    # valleys_filtered_exp_amp[:,2] = [data_smooth_start_time_to_index[i[0]] for i in valleys_filtered_exp_amp]\n",
    "    return np.array(itemgetter(*list(peaks_filtered_exp_amp[:,0]))(data_smooth_start_time_to_index)),\\\n",
    "    np.array(itemgetter(*list(valleys_filtered_exp_amp[:,0]))(data_smooth_start_time_to_index))\n",
    "\n",
    "rip_stream = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "rip_data = CC.get_stream(rip_stream)\n",
    "\n",
    "rip_data.show(5,False)\n",
    "rip_clean_data = rip_data.filter(F.col('quality')=='Acceptable')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"indicator\", StringType())\n",
    "])\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(key,data):\n",
    "    if data.shape[0]>2*21.33*10:\n",
    "        data = data.sort_values('time').reset_index(drop=True)\n",
    "        r = np.zeros((data.shape[0],2))\n",
    "        r[:,0] = 1000*data['time'].values\n",
    "        r[:,1] = data['respiration'].values\n",
    "        peak_index,valley_index = compute_peak_valley(r)\n",
    "        data['indicator'] = 1\n",
    "        data['indicator'].loc[peak_index] = 'p'\n",
    "        data['indicator'].loc[valley_index] = 'v'\n",
    "        data = data[data.indicator.isin(['p','v'])]\n",
    "        return data[['timestamp','localtime','version','user','indicator','respiration']]\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['timestamp','localtime','version','user','indicator','respiration'])\n",
    "rip_clean_data = rip_clean_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "peak_valley_data = rip_clean_data.compute(peak_valley,windowDuration=60,startTime='0 seconds')\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.peak.valley'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Peak Valley of Autosense Respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"indicator\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"Peak and valley indicator in respiration signal\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration amplitude\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','Peak Valley').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "\n",
    "peak_valley_data.metadata = stream_metadata\n",
    "\n",
    "CC.save_stream(peak_valley_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28534604"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.peak.valley').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- localtime: timestamp (nullable = true)\n",
      " |-- start: timestamp (nullable = true)\n",
      " |-- end: timestamp (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- inspiration: double (nullable = true)\n",
      " |-- expiration: double (nullable = true)\n",
      " |-- respiration: double (nullable = true)\n",
      " |-- ieratio: double (nullable = true)\n",
      " |-- stretch: double (nullable = true)\n",
      " |-- pinspiration: double (nullable = true)\n",
      " |-- pexpiration: double (nullable = true)\n",
      " |-- prespiration: double (nullable = true)\n",
      " |-- pstretch: double (nullable = true)\n",
      " |-- ninspiration: double (nullable = true)\n",
      " |-- nexpiration: double (nullable = true)\n",
      " |-- nrespiration: double (nullable = true)\n",
      " |-- nstretch: double (nullable = true)\n",
      " |-- rexpiration: double (nullable = true)\n",
      " |-- rstretch: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rip_cycle_feature_computation(peaks_datastream: np.ndarray,\n",
    "                                  valleys_datastream: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Respiration Feature Implementation. The respiration feature values are\n",
    "    derived from the following paper:\n",
    "    'puffMarker: a multi-sensor approach for pinpointing the timing of first lapse in smoking cessation'\n",
    "    Removed due to lack of current use in the implementation\n",
    "    roc_max = []  # 8. ROC_MAX = max(sample[j]-sample[j-1])\n",
    "    roc_min = []  # 9. ROC_MIN = min(sample[j]-sample[j-1])\n",
    "\n",
    "    :param peaks_datastream: list of peak datapoints\n",
    "    :param valleys_datastream: list of valley datapoints\n",
    "    :return: lists of DataPoints each representing a specific feature calculated from the respiration cycle\n",
    "    found from the peak valley inputs\n",
    "    \"\"\"\n",
    "\n",
    "    inspiration_duration = []  # 1 Inhalation duration\n",
    "    expiration_duration = []  # 2 Exhalation duration\n",
    "    respiration_duration = []  # 3 Respiration duration\n",
    "    inspiration_expiration_ratio = []  # 4 Inhalation and Exhalation ratio\n",
    "    stretch = []  # 5 Stretch\n",
    "    upper_stretch = []  # 6. Upper portion of the stretch calculation\n",
    "    lower_stretch = []  # 7. Lower portion of the stretch calculation\n",
    "    delta_previous_inspiration_duration = []  # 10. BD_INSP = INSP(i)-INSP(i-1)\n",
    "    delta_previous_expiration_duration = []  # 11. BD_EXPR = EXPR(i)-EXPR(i-1)\n",
    "    delta_previous_respiration_duration = []  # 12. BD_RESP = RESP(i)-RESP(i-1)\n",
    "    delta_previous_stretch_duration = []  # 14. BD_Stretch= Stretch(i)-Stretch(i-1)\n",
    "    delta_next_inspiration_duration = []  # 19. FD_INSP = INSP(i)-INSP(i+1)\n",
    "    delta_next_expiration_duration = []  # 20. FD_EXPR = EXPR(i)-EXPR(i+1)\n",
    "    delta_next_respiration_duration = []  # 21. FD_RESP = RESP(i)-RESP(i+1)\n",
    "    delta_next_stretch_duration = []  # 23. FD_Stretch= Stretch(i)-Stretch(i+1)\n",
    "    neighbor_ratio_expiration_duration = []  # 29. D5_EXPR(i) = EXPR(i) / avg(EXPR(i-2)...EXPR(i+2))\n",
    "    neighbor_ratio_stretch_duration = []  # 32. D5_Stretch = Stretch(i) / avg(Stretch(i-2)...Stretch(i+2))\n",
    "\n",
    "    valleys = valleys_datastream\n",
    "    peaks = peaks_datastream[:-1]\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "\n",
    "        delta = peak[0] - valleys[i][0]\n",
    "        inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - peak[0]\n",
    "        expiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - valley_start_time\n",
    "        respiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        ratio = (peak[0] - valley_start_time) / (valleys[i + 1][0] - peak[0])\n",
    "        inspiration_expiration_ratio.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        value = peak[1] - valleys[i + 1][1]\n",
    "        stretch.append(np.array([valley_start_time,valley_end_time,value]))\n",
    "\n",
    "    for i, point in enumerate(inspiration_duration):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "        if i == 0:  # Edge case\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i - 1][2]\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i - 1][2]\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i - 1][2]\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i - 1][2]\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        if i == len(inspiration_duration) - 1:\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i + 1][2]\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i + 1][2]\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i + 1][2]\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i + 1][2]\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        stretch_average = 0\n",
    "        expiration_average = 0\n",
    "        count = 0.0\n",
    "        for j in [-2, -1, 1, 2]:\n",
    "            if i + j < 0 or i + j >= len(inspiration_duration):\n",
    "                continue\n",
    "            stretch_average += stretch[i + j][2]\n",
    "            expiration_average += expiration_duration[i + j][2]\n",
    "            count += 1\n",
    "\n",
    "        stretch_average /= count\n",
    "        expiration_average /= count\n",
    "\n",
    "        ratio = stretch[i][2] / stretch_average\n",
    "        neighbor_ratio_stretch_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        ratio = expiration_duration[i][2] / expiration_average\n",
    "        neighbor_ratio_expiration_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "    # Begin assembling datastream for output\n",
    "    inspiration_duration_datastream = np.array(inspiration_duration)[1:-1]\n",
    "\n",
    "    expiration_duration_datastream = np.array(expiration_duration)[1:-1]\n",
    "\n",
    "    respiration_duration_datastream = np.array(respiration_duration)[1:-1]\n",
    "\n",
    "    inspiration_expiration_ratio_datastream = np.array(inspiration_expiration_ratio)[1:-1]\n",
    "\n",
    "    stretch_datastream = np.array(stretch)[1:-1]\n",
    "\n",
    "    delta_previous_inspiration_duration_datastream = np.array(delta_previous_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_expiration_duration_datastream = np.array(delta_previous_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_respiration_duration_datastream = np.array(delta_previous_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_stretch_duration_datastream = np.array(delta_previous_stretch_duration)[1:-1]\n",
    "\n",
    "    delta_next_inspiration_duration_datastream = np.array(delta_next_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_expiration_duration_datastream = np.array(delta_next_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_respiration_duration_datastream = np.array(delta_next_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_stretch_duration_datastream = np.array(delta_next_stretch_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_expiration_datastream = np.array(neighbor_ratio_expiration_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_stretch_datastream = np.array(neighbor_ratio_stretch_duration)[1:-1]\n",
    "\n",
    "    return np.concatenate([inspiration_duration_datastream,\n",
    "                           expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           inspiration_expiration_ratio_datastream[:,2].reshape(-1,1),\n",
    "                           stretch_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_expiration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_stretch_datastream[:,2].reshape(-1,1)],axis=1)\n",
    "stream_name = 'org.md2k.autosense.rip.peak.valley'\n",
    "\n",
    "data = CC.get_stream(stream_name)\n",
    "\n",
    "data_time = data.withColumn('time',1000*F.col('timestamp').cast('double'))\n",
    "\n",
    "# data_time.sort('timestamp').show(5,False)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"inspiration\", DoubleType()),\n",
    "    StructField(\"expiration\", DoubleType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"ieratio\", DoubleType()),\n",
    "    StructField(\"stretch\", DoubleType()),\n",
    "    StructField(\"pinspiration\", DoubleType()),\n",
    "    StructField(\"pexpiration\", DoubleType()),\n",
    "    StructField(\"prespiration\", DoubleType()),\n",
    "    StructField(\"pstretch\", DoubleType()),\n",
    "    StructField(\"ninspiration\", DoubleType()),\n",
    "    StructField(\"nexpiration\", DoubleType()),\n",
    "    StructField(\"nrespiration\", DoubleType()),\n",
    "    StructField(\"nstretch\", DoubleType()),\n",
    "    StructField(\"rexpiration\", DoubleType()),\n",
    "    StructField(\"rstretch\", DoubleType())    \n",
    "])\n",
    "\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(key,data):\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "    if data.shape[0]>8:\n",
    "        index_dict = {}\n",
    "        data = data.sort_values('time').reset_index(drop=True)\n",
    "        for i in range(data.shape[0]):\n",
    "            index_dict[data['time'].loc[i]] = i\n",
    "        peak_data = data[data.indicator.isin(['p'])]\n",
    "        valley_data = data[data.indicator.isin(['v'])]\n",
    "        peaks = peak_data[['time','respiration']].values\n",
    "        valleys = valley_data[['time','respiration']].values\n",
    "        features = rip_cycle_feature_computation(peaks,valleys)\n",
    "        timestamp_col = data['timestamp'].values\n",
    "        localtime_col = data['localtime'].values\n",
    "        user_id = data['user'].values[0]\n",
    "        version_id = data['version'].values[0]\n",
    "        data1 = pd.DataFrame()\n",
    "        ind_col_1 = np.array([index_dict[features[i][0]] for i in range(features.shape[0])])\n",
    "        ind_col_2 = np.array([index_dict[features[i][1]] for i in range(features.shape[0])])\n",
    "        timestamp_col_s = timestamp_col[ind_col_1]\n",
    "        timestamp_col_e = timestamp_col[ind_col_2]\n",
    "        localtime_col = localtime_col[ind_col_1]\n",
    "        data1['timestamp'] = timestamp_col_s\n",
    "        data1['start'] = timestamp_col_s\n",
    "        data1['end'] = timestamp_col_e\n",
    "        data1['localtime'] = localtime_col\n",
    "        data1['user'] = [user_id]*timestamp_col_e.reshape(-1).shape[0]\n",
    "        data1['version'] = [version_id]*timestamp_col_e.reshape(-1).shape[0]\n",
    "        for i in range(2,features.shape[1],1):\n",
    "            data1[columns[i-2]] = list(features[:,i])\n",
    "        return data1\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=columns+['user','version','timestamp','localtime','start','end'])\n",
    "\n",
    "\n",
    "respiration_features = data_time.compute(peak_valley,windowDuration=240,startTime='0 seconds')\n",
    "\n",
    "# respiration_features.sort('timestamp').show(5,False)\n",
    "respiration_features.printSchema()\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.cycle.features'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Features from autosense respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"start\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"start time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"end\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"end time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"inspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"expiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"expiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ieratio\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration to expiration ratio\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"stretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pinspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"prespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ninspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nrespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor stretch\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','cycle features').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "respiration_features.metadata = stream_metadata\n",
    "CC.save_stream(respiration_features,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13062513"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.cycle.features').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+-----------------------+-----------------------+-----------+----------+-----------+-------------------+-------+--------------------+-------------------+---------------------+--------+--------------------+--------------------+--------------------+--------+-------------------+-------------------+------------------------------------+-------+\n",
      "|timestamp              |localtime              |start                  |end                    |inspiration|expiration|respiration|ieratio            |stretch|pinspiration        |pexpiration        |prespiration         |pstretch|ninspiration        |nexpiration         |nrespiration        |nstretch|rexpiration        |rstretch           |user                                |version|\n",
      "+-----------------------+-----------------------+-----------------------+-----------------------+-----------+----------+-----------+-------------------+-------+--------------------+-------------------+---------------------+--------+--------------------+--------------------+--------------------+--------+-------------------+-------------------+------------------------------------+-------+\n",
      "|2019-02-01 07:12:10.776|2019-02-01 01:12:10.776|2019-02-01 07:12:10.776|2019-02-01 07:12:12.339|0.417      |1.146     |1.563      |0.36387434554973824|77.0   |-0.13200000000000006|0.08199999999999985|-0.050000000000000044|13.0    |-0.27299999999999996|-0.3640000000000001 |-0.6370000000000002 |-57.0   |1.0386706948640483 |0.7107692307692308 |40cf2f37-51f5-3066-b248-f32d1f6256b6|1      |\n",
      "|2019-02-01 07:12:12.339|2019-02-01 01:12:12.339|2019-02-01 07:12:12.339|2019-02-01 07:12:14.539|0.69       |1.51      |2.2        |0.45695364238410596|134.0  |0.27299999999999996 |0.3640000000000001 |0.6370000000000002   |57.0    |-0.835              |0.774               |-0.06099999999999994|7.0     |1.4561234329797494 |1.8108108108108107 |40cf2f37-51f5-3066-b248-f32d1f6256b6|1      |\n",
      "|2019-02-01 07:12:14.539|2019-02-01 01:12:14.539|2019-02-01 07:12:14.539|2019-02-01 07:12:16.8  |1.525      |0.736     |2.261      |2.0720108695652173 |127.0  |0.835               |-0.774             |0.06099999999999994  |-7.0    |-0.2180000000000002 |-0.46599999999999997|-0.6839999999999997 |99.0    |0.39076187947969204|1.9766536964980546 |40cf2f37-51f5-3066-b248-f32d1f6256b6|1      |\n",
      "|2019-02-01 07:12:16.8  |2019-02-01 01:12:16.8  |2019-02-01 07:12:16.8  |2019-02-01 07:12:19.745|1.743      |1.202     |2.945      |1.4500831946755408 |28.0   |0.2180000000000002  |0.46599999999999997|0.6839999999999997   |-99.0   |0.7330000000000001  |-2.474              |-1.741              |10.0    |0.5988292439905343 |0.34782608695652173|40cf2f37-51f5-3066-b248-f32d1f6256b6|1      |\n",
      "|2019-02-01 07:12:19.745|2019-02-01 01:12:19.745|2019-02-01 07:12:19.745|2019-02-01 07:12:24.431|1.01       |3.676     |4.686      |0.27475516866158867|18.0   |-0.7330000000000001 |2.474              |1.741                |-10.0   |-0.4750000000000001 |1.569               |1.0939999999999999  |-25.0   |2.900197238658777  |0.3116883116883117 |40cf2f37-51f5-3066-b248-f32d1f6256b6|1      |\n",
      "+-----------------------+-----------------------+-----------------------+-----------------------+-----------+----------+-----------+-------------------+-------+--------------------+-------------------+---------------------+--------+--------------------+--------------------+--------------------+--------+-------------------+-------------------+------------------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------------+-----------------------+-----------------------+-----------------------+-------+------------------------------------+-----------+----------+-----------+------------------+-------+------------+-----------+------------+--------+-------------------+-----------+------------+--------+-------------------+--------+--------+\n",
      "|timestamp              |localtime              |start                  |end                    |version|user                                |inspiration|expiration|respiration|ieratio           |stretch|pinspiration|pexpiration|prespiration|pstretch|ninspiration       |nexpiration|nrespiration|nstretch|rexpiration        |rstretch|day     |\n",
      "+-----------------------+-----------------------+-----------------------+-----------------------+-------+------------------------------------+-----------+----------+-----------+------------------+-------+------------+-----------+------------+--------+-------------------+-----------+------------+--------+-------------------+--------+--------+\n",
      "|2018-10-01 07:40:15.27 |2018-10-01 01:40:15.27 |2018-10-01 07:40:15.27 |2018-10-01 07:40:18.126|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|0.417      |2.439     |2.856      |26.258064516129032|75.0   |22.355      |117.744    |117.887     |86.0    |0.10199999999999998|113.141    |113.044     |3975.0  |0.5020240137221269 |Infinity|20181001|\n",
      "|2018-10-01 07:40:18.126|2018-10-01 01:40:18.126|2018-10-01 07:40:18.126|2018-10-01 07:40:20.144|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|0.327      |1.703     |2.018      |26.258064516129032|48.0   |22.355      |117.744    |117.887     |-27.0   |-0.201             |113.141    |113.044     |3975.0  |0.43566129444870816|Infinity|20181001|\n",
      "|2018-10-01 07:40:20.144|2018-10-01 01:40:20.144|2018-10-01 07:40:20.144|2018-10-01 07:40:29.157|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|0.516      |8.497     |9.013      |26.258064516129032|39.0   |22.355      |117.744    |117.887     |-9.0    |0.14400000000000002|113.141    |113.044     |3975.0  |6.5957694546865895 |Infinity|20181001|\n",
      "|2018-10-01 07:40:29.157|2018-10-01 01:40:29.157|2018-10-01 07:40:29.157|2018-10-01 07:40:29.854|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|0.372      |0.641     |1.394      |26.258064516129032|39.0   |22.355      |117.744    |117.887     |0.0     |-0.13              |113.141    |113.044     |3975.0  |0.13457862253536512|Infinity|20181001|\n",
      "|2018-10-01 07:40:29.854|2018-10-01 01:40:29.854|2018-10-01 07:40:29.854|2018-10-01 07:40:31.042|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|0.502      |0.686     |1.394      |26.258064516129032|80.0   |22.355      |117.744    |117.887     |41.0    |-0.573             |113.141    |113.044     |3975.0  |0.2673682159212706 |Infinity|20181001|\n",
      "+-----------------------+-----------------------+-----------------------+-----------------------+-------+------------------------------------+-----------+----------+-----------+------------------+-------+------------+-----------+------------+--------+-------------------+-----------+------------+--------+-------------------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr_stream = 'org.md2k.autosense.rip.cycle.features'\n",
    "rr_intervals = CC.get_stream(rr_stream)\n",
    "rr_intervals.show(5,False)\n",
    "rr_intervals = rr_intervals.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_90 = []\n",
    "for c in columns:\n",
    "    percentile_90 = F.expr('percentile_approx('+str(c)+',0.90)')\n",
    "    temp = rr_intervals.groupBy(['user','day']).agg(percentile_90.alias(str(c)))\n",
    "    df_90.append(temp.toPandas())\n",
    "\n",
    "from copy import deepcopy\n",
    "df_90_final = deepcopy(df_90[0])\n",
    "for i in range(1,len(df_90),1):\n",
    "    df_90_final = df_90_final.join(df_90[i].set_index(['user','day']),on=['user','day'], how='left')\n",
    "\n",
    "df_90_final.set_index(['user','day'],inplace=True)\n",
    "\n",
    "df_10 = []\n",
    "for c in columns:\n",
    "    percentile_10 = F.expr('percentile_approx('+str(c)+',0.10)')\n",
    "    temp = rr_intervals.groupBy(['user','day']).agg(percentile_10.alias(str(c)))\n",
    "    df_10.append(temp.toPandas())\n",
    "\n",
    "from copy import deepcopy\n",
    "df_10_final = deepcopy(df_10[0])\n",
    "for i in range(1,len(df_10),1):\n",
    "    df_10_final = df_10_final.join(df_10[i].set_index(['user','day']),on=['user','day'], how='left')\n",
    "\n",
    "df_10_final.set_index(['user','day'],inplace=True)\n",
    "\n",
    "# df_10_final.loc['c64ca471-369e-43fa-a07b-8260fd1c745c','20190612']['inspiration']\n",
    "\n",
    "df_90_final.head()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"inspiration\", DoubleType()),\n",
    "    StructField(\"expiration\", DoubleType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"ieratio\", DoubleType()),\n",
    "    StructField(\"stretch\", DoubleType()),\n",
    "    StructField(\"pinspiration\", DoubleType()),\n",
    "    StructField(\"pexpiration\", DoubleType()),\n",
    "    StructField(\"prespiration\", DoubleType()),\n",
    "    StructField(\"pstretch\", DoubleType()),\n",
    "    StructField(\"ninspiration\", DoubleType()),\n",
    "    StructField(\"nexpiration\", DoubleType()),\n",
    "    StructField(\"nrespiration\", DoubleType()),\n",
    "    StructField(\"nstretch\", DoubleType()),\n",
    "    StructField(\"rexpiration\", DoubleType()),\n",
    "    StructField(\"rstretch\", DoubleType()),\n",
    "    StructField(\"day\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(data):\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "    if data.shape[0]>1:\n",
    "        user = data['user'].loc[0]\n",
    "        day = data['day'].loc[0]\n",
    "        for c in columns:\n",
    "            upper = df_90_final.loc[user,day][c]\n",
    "            lower = df_10_final.loc[user,day][c]\n",
    "            data[c][data[c]>upper] = upper\n",
    "            data[c][data[c]<lower] = lower\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=columns+['user','version','timestamp','localtime','start','end','day'])\n",
    "\n",
    "rr_intervals_winsorized = rr_intervals.groupBy(['user','day']).apply(peak_valley)\n",
    "\n",
    "rr_intervals_winsorized.show(5,False)\n",
    "stream_name = 'org.md2k.autosense.rip.cycle.features.winsorized'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Features from autosense respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"start\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"start time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"end\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"end time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"inspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"expiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"expiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ieratio\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration to expiration ratio\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"stretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pinspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"prespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ninspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nrespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"day\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"day localtime\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_percentiles.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','cycle features winsorized').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "rr_intervals_winsorized_final =DataStream(data=rr_intervals_winsorized,metadata=stream_metadata)\n",
    "CC.save_stream(rr_intervals_winsorized_final,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13062513"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.cycle.features.winsorized').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------+--------+------------------+------------------+-----------------+------------------+-----------------+--------------------+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-----------------+-------------------+------------------+------------------+-------------------+--------------+------------------+--------------+------------------+----------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------+------------------+----------+-------------------+-------------------+-------------------+------------------+\n",
      "|              window|           timestamp|           localtime|                user|version|     day|     m_inspiration|      m_expiration|    m_respiration|         m_ieratio|        m_stretch|      m_pinspiration|       m_pexpiration|     m_rexpiration|        m_rstretch|     s_inspiration|      s_expiration|     s_respiration|          s_ieratio|        s_stretch|     s_pinspiration|     s_pexpiration|     s_rexpiration|         s_rstretch|80_inspiration|     80_expiration|80_respiration|        80_ieratio|80_stretch|    80_pinspiration|     80_pexpiration|    80_rexpiration|       80_rstretch|    20_inspiration|     20_expiration|20_respiration|        20_ieratio|20_stretch|    20_pinspiration|     20_pexpiration|     20_rexpiration|       20_rstretch|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------+--------+------------------+------------------+-----------------+------------------+-----------------+--------------------+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-----------------+-------------------+------------------+------------------+-------------------+--------------+------------------+--------------+------------------+----------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------+------------------+----------+-------------------+-------------------+-------------------+------------------+\n",
      "|[2017-07-14 20:25...|2017-07-14 20:25:...|2017-07-14 14:25:...|047dfad3-0807-37f...|      1|20170714|0.9783529411764706|             1.839|2.802647058823529|0.7006518166930862|845.2941176470588|-0.01282352941176...|-0.00711764705882...|0.9181089796016338|1.0471695086754067|0.3730247882474553|1.4431870114437702|1.3810067858801631|0.46641424032790607|505.2377861841247|0.44589127532590794|1.2271892112849256|0.5756304952963442|0.49760418228537356|        1.2114|             2.057|        3.1048|0.9205728255857456|    1148.2|0.40779999999999994| 0.6063999999999999|1.1430217750674294|1.5357228728154102|            0.6306|             1.219|        1.8756|0.3674320522877803|     578.8|-0.4975999999999999|            -0.5094| 0.3815355959206901|0.6451320319241999|\n",
      "|[2017-07-14 20:46...|2017-07-14 20:46:...|2017-07-14 14:46:...|047dfad3-0807-37f...|      1|20170714|1.0300526315789476|1.4406842105263158|2.434631578947368| 0.940496971420796|778.7368421052631|0.031210526315789473|0.005842105263157871|0.9199826438616205|0.9581783586541676| 0.541902458800291|1.4404689071669057|1.4282298760869585| 0.6157125074817668|375.0007173482417|0.39896582685005993|1.1598711740322187|0.6089017563777538| 0.4139758656909503|        1.3354|1.6555999999999997|        2.7584| 1.265881925804904|    1037.6|             0.3954|0.48300000000000004|1.2520148410406677| 1.226969950508388|0.5408000000000001|0.7607999999999999|        1.5314| 0.564212837747369|     514.8|-0.2693999999999999|-0.5167999999999999|0.43056993586102926|0.5888460950263261|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------+--------+------------------+------------------+-----------------+------------------+-----------------+--------------------+--------------------+------------------+------------------+------------------+------------------+------------------+-------------------+-----------------+-------------------+------------------+------------------+-------------------+--------------+------------------+--------------+------------------+----------+-------------------+-------------------+------------------+------------------+------------------+------------------+--------------+------------------+----------+-------------------+-------------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "True metadata\n",
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- localtime: timestamp (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- version: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- m_inspiration: double (nullable = true)\n",
      " |-- m_expiration: double (nullable = true)\n",
      " |-- m_respiration: double (nullable = true)\n",
      " |-- m_ieratio: double (nullable = true)\n",
      " |-- m_stretch: double (nullable = true)\n",
      " |-- m_pinspiration: double (nullable = true)\n",
      " |-- m_pexpiration: double (nullable = true)\n",
      " |-- m_rexpiration: double (nullable = true)\n",
      " |-- m_rstretch: double (nullable = true)\n",
      " |-- s_inspiration: double (nullable = true)\n",
      " |-- s_expiration: double (nullable = true)\n",
      " |-- s_respiration: double (nullable = true)\n",
      " |-- s_ieratio: double (nullable = true)\n",
      " |-- s_stretch: double (nullable = true)\n",
      " |-- s_pinspiration: double (nullable = true)\n",
      " |-- s_pexpiration: double (nullable = true)\n",
      " |-- s_rexpiration: double (nullable = true)\n",
      " |-- s_rstretch: double (nullable = true)\n",
      " |-- 80_inspiration: double (nullable = true)\n",
      " |-- 80_expiration: double (nullable = true)\n",
      " |-- 80_respiration: double (nullable = true)\n",
      " |-- 80_ieratio: double (nullable = true)\n",
      " |-- 80_stretch: double (nullable = true)\n",
      " |-- 80_pinspiration: double (nullable = true)\n",
      " |-- 80_pexpiration: double (nullable = true)\n",
      " |-- 80_rexpiration: double (nullable = true)\n",
      " |-- 80_rstretch: double (nullable = true)\n",
      " |-- 20_inspiration: double (nullable = true)\n",
      " |-- 20_expiration: double (nullable = true)\n",
      " |-- 20_respiration: double (nullable = true)\n",
      " |-- 20_ieratio: double (nullable = true)\n",
      " |-- 20_stretch: double (nullable = true)\n",
      " |-- 20_pinspiration: double (nullable = true)\n",
      " |-- 20_pexpiration: double (nullable = true)\n",
      " |-- 20_rexpiration: double (nullable = true)\n",
      " |-- 20_rstretch: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_name = 'org.md2k.autosense.rip.cycle.features.winsorized'\n",
    "df = CC.get_stream(stream_name)\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"m_inspiration\", DoubleType()),\n",
    "    StructField(\"s_inspiration\", DoubleType()),\n",
    "    StructField(\"80_inspiration\", DoubleType()),\n",
    "    StructField(\"20_inspiration\", DoubleType()),\n",
    "    StructField(\"m_expiration\", DoubleType()),\n",
    "    StructField(\"s_expiration\", DoubleType()),\n",
    "    StructField(\"80_expiration\", DoubleType()),\n",
    "    StructField(\"20_expiration\", DoubleType()),\n",
    "    StructField(\"m_respiration\", DoubleType()),\n",
    "    StructField(\"s_respiration\", DoubleType()),\n",
    "    StructField(\"80_respiration\", DoubleType()),\n",
    "    StructField(\"20_respiration\", DoubleType()),\n",
    "    StructField(\"m_ieratio\", DoubleType()),\n",
    "    StructField(\"s_ieratio\", DoubleType()),\n",
    "    StructField(\"80_ieratio\", DoubleType()),\n",
    "    StructField(\"20_ieratio\", DoubleType()),\n",
    "    StructField(\"m_stretch\", DoubleType()),\n",
    "    StructField(\"s_stretch\", DoubleType()),\n",
    "    StructField(\"80_stretch\", DoubleType()),\n",
    "    StructField(\"20_stretch\", DoubleType()),\n",
    "    StructField(\"m_pinspiration\", DoubleType()),\n",
    "    StructField(\"s_pinspiration\", DoubleType()),\n",
    "    StructField(\"80_pinspiration\", DoubleType()),\n",
    "    StructField(\"20_pinspiration\", DoubleType()),\n",
    "    StructField(\"m_pexpiration\", DoubleType()),\n",
    "    StructField(\"s_pexpiration\", DoubleType()),\n",
    "    StructField(\"80_pexpiration\", DoubleType()),\n",
    "    StructField(\"20_pexpiration\", DoubleType()),\n",
    "    StructField(\"m_rexpiration\", DoubleType()),\n",
    "    StructField(\"s_rexpiration\", DoubleType()),\n",
    "    StructField(\"80_rexpiration\", DoubleType()),\n",
    "    StructField(\"20_rexpiration\", DoubleType()),\n",
    "    StructField(\"m_rstretch\", DoubleType()),\n",
    "    StructField(\"s_rstretch\", DoubleType()),\n",
    "    StructField(\"80_rstretch\", DoubleType()),\n",
    "    StructField(\"20_rstretch\", DoubleType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def minute_level_rip_features(key,data):\n",
    "    ms82 = ['m_','s_','80_','20_']\n",
    "    all_columns = ['start','end','timestamp','localtime','user','version','day']\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "    if data.shape[0]>5:\n",
    "        all_data = []\n",
    "        all_data.extend([key[2]['start'],key[2]['end'],data['timestamp'].loc[0],\n",
    "                        data['localtime'].loc[0],data['user'].loc[0],data['version'].loc[0],\n",
    "                        data['day'].loc[0]])\n",
    "        data = data[columns]\n",
    "        list_col = [list(data.mean()),list(data.std()),\n",
    "                    list(data.quantile(.8)),list(data.quantile(.2))]\n",
    "        for i,c in enumerate(ms82):\n",
    "            all_columns.extend([c+j for j in columns])\n",
    "            all_data.extend(list_col[i])\n",
    "        return pd.DataFrame([all_data],columns=all_columns)\n",
    "    else:\n",
    "        for i,c in enumerate(ms82):\n",
    "            all_columns.extend([c+j for j in columns])\n",
    "        return pd.DataFrame([],columns=all_columns)\n",
    "df_minutewise = df.compute(minute_level_rip_features,windowDuration=60,startTime='0 seconds',slideDuration=5)\n",
    "ms82 = ['m_','s_','80_','20_']\n",
    "all_columns = [F.struct('start', 'end').alias('window'),'timestamp',\n",
    "               'localtime','user','version','day']\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "      'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "for i,c in enumerate(ms82):\n",
    "    all_columns.extend([c+j for j in columns])\n",
    "df_minutewise = df_minutewise.select(*all_columns)\n",
    "df_minutewise.show(2)\n",
    "schema = df_minutewise._data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.minute.features').set_description(\"Respiration Minute Features\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration Minute Level Features\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "print(stream_metadata.is_valid(),'metadata')\n",
    "ds = DataStream(data=df_minutewise._data,metadata=stream_metadata)\n",
    "ds.printSchema()\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11001696"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features').sort('window').select('window','timestamp').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----------------------+\n",
      "|window                                    |timestamp              |\n",
      "+------------------------------------------+-----------------------+\n",
      "|[2017-06-05 15:40:35, 2017-06-05 15:41:35]|2017-06-05 15:41:19.749|\n",
      "|[2017-06-05 15:40:40, 2017-06-05 15:41:40]|2017-06-05 15:41:19.749|\n",
      "|[2017-06-05 15:40:45, 2017-06-05 15:41:45]|2017-06-05 15:41:19.749|\n",
      "+------------------------------------------+-----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features').sort('window').select('window','timestamp').show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36\n",
      "+-----------------------+-----------------------+------------------------------------+--------+-------+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|timestamp              |localtime              |user                                |day     |version|start              |end                |features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+-----------------------+-----------------------+------------------------------------+--------+-------+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2018-10-01 06:30:42.741|2018-10-01 00:30:42.741|0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|1      |2018-10-01 06:30:40|2018-10-01 06:31:40|[-0.4726924171941481, 0.12288176513693069, -0.020110806799688123, -9.564790139647256, -0.8615119684848807, -6.237590786749803, -1.6308180090908357, -0.008348857328912999,, 0.7797464152521043, 0.278401970030653, 0.3395732881748961, 0.3261612388598329, -1.085122611855709, 0.3098989890439496, 0.23025333642628978, 0.17219942479426448,, -0.904842536810977, -0.5786339977671698, -0.72926582670417, -4.109505765214849, -0.5190245614966847, -4.121722357575275, -Infinity, -0.7592805073066548,, -0.6192049029784313, 0.6321273000921551, 0.4172383465494161, -4.109505765214849, -0.30881078971806475, -Infinity, -Infinity, -0.019437114214186882,]        |\n",
      "|2018-10-01 06:34:11.843|2018-10-01 00:34:11.843|0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|1      |2018-10-01 06:34:05|2018-10-01 06:35:05|[-0.38756853230399413, 0.8513826821145819, 1.200331897833235, -9.564790139647256, -0.7721838912346911, -6.237590786749803, -1.6308180090908357, -0.24521469193337697,, 0.998824799533007, 0.8845605611950409, 0.8930618486689191, 0.36061971808629634, -1.0839585366607811, 0.32777488010106215, 0.25452404175418597, -0.2713590275712855,, -0.6922152907430975, 1.320031434938286, 1.5465249315260932, -4.109505765214849, -0.4984429248627774, -5.152152946969094, -Infinity, 0.9911152012254687,, -0.7504167211237509, 0.31454901499005766, 0.1788430440651947, -4.109505765214849, 0.05218970355514351, -Infinity, -Infinity, -0.7649004381357817,]             |\n",
      "|2018-10-01 06:37:45.905|2018-10-01 00:37:45.905|0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|1      |2018-10-01 06:37:45|2018-10-01 06:38:45|[0.5903888270590283, 1.5209512075952782, 1.571830758273326, -9.564790139647256, -0.7282155541458126, -4.990072629399842, -1.6308180090908357, -0.26569126762749906,, 1.2710981620119595, 1.8242990351653432, 1.7638555801053746, 0.40827618226333107, -1.0569140243321673, -1.4150784248590305, 0.28809070009127125, -0.36011443073701604,, 0.97239751656251, 3.2432848383137967, 3.274426619537728, -4.109505765214849, -0.4787968171667748, -4.121722357575275, -Infinity, 0.7816535233997394,, 0.210241233154483, -0.18606800056705045, -0.10537133653683078, -4.109505765214849, 0.16098437276076794, -Infinity, -Infinity, -1.157206469488091,]                |\n",
      "|2018-10-01 06:51:25.144|2018-10-01 00:51:25.144|0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|1      |2018-10-01 06:51:25|2018-10-01 06:52:25|[-0.9022885887925657, -0.6287556130306786, -0.8462771578921817, -9.564790139647256, -0.7593829829683088, -4.990072629399842, -1.6308180090908357, -0.44824931934753726,, -0.8710832104221, -1.0620899045651073, -1.0328953280678257, 0.40827618226333107, -1.0738430797703309, -1.4150784248590305, 0.28809070009127125, -0.5420230137917075,, -0.9639349724427452, -0.8125683198511023, -0.9647914595186433, -4.109505765214849, -0.46944152778772597, -4.121722357575275, -Infinity, -0.09863384075059153,, -0.7738474029354152, 0.4653226033803825, 0.2586861900922604, -4.109505765214849, 0.06208012802838212, -Infinity, -Infinity, 1.4344419334938332,]      |\n",
      "|2018-10-01 06:55:09.802|2018-10-01 00:55:09.802|0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|1      |2018-10-01 06:55:05|2018-10-01 06:56:05|[-0.05362925155436087, 0.06906386529892758, -0.001663089414820561, -9.564790139647256, 0.38156753642663677, -4.990072629399842, -1.6308180090908357, -0.12975325418777794,, 1.064343596184124, -0.23453678231767164, -0.3840316000020014, 0.40827618226333107, 0.9211491864193335, -1.4150784248590305, 0.28809070009127125, -0.04793673729479924,, -0.48380893293463006, 0.5123468104891505, 0.43157383668204047, -4.109505765214849, -0.413309791513433, -4.121722357575275, -Infinity, -0.5149333845452293,, -0.1334087667499262, -0.5553922682257194, 0.09990720651570967, -4.109505765214849, -0.14397038183075503, -Infinity, -Infinity, -1.0696923399293692,]|\n",
      "+-----------------------+-----------------------+------------------------------------+--------+-------+-------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rip_data = CC.get_stream('org.md2k.autosense.rip.minute.features')\n",
    "activity_data = CC.get_stream('org.md2k.autosense.accel.activity.60seconds')\n",
    "\n",
    "df = rip_data.join(activity_data.drop(*['timestamp','localtime','version']),on=['user','window'],how='left')\n",
    "df = df.filter(F.col('activity')!=1)\n",
    "df = df.drop(*['activity'])\n",
    "df = df.withColumn('start',F.col('window').start)\n",
    "df = df.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "\n",
    "# df.printSchema()\n",
    "ms82 = ['m_','s_','80_','20_']\n",
    "all_columns = ['start','end','timestamp','localtime','user','version','day']\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "feature_columns = []\n",
    "for i,c in enumerate(ms82):\n",
    "    feature_columns.extend([c+j for j in columns])\n",
    "    all_columns.extend([c+j for j in columns])\n",
    "\n",
    "df = df.select(*all_columns)\n",
    "# all_columns\n",
    "# df.count()\n",
    "# CC.get_stream('org.md2k.autosense.rip.minute.features').count()\n",
    "\n",
    "feature_columns_action_std = [F.stddev(i).alias(i) for i in feature_columns]\n",
    "feature_columns_action_mean = [F.mean(i).alias(i) for i in feature_columns]\n",
    "\n",
    "std_all = df.groupBy(['user','day']).agg(*feature_columns_action_std)\n",
    "mean_all = df.groupBy(['user','day']).agg(*feature_columns_action_mean)\n",
    "\n",
    "mean_all = mean_all.toPandas()\n",
    "std_all = std_all.toPandas()\n",
    "mean_all.set_index(['user','day'],inplace=True)\n",
    "std_all.set_index(['user','day'],inplace=True)\n",
    "\n",
    "basic_schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType()),\n",
    "            StructField(\"localtime\", TimestampType()),\n",
    "            StructField(\"user\", StringType()),\n",
    "            StructField(\"day\", StringType()),\n",
    "            StructField(\"version\", IntegerType()),\n",
    "            StructField(\"start\", TimestampType()),\n",
    "            StructField(\"end\", TimestampType())\n",
    "])\n",
    "\n",
    "features_list = []\n",
    "for c in feature_columns:\n",
    "    features_list.append(StructField(c, DoubleType(), True))\n",
    "features_schema = StructType(basic_schema.fields + features_list)\n",
    "\n",
    "@pandas_udf(features_schema, PandasUDFType.GROUPED_MAP)\n",
    "def standardize_rip_feature(data):\n",
    "    if data.shape[0]>10:\n",
    "        for c in feature_columns:\n",
    "            m = mean_all.loc[data.loc[0]['user'],data.loc[0]['day']][c]\n",
    "            s = std_all.loc[data.loc[0]['user'],data.loc[0]['day']][c]\n",
    "            data[c] = (data[c] - m)/s\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=all_columns)\n",
    "\n",
    "df_standardized = df.groupBy(['user','day']).apply(standardize_rip_feature)\n",
    "columns = ['m_inspiration', 'm_expiration', 'm_respiration',\n",
    "       'm_ieratio', 'm_stretch', 'm_pinspiration', 'm_pexpiration',\n",
    "       'm_rexpiration', 'm_rstretch', 's_inspiration', 's_expiration',\n",
    "       's_respiration', 's_ieratio', 's_stretch', 's_pinspiration',\n",
    "       's_pexpiration', 's_rexpiration', 's_rstretch', '80_inspiration',\n",
    "       '80_expiration', '80_respiration', '80_ieratio', '80_stretch',\n",
    "       '80_pinspiration', '80_pexpiration', '80_rexpiration', '80_rstretch',\n",
    "       '20_inspiration', '20_expiration', '20_respiration', '20_ieratio',\n",
    "       '20_stretch', '20_pinspiration', '20_pexpiration', '20_rexpiration',\n",
    "       '20_rstretch']\n",
    "print(len(feature_columns),len(columns))\n",
    "df_standardized_array = df_standardized.withColumn('features',F.array(*columns)).drop(*feature_columns)\n",
    "\n",
    "df_standardized_array.show(5,False)\n",
    "\n",
    "df_standardized_array = df_standardized_array.select(\"user\",'version','timestamp','localtime','day',\n",
    "              F.struct('start', 'end').alias('window'),'features')\n",
    "\n",
    "schema = df_standardized_array.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.minute.features.standardized.final').set_description(\"RIP features standardized\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration features standardized\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=df_standardized_array,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- localtime: timestamp (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- version: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features.standardized.final').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10153902"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features.standardized.final').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-----------------------+-------+------------------------------------+--------+--------------------+\n",
      "|timestamp              |start              |end                |localtime              |version|user                                |day     |stress_likelihood   |\n",
      "+-----------------------+-------------------+-------------------+-----------------------+-------+------------------------------------+--------+--------------------+\n",
      "|2017-07-14 22:46:12.795|2017-07-14 22:46:05|2017-07-14 22:47:05|2017-07-14 16:46:12.795|1      |047dfad3-0807-37f0-bc90-7fd9dedb9bde|20170714|0.048676089913466614|\n",
      "|2017-07-14 22:47:35.873|2017-07-14 22:47:35|2017-07-14 22:48:35|2017-07-14 16:47:35.873|1      |047dfad3-0807-37f0-bc90-7fd9dedb9bde|20170714|0.04257080774060514 |\n",
      "|2017-07-14 23:08:11.709|2017-07-14 23:08:05|2017-07-14 23:09:05|2017-07-14 17:08:11.709|1      |047dfad3-0807-37f0-bc90-7fd9dedb9bde|20170714|0.5979990117721786  |\n",
      "|2017-07-14 23:16:35.428|2017-07-14 23:16:35|2017-07-14 23:17:35|2017-07-14 17:16:35.428|1      |047dfad3-0807-37f0-bc90-7fd9dedb9bde|20170714|0.16252324830357956 |\n",
      "+-----------------------+-------------------+-------------------+-----------------------+-------+------------------------------------+--------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = CC.get_stream('org.md2k.autosense.rip.minute.features.standardized.final')\n",
    "\n",
    "# df.show(5,False)\n",
    "\n",
    "df = df.withColumn('start',F.col('window').start)\n",
    "df = df.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "\n",
    "schema = StructType([StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"stress_likelihood\", DoubleType())\n",
    "])\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "ecg_model = pickle.load(open('./models/rip_model.p','rb'))\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def ecg_r_peak(key,data):\n",
    "    if data.shape[0]>0:\n",
    "        features = []\n",
    "        for i in range(data.shape[0]):\n",
    "            features.append(np.array(data['features'][i]))\n",
    "        features = np.array(features)\n",
    "        features[features>10] = 10\n",
    "        features[features<-10] = -10\n",
    "        probs = ecg_model.predict_proba(np.nan_to_num(features))[:,1]\n",
    "        data['stress_likelihood'] = probs\n",
    "        data = data[['timestamp','start','end','version','user','day',\n",
    "                     'localtime','stress_likelihood']]\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['timestamp','version','user','day',\n",
    "                                        'localtime','stress_likelihood','start','end'])\n",
    "\n",
    "df_stress = df.compute(ecg_r_peak,windowDuration=6000,startTime='0 seconds')\n",
    "\n",
    "df_stress.show(4,False)\n",
    "\n",
    "df_stress = df_stress.select(\"user\",'version','timestamp','localtime','day',\n",
    "              F.struct('start', 'end').alias('window'),'stress_likelihood')\n",
    "\n",
    "schema = df_stress._data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.stress.likelihood').set_description(\"Stress from Respiration\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration stress likelihood\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=df_stress._data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: double (nullable = true)\n",
      " |-- localtime: double (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- stress_likelihood: double (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- version: integer (nullable = false)\n",
      " |-- start: double (nullable = true)\n",
      " |-- end: double (nullable = true)\n",
      " |-- likelihood_mean: integer (nullable = false)\n",
      "\n",
      "+-----------------------+-----------------------+-------+------------------------------------+--------+-------------------+-------+---------------+------------------------------------------+\n",
      "|timestamp              |localtime              |version|user                                |day     |stress_likelihood  |imputed|likelihood_mean|window                                    |\n",
      "+-----------------------+-----------------------+-------+------------------------------------+--------+-------------------+-------+---------------+------------------------------------------+\n",
      "|2018-10-01 06:08:20.841|2018-10-01 00:08:20.841|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|0.03634472494972216|0      |1.0            |[2018-10-01 06:07:35, 2018-10-01 06:08:35]|\n",
      "|2018-10-01 06:08:20.841|2018-10-01 00:08:20.841|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|0.0379795545167536 |0      |1.0            |[2018-10-01 06:07:40, 2018-10-01 06:08:40]|\n",
      "|2018-10-01 06:08:20.841|2018-10-01 00:08:20.841|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|0.04018543316769786|0      |1.0            |[2018-10-01 06:07:45, 2018-10-01 06:08:45]|\n",
      "|2018-10-01 06:08:20.841|2018-10-01 00:08:20.841|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|0.03185017780927825|0      |1.0            |[2018-10-01 06:07:50, 2018-10-01 06:08:50]|\n",
      "|2018-10-01 06:08:20.841|2018-10-01 00:08:20.841|1      |0a967d9d-c5dd-3969-a50d-4428ffbad7d4|20181001|0.03185017780927825|0      |1.0            |[2018-10-01 06:07:55, 2018-10-01 06:08:55]|\n",
      "+-----------------------+-----------------------+-------+------------------------------------+--------+-------------------+-------+---------------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress_data = CC.get_stream('org.md2k.autosense.rip.stress.likelihood')\n",
    "stress_data = stress_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "stress_data = stress_data.withColumn('start',F.col('window').start)\n",
    "stress_data = stress_data.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "stress_data = stress_data.withColumn('start',F.col('start').cast('double'))\n",
    "stress_data = stress_data.withColumn('end',F.col('end').cast('double'))\n",
    "stress_data = stress_data.withColumn('localtime',F.col('localtime').cast('double'))\n",
    "stress_data = stress_data.withColumn('timestamp',F.col('timestamp').cast('double'))\n",
    "stress_data  = stress_data.withColumn('likelihood_mean',F.lit(1))\n",
    "stress_data.printSchema()\n",
    "\n",
    "schema = StructType([StructField(\"timestamp\", DoubleType()),\n",
    "    StructField(\"start\", DoubleType()),\n",
    "    StructField(\"end\", DoubleType()),\n",
    "    StructField(\"localtime\", DoubleType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"stress_likelihood\", DoubleType()),\n",
    "    StructField(\"imputed\", IntegerType()),\n",
    "    StructField(\"likelihood_mean\", DoubleType())\n",
    "])\n",
    "step = 5\n",
    "smoothing = int(60*2/step)\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def impute_forwardfill(data):\n",
    "    data = data.sort_values('start').reset_index(drop=True)\n",
    "    start = data['start'][0]\n",
    "    all_rows = []\n",
    "    for i,row in data.iterrows():\n",
    "        if row['start']==start:\n",
    "            all_rows.append([row['timestamp'],row['localtime'],row['start'],row['end'],\n",
    "                             row['version'],row['user'],row['day'],row['stress_likelihood'],\n",
    "                             row['likelihood_mean'],0])\n",
    "            start = row['end']\n",
    "        else:\n",
    "            k = 1\n",
    "            while (start+k*step)<=row['start']:\n",
    "                all_rows.append([data.loc[i-1]['timestamp']+k*step,data.loc[i-1]['localtime']+k*step,\n",
    "                                 data.loc[i-1]['start']+k*step,data.loc[i-1]['end']+k*step,\n",
    "                                 row['version'],row['user'],row['day'],\n",
    "                                 data.loc[i-1]['stress_likelihood'],data.loc[i-1]['likelihood_mean'],1])\n",
    "                k+=1\n",
    "            all_rows.append([row['timestamp'],row['localtime'],row['start'],row['end'],\n",
    "                             row['version'],row['user'],row['day'],row['stress_likelihood'],\n",
    "                             row['likelihood_mean'],0])\n",
    "            start = row['end']    \n",
    "    return pd.DataFrame(all_rows,columns=['timestamp','localtime','start','end',\n",
    "                                          'version','user','day','stress_likelihood','likelihood_mean','imputed'])\n",
    "\n",
    "stress_imputed_data = stress_data.groupBy(['user','day']).apply(impute_forwardfill)\n",
    "stress_imputed_data = stress_imputed_data.withColumn('start',F.col('start').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('end',F.col('end').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('localtime',F.col('localtime').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('timestamp',F.col('timestamp').cast('timestamp'))\n",
    "cols = list(stress_imputed_data.columns)\n",
    "cols.append(F.struct('start', 'end').alias('window'))\n",
    "cols.remove('start')\n",
    "cols.remove('end')\n",
    "stress_imputed_data = stress_imputed_data.select(*cols)\n",
    "stress_imputed_data.show(5,False)\n",
    "schema = stress_imputed_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.stress.likelihood.ffill').set_description(\"rip stress imputed\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"stress forward fill imputed\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=stress_imputed_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- localtime: timestamp (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- stress_likelihood: double (nullable = true)\n",
      " |-- imputed: integer (nullable = true)\n",
      " |-- likelihood_mean: double (nullable = true)\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- version: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.stress.likelihood.ffill').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------+--------------------+-------+------------------------------------+-------+-------------+-------------+----+-------+\n",
      "|timestamp       |localtime       |day     |stress_probability  |imputed|user                                |version|start        |end          |hour|weekday|\n",
      "+----------------+----------------+--------+--------------------+-------+------------------------------------+-------+-------------+-------------+----+-------+\n",
      "|1.497938430723E9|1.497916830723E9|20170620|0.09119286521781543 |0      |fd053cef-b5a5-3fe6-a158-b6702bce0665|1      |1.49793842E9 |1.49793848E9 |0   |Tuesday|\n",
      "|1.497938430723E9|1.497916830723E9|20170620|0.03667934123607663 |0      |fd053cef-b5a5-3fe6-a158-b6702bce0665|1      |1.497938425E9|1.497938485E9|0   |Tuesday|\n",
      "|1.497938430723E9|1.497916830723E9|20170620|0.053575755675947086|0      |fd053cef-b5a5-3fe6-a158-b6702bce0665|1      |1.49793843E9 |1.49793849E9 |0   |Tuesday|\n",
      "|1.497938439659E9|1.497916839659E9|20170620|0.08419969059885672 |0      |fd053cef-b5a5-3fe6-a158-b6702bce0665|1      |1.497938435E9|1.497938495E9|0   |Tuesday|\n",
      "|1.49793844478E9 |1.49791684478E9 |20170620|0.1772748484338943  |0      |fd053cef-b5a5-3fe6-a158-b6702bce0665|1      |1.49793844E9 |1.4979385E9  |0   |Tuesday|\n",
      "+----------------+----------------+--------+--------------------+-------+------------------------------------+-------+-------------+-------------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,GridSearchCV,KFold,train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "def best_fit_slope(ys):\n",
    "    return np.mean(np.diff(ys))\n",
    "\n",
    "def get_features(y):\n",
    "    tmp = y[-1,0]\n",
    "    return [tmp,\n",
    "#             np.median(y),\n",
    "#             np.std(y),\n",
    "#             np.percentile(y,80),\n",
    "#             np.percentile(y,20),\n",
    "            best_fit_slope(y[:,0])]\n",
    "\n",
    "def get_trained_model(X_train,y_train):\n",
    "    paramGrid = {'rf__n_neighbors':[3,4,5,6,7,8,9],\n",
    "                 }\n",
    "    clf = Pipeline([('rf',KNeighborsRegressor())])\n",
    "    gkf = KFold(n_splits=5)\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=gkf.split(X_train),\n",
    "                               scoring='r2',verbose=5)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf\n",
    "\n",
    "weekday_dict = {'Wednesday':5,\n",
    "                'Saturday':1,\n",
    "                'Thursday':6,\n",
    "                'Tuesday':4,\n",
    "                'Friday':0,\n",
    "                'Sunday':2,\n",
    "                'Monday':3}\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", DoubleType()),\n",
    "    StructField(\"start\", DoubleType()),\n",
    "    StructField(\"end\", DoubleType()),\n",
    "    StructField(\"localtime\", DoubleType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"weekday\", StringType()),\n",
    "    StructField(\"hour\", IntegerType()),\n",
    "    StructField(\"stress_probability\", DoubleType()),\n",
    "    StructField(\"imputed\", IntegerType()),\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fillup_imputation(data):\n",
    "    data = data.sort_values('start').reset_index(drop=True)\n",
    "    data['stress_probability'] = data['stress_probability'].rolling(window=lookback).mean()\n",
    "    data = data.dropna().sort_values('start').reset_index(drop=True)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i,row in data.iterrows():\n",
    "        if i<lookback:\n",
    "            continue\n",
    "        if row['imputed'] in [1]:\n",
    "            continue\n",
    "        prev_imputed = list(data['imputed'].iloc[i-lookback:i].values)\n",
    "        if prev_imputed.count(1)<=.33*lookback:\n",
    "            feature = data[['stress_probability','imputed','timestamp']].iloc[i-lookback:i]\n",
    "#             .sort_values('timestamp').reset_index(drop=True)\n",
    "            feature = feature[feature.imputed.isin([0,2])][['stress_probability','timestamp']].values\n",
    "            X.append(np.array(get_features(feature)+[row['hour'],weekday_dict[row['weekday']]]))\n",
    "            y.append(row['stress_probability'])\n",
    "    if len(X)<100:\n",
    "        return data\n",
    "    X = np.array(X)\n",
    "    X_s = X[:,:-2]\n",
    "    X_hour = X[:,-2:-1].reshape(-1,1)\n",
    "    X_weekday = X[:,-1:].reshape(-1,1)\n",
    "    clf_hour = OneHotEncoder().fit(np.arange(0,25,1).reshape(-1,1))\n",
    "    clf_week_day = OneHotEncoder().fit(np.arange(0,7,1).reshape(-1,1))\n",
    "    X_hour = clf_hour.transform(X_hour).todense().reshape(X.shape[0],-1)\n",
    "    X_weekday = clf_week_day.transform(X_weekday).todense().reshape(X.shape[0],-1)\n",
    "    X = np.concatenate([X_s,X_hour,X_weekday],axis=1)\n",
    "    y = np.array(y)\n",
    "    print(X.shape)\n",
    "    clf = get_trained_model(X,y)\n",
    "    start = data[data.imputed==1].shape[0]+1\n",
    "#     return data\n",
    "    count = 0\n",
    "    while data[data.imputed==1].shape[0]<start and data[data.imputed==1].shape[0]>0:\n",
    "        start = data[data.imputed==1].shape[0]\n",
    "        print(start,count)\n",
    "        X = []\n",
    "        y = []\n",
    "        index = []\n",
    "        for i,row in data.iterrows():\n",
    "            if i<lookback:\n",
    "                continue\n",
    "            if row['imputed'] in [0,2]:\n",
    "                continue\n",
    "            prev_imputed = list(data['imputed'].iloc[i-lookback:i].values)\n",
    "            if prev_imputed.count(1)<=.33*lookback:\n",
    "                feature = data[['stress_probability','imputed','timestamp']].iloc[i-lookback:i]\n",
    "#                 .sort_values('timestamp').reset_index(drop=True)\n",
    "                feature = feature[feature.imputed.isin([0,2])][['stress_probability','timestamp']].values\n",
    "                X.append(np.array(get_features(feature)+[row['hour'],weekday_dict[row['weekday']]]))\n",
    "                y.append(row['stress_probability'])\n",
    "                index.append(i)\n",
    "        count+=1\n",
    "        if len(X)==0:\n",
    "            break\n",
    "        X = np.array(X)\n",
    "        X_s = X[:,:-2]\n",
    "        X_hour = X[:,-2:-1].reshape(-1,1)\n",
    "        X_weekday = X[:,-1:].reshape(-1,1)\n",
    "        X_hour = clf_hour.transform(X_hour).todense().reshape(X.shape[0],-1)\n",
    "        X_weekday = clf_week_day.transform(X_weekday).todense().reshape(X.shape[0],-1)\n",
    "        X = np.concatenate([X_s,X_hour,X_weekday],axis=1)\n",
    "#         break\n",
    "        data.loc[index,'stress_probability'] = clf.predict(X).reshape(-1)\n",
    "        data.loc[index,'imputed'] = 2\n",
    "    return data\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fillup_imputation_simple(data):\n",
    "    data = data.sort_values('start').reset_index(drop=True)\n",
    "    data['stress_probability'] = data['stress_probability'].rolling(window=lookback).mean()\n",
    "    data = data.dropna().sort_values('start').reset_index(drop=True)\n",
    "    if data.shape[0]<100:\n",
    "        return data\n",
    "    Xs = []\n",
    "    Xweekday = []\n",
    "    Xhour = []\n",
    "    index = []\n",
    "    for i,row in data.iterrows():\n",
    "        if row['imputed'] in [1]:\n",
    "            Xs.append(np.nan)\n",
    "            index.append(i)\n",
    "        else:\n",
    "            Xs.append(row['stress_probability'])\n",
    "        Xweekday.append(weekday_dict[row['weekday']])\n",
    "        Xhour.append(row['hour'])\n",
    "    X_s = np.array(Xs).reshape(-1,1)\n",
    "    clf_hour = OneHotEncoder().fit(np.arange(0,25,1).reshape(-1,1))\n",
    "    clf_week_day = OneHotEncoder().fit(np.arange(0,7,1).reshape(-1,1))\n",
    "    X_hour = clf_hour.transform(np.array(Xhour).reshape(-1,1)).todense().reshape(X_s.shape[0],-1)\n",
    "    X_weekday = clf_week_day.transform(np.array(Xweekday).reshape(-1,1)).todense().reshape(X_s.shape[0],-1)\n",
    "    X = np.concatenate([X_hour,X_weekday,X_s],axis=1)\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X = imputer.fit_transform(X)[np.array(index)]\n",
    "    data.loc[index,'stress_probability'] = X[:,-1]\n",
    "    data.loc[index,'imputed'] = 2\n",
    "    return data\n",
    "\n",
    "step = 5\n",
    "lookback = int(3*(60/5))\n",
    "stream_name = 'org.md2k.autosense.rip.stress.likelihood.ffill'\n",
    "stress_data = CC.get_stream(stream_name)\n",
    "stress_data = stress_data.withColumnRenamed('stress_likelihood','stress_probability').drop('likelihood_mean')\n",
    "# stress_data = stress_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "stress_data = stress_data.withColumn('start',F.col('window').start)\n",
    "stress_data = stress_data.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "stress_data = stress_data.withColumn('start',F.col('start').cast('double'))\n",
    "stress_data = stress_data.withColumn('end',F.col('end').cast('double'))\n",
    "stress_data = stress_data.withColumn('hour',F.hour('localtime'))\n",
    "stress_data = stress_data.withColumn('weekday',F.date_format('localtime','EEEE'))\n",
    "stress_data = stress_data.withColumn('localtime',F.col('localtime').cast('double'))\n",
    "stress_data = stress_data.withColumn('timestamp',F.col('timestamp').cast('double'))\n",
    "stress_data.show(5,False)\n",
    "data_1 = stress_data._data.toPandas()\n",
    "stress_imputed_data = stress_data.groupBy(['user','version']).apply(fillup_imputation_simple)\n",
    "stress_imputed_data = stress_imputed_data.withColumn('start',F.col('start').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('end',F.col('end').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('localtime',F.col('localtime').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('timestamp',F.col('timestamp').cast('timestamp'))\n",
    "cols = list(stress_imputed_data.columns)\n",
    "cols.append(F.struct('start', 'end').alias('window'))\n",
    "cols.remove('start')\n",
    "cols.remove('end')\n",
    "cols.remove('hour')\n",
    "cols.remove('weekday')\n",
    "cols.remove('day')\n",
    "stress_imputed_data = stress_imputed_data.select(*cols)\n",
    "# stress_imputed_data.show(100,False) \n",
    "schema = stress_imputed_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.stress.likelihood.imputed').set_description(\"stress imputed\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"stress imputed in KNN\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=stress_imputed_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)\n",
    "# stream_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa1 = fillup_imputation_simple(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(aa1['imputed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(aa1['stress_probability'][18000:19900])\n",
    "# plt.plot(aa1['imputed'][18000:19900])\n",
    "plt.hist(aa1[aa1.imputed.isin([0])]['stress_probability'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream('org.md2k.autosense.rip.stress.likelihood.imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19245841"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(10,3),columns=['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['a','b']].iloc[8-3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df.iterrows():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data,open('./data/rice_1st_version_rip_only.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_data = CC.get_stream('org.md2k.autosense.rip.stress.likelihood')._data.toPandas()\n",
    "# quality_data = CC.get_stream('org.md2k.autosense.rip.quality.60seconds')._data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i,df in data.groupby(['user','day']):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    # plt.plot(stress_data['timestamp'][stress_data.user==stress_data['user'].iloc[20000]][:300],\n",
    "    #          stress_data['stress_likelihood'][stress_data.user==stress_data['user'].iloc[20000]][:300])\n",
    "    plt.plot(df['stress_probability'][:10000])\n",
    "    plt.show()\n",
    "# stress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(stress_data,open('./rice_data/stress_rip_only_no_c6_nw.p','wb'))\n",
    "pickle.dump(quality_data,open('./rice_data/rip_quality_60_seconds_nw.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CC.get_stream('org.md2k.motionsensehrv.ppg.left.stress.md2k_aa_rice.imputed.ffill')\n",
    "emas = CC.get_stream(\"perceived.stress.score--org.md2k.ema_scheduler--phone\").drop(*['timestamp',\n",
    "                                                                                    'localtime',\n",
    "                                                                                    'version'])\n",
    "all_stress = ds.join(emas,on=['user','window'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
