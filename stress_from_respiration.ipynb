{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cerebralcortex.util.helper_methods import get_study_names\n",
    "# sn = get_study_names(\"/home/jupyter/cc3_conf/\")\n",
    "# print(sn)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, LongType, \\\n",
    "DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "ModuleMetadata\n",
    "import pandas as pd\n",
    "from cerebralcortex import Kernel\n",
    "study_name = 'md2k_aa_rice'\n",
    "CC = Kernel(\"/home/jupyter/cc3_conf/\", study_name=study_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_respiration_quality(data,\n",
    "                                window_size=3,\n",
    "                                outlier_threshold_high = 4000,\n",
    "                                outlier_threshold_low = 20,\n",
    "                                slope_threshold = 300,\n",
    "                                eck_threshold_band_loose = 175,\n",
    "                                eck_threshold_band_off = 20,\n",
    "                                minimum_expected_samples = 3*(0.33)*21.33,\n",
    "                                acceptable_outlier_percent = 34):\n",
    "    \n",
    "    def get_metadata(stream_name):\n",
    "        stream_metadata = Metadata()\n",
    "        stream_metadata.set_name(stream_name).set_description(\"Chest Respiration quality per sample 3 seconds\") \\\n",
    "            .add_dataDescriptor(\n",
    "            DataDescriptor().set_name(\"quality\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "            \"Respiration data quality\").set_attribute('Loose/Improper Attachment','Electrode Displacement').set_attribute('Sensor off Body',\\\n",
    "            'Autosense not worn').set_attribute('Battery down/Disconnected', \\\n",
    "            'No data is present - Can be due to battery down or sensor disconnection').set_attribute('Interittent Data Loss', \\\n",
    "             'Not enough samples are present').set_attribute('Acceptable','Good Quality')) \\\n",
    "            .add_dataDescriptor(\n",
    "            DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "            \"respiration sample value\")) \\\n",
    "            .add_module(\n",
    "            ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/rip_quality.ipynb\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "                \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "        return stream_metadata\n",
    "    \n",
    "    def get_quality(data):\n",
    "        data_quality_band_loose = 'Loose/Improper Attachment'\n",
    "        data_quality_not_worn = 'Sensor off Body'\n",
    "        data_quality_band_off = 'Battery down/Disconnected'\n",
    "        data_quality_missing = 'Interittent Data Loss' \n",
    "        data_quality_good = 'Acceptable'\n",
    "        if (len(data)== 0):\n",
    "            return data_quality_band_off\n",
    "        range_data = max(data)-min(data)\n",
    "        if range_data<=eck_threshold_band_off:\n",
    "            return data_quality_not_worn\n",
    "        if (len(data)<=minimum_expected_samples) :\n",
    "            return data_quality_missing\n",
    "        if range_data<=eck_threshold_band_loose:\n",
    "            return data_quality_band_loose\n",
    "\n",
    "        outlier_counts = 0 \n",
    "        for i in range(0,len(data)):\n",
    "            im,ip  = i,i\n",
    "            if i==0:\n",
    "                im = len(data)-1\n",
    "            else:\n",
    "                im = i-1\n",
    "            if i == len(data)-1:\n",
    "                ip = 0\n",
    "            else:\n",
    "                ip = ip+1\n",
    "            stuck = ((data[i]==data[im]) and (data[i]==data[ip]))\n",
    "            flip = ((abs(data[i]-data[im])>((int(outlier_threshold_high)))) or (abs(data[i]-data[ip])>((int(outlier_threshold_high)))))\n",
    "            disc = ((abs(data[i]-data[im])>((int(slope_threshold)))) and (abs(data[i]-data[ip])>((int(slope_threshold)))))\n",
    "            if disc:\n",
    "                outlier_counts += 1\n",
    "            elif stuck:\n",
    "                outlier_counts +=1\n",
    "            elif flip:\n",
    "                outlier_counts +=1\n",
    "            elif data[i] >= outlier_threshold_high:\n",
    "                outlier_counts +=1\n",
    "            elif data[i]<= outlier_threshold_low:\n",
    "                outlier_counts +=1\n",
    "        if (100*outlier_counts>acceptable_outlier_percent*len(data)):\n",
    "            return data_quality_band_loose\n",
    "        return data_quality_good\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType()),\n",
    "        StructField(\"localtime\", TimestampType()),\n",
    "        StructField(\"version\", IntegerType()),\n",
    "        StructField(\"user\", StringType()),\n",
    "        StructField(\"quality\", StringType()),\n",
    "        StructField(\"respiration\", DoubleType())\n",
    "    ])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def ecg_quality(key,data):\n",
    "        data['quality'] = ''\n",
    "        if data.shape[0]>0:\n",
    "            data = data.sort_values('timestamp')\n",
    "            data['quality'] = get_quality(list(data['respiration']))\n",
    "        return data\n",
    "\n",
    "    stream_name = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "    rip_quality_stream = rip.compute(ecg_quality,windowDuration=window_size,startTime='0 seconds')\n",
    "    data = rip_quality_stream._data\n",
    "    ds = DataStream(data=data,metadata=get_metadata(stream_name))\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rip_stream = 'respiration--org.md2k.autosense--autosense_chest--chest'\n",
    "rip = CC.get_stream(rip_stream)\n",
    "ds = compute_respiration_quality(rip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.quality.per.sample').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"quality\", StringType())\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def ecg_quality_60(key,data):\n",
    "    if data.shape[0]>0:\n",
    "        quals = list(data['quality'].values)\n",
    "        qual = Counter(quals).most_common()[0][0]\n",
    "        data = data[:1].reset_index(drop=True)\n",
    "        data['quality'].set_value(0,qual)\n",
    "        data['start'] = [key[2]['start']]\n",
    "        data['end'] = [key[2]['end']]\n",
    "        return data[['start','end','timestamp','localtime','version','user','quality']]\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['start','end','timestamp','localtime','version','user','quality'])\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.quality.60seconds'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Chest RIP quality 60 seconds\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"quality\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"ECG data quality\").set_attribute('Loose/Improper Attachment','Electrode Displacement').set_attribute('Sensor off Body',\\\n",
    "    'Autosense not worn').set_attribute('Battery down/Disconnected', \\\n",
    "    'No data is present - Can be due to battery down or sensor disconnection').set_attribute('Interittent Data Loss', \\\n",
    "     'Not enough samples are present').set_attribute('Acceptable','Good Quality')) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"window\").set_type(\"struct\").set_attribute(\"description\", \\\n",
    "    \"window start and end time in UTC\").set_attribute('start', \\\n",
    "    'start of 1 minute window').set_attribute('end','end of 1 minute window')) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "\n",
    "stream_metadata.is_valid()\n",
    "ecg_stream = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "ecg = CC.get_stream(ecg_stream)\n",
    "ecg_quality_stream = ecg.compute(ecg_quality_60,windowDuration=60,startTime='0 seconds')\n",
    "ecg_quality_stream = ecg_quality_stream.select('timestamp', F.struct('start', 'end').alias('window'),\n",
    "                                   'localtime','quality','user','version')\n",
    "ecg_quality_stream.printSchema()\n",
    "data = ecg_quality_stream._data\n",
    "ds = DataStream(data=data,metadata=stream_metadata)\n",
    "CC.save_stream(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.quality.60seconds').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "# TODO: What is this?\n",
    "class Quality(Enum):\n",
    "    ACCEPTABLE = 1\n",
    "    UNACCEPTABLE = 0\n",
    "from operator import itemgetter\n",
    "\n",
    "def smooth(data:np.ndarray,\n",
    "           span: int = 5)-> np.ndarray:\n",
    "    \"\"\"\n",
    "\n",
    "    :rtype: object\n",
    "    :param data:\n",
    "    :param span:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return []\n",
    "    # plt.figure()\n",
    "    # plt.plot(data[:,1])\n",
    "    sample = data[:,1]\n",
    "    sample_middle = np.convolve(sample, np.ones(span, dtype=int), 'valid') / span\n",
    "    divisor = np.arange(1, span - 1, 2)\n",
    "    sample_start = np.cumsum(sample[:span - 1])[::2] / divisor\n",
    "    sample_end = (np.cumsum(sample[:-span:-1])[::2] / divisor)[::-1]\n",
    "    sample_smooth = np.concatenate((sample_start, sample_middle, sample_end))\n",
    "    data[:,1] = sample_smooth\n",
    "\n",
    "    # plt.plot(data[:,1])\n",
    "    # plt.show()\n",
    "    return data\n",
    "\n",
    "\n",
    "def moving_average_curve(data: np.ndarray,\n",
    "                         window_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Moving average curve from filtered (using moving average) samples.\n",
    "\n",
    "    :rtype: object\n",
    "    :return: mac\n",
    "    :param data:\n",
    "    :param window_length:\n",
    "    \"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return []\n",
    "    # plt.figure()\n",
    "    # plt.plot(data[:,1])\n",
    "    sample = data[:,1]\n",
    "    for i in range(window_length, len(sample) - (window_length + 1)):\n",
    "        sample_avg = np.mean(sample[i - window_length:i + window_length + 1])\n",
    "        data[i,1] = sample_avg\n",
    "\n",
    "    # plt.plot(data[:,1])\n",
    "    # plt.show()\n",
    "    return data[np.array(range(window_length, len(sample) - (window_length + 1)))]\n",
    "\n",
    "\n",
    "def up_down_intercepts(data: np.ndarray,\n",
    "                       mac: np.ndarray,\n",
    "                       data_start_time_to_index: dict) -> [np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns Up and Down Intercepts.\n",
    "    Moving Average Centerline curve intersects breath cycle twice. Once in the inhalation branch\n",
    "    (Up intercept) and in the exhalation branch (Down intercept).\n",
    "\n",
    "    :param data_start_time_to_index:\n",
    "    :rtype: object\n",
    "    :return up_intercepts, down_intercepts:\n",
    "    :param data:\n",
    "    :param mac:\n",
    "    \"\"\"\n",
    "\n",
    "    up_intercepts = []\n",
    "    down_intercepts = []\n",
    "\n",
    "    subsets = []\n",
    "    for i in range(len(mac)):\n",
    "        data_index = data_start_time_to_index[mac[i,0]]\n",
    "        subsets.append(data[data_index,1])\n",
    "    # plt.plot(data[:,0],data[:,1])\n",
    "    # plt.plot(mac[:,0],mac[:,1])\n",
    "    # # plt.plot(mac[:,0],subsets)\n",
    "    # plt.show()\n",
    "    if len(subsets) == len(mac):\n",
    "        for i in range(len(mac) - 1):\n",
    "            if subsets[i] <= mac[i,1] and mac[i + 1,1] <= subsets[i + 1]:\n",
    "                up_intercepts.append(mac[i + 1])\n",
    "            elif subsets[i] >= mac[i,1] and mac[i + 1,1] >= subsets[i + 1]:\n",
    "                down_intercepts.append(mac[i + 1])\n",
    "    else:\n",
    "        raise Exception(\"Data sample not found at Moving Average Curve.\")\n",
    "\n",
    "    return up_intercepts, down_intercepts\n",
    "\n",
    "def filter_intercept_outlier(up_intercepts: List,\n",
    "                             down_intercepts: List) -> [List,List]:\n",
    "    \"\"\"\n",
    "    Remove two or more consecutive up or down intercepts.\n",
    "\n",
    "    :rtype: object\n",
    "    :return up_intercepts_updated, down_intercepts_updated:\n",
    "    :param up_intercepts:\n",
    "    :param down_intercepts:\n",
    "    \"\"\"\n",
    "\n",
    "    up_intercepts_filtered = []\n",
    "    down_intercepts_filtered = []\n",
    "\n",
    "    for index in range(len(down_intercepts) - 1):\n",
    "        up_intercepts_between_down_intercepts = []\n",
    "        for ui in up_intercepts:\n",
    "            if down_intercepts[index][0] <= ui[0] <= down_intercepts[index + 1][0]:\n",
    "                up_intercepts_between_down_intercepts.append(ui)\n",
    "\n",
    "        if len(up_intercepts_between_down_intercepts) > 0:\n",
    "            up_intercepts_filtered.append(up_intercepts_between_down_intercepts[-1])\n",
    "\n",
    "    up_intercepts_after_down = [ui for ui in up_intercepts if ui[0] > down_intercepts[-1][0]]\n",
    "    if len(up_intercepts_after_down) > 0:\n",
    "        up_intercepts_filtered.append(up_intercepts_after_down[-1])\n",
    "\n",
    "    down_intercepts_before_up = [di for di in down_intercepts if di[0] < up_intercepts_filtered[0][0]]\n",
    "    if len(down_intercepts_before_up) > 0:\n",
    "        down_intercepts_filtered.append(down_intercepts_before_up[-1])\n",
    "\n",
    "    for index in range(len(up_intercepts_filtered) - 1):\n",
    "        down_intercepts_between_up_intercepts = []\n",
    "        for di in down_intercepts:\n",
    "            if up_intercepts_filtered[index][0] <= di[0] <= up_intercepts_filtered[index + 1][0]:\n",
    "                down_intercepts_between_up_intercepts.append(di)\n",
    "\n",
    "        if len(down_intercepts_between_up_intercepts) > 0:\n",
    "            down_intercepts_filtered.append(down_intercepts_between_up_intercepts[-1])\n",
    "\n",
    "    down_intercepts_after_up = [di for di in down_intercepts if di[0] > up_intercepts_filtered[-1][0]]\n",
    "    if len(down_intercepts_after_up) > 0:\n",
    "        down_intercepts_filtered.append(down_intercepts_after_up[-1])\n",
    "\n",
    "    up_intercepts_truncated = []\n",
    "    for ui in up_intercepts_filtered:\n",
    "        if ui[0] >= down_intercepts_filtered[0][0]:\n",
    "            up_intercepts_truncated.append(ui)\n",
    "\n",
    "    min_length = min(len(up_intercepts_truncated), len(down_intercepts_filtered))\n",
    "\n",
    "    up_intercepts_updated = up_intercepts_truncated[:min_length]\n",
    "    down_intercepts_updated = down_intercepts_filtered[:min_length]\n",
    "\n",
    "    return up_intercepts_updated, down_intercepts_updated\n",
    "\n",
    "\n",
    "\n",
    "def generate_peak_valley(up_intercepts: List,\n",
    "                         down_intercepts: List,\n",
    "                         data: np.ndarray) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Compute peak valley from up intercepts and down intercepts indices.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks, valleys:\n",
    "    :param up_intercepts:\n",
    "    :param down_intercepts:\n",
    "    :param data:\n",
    "    \"\"\"\n",
    "    peaks = []\n",
    "    valleys = []\n",
    "\n",
    "    last_iterated_index = 0\n",
    "    for i in range(len(down_intercepts) - 1):\n",
    "        peak = None\n",
    "        valley = None\n",
    "\n",
    "        for j in range(last_iterated_index, len(data)):\n",
    "            if down_intercepts[i][0] <= data[j][0] <= up_intercepts[i][0]:\n",
    "                if valley is None or data[j][1] < valley[1]:\n",
    "                    valley = data[j]\n",
    "            elif up_intercepts[i][0] <= data[j][0] <= down_intercepts[i + 1][0]:\n",
    "                if peak is None or data[j][1] > peak[1]:\n",
    "                    peak = data[j]\n",
    "            elif data[j][0] > down_intercepts[i + 1][0]:\n",
    "                last_iterated_index = j\n",
    "                break\n",
    "        if peak is None or valley is None:\n",
    "            continue\n",
    "\n",
    "        valleys.append(valley)\n",
    "        peaks.append(peak)\n",
    "\n",
    "    return peaks, valleys\n",
    "\n",
    "\n",
    "def correct_valley_position(peaks: List,\n",
    "                            valleys: List,\n",
    "                            up_intercepts: List,\n",
    "                            data: np.ndarray,\n",
    "                            data_start_time_to_index: dict) -> List:\n",
    "    \"\"\"\n",
    "    Correct Valley position by locating actual valley using maximum slope algorithm which is\n",
    "    located between current valley and following peak.\n",
    "\n",
    "    Algorithm - push valley towards right:\n",
    "    Search for points lies in between current valley and following Up intercept.\n",
    "    Calculate slopes at those points.\n",
    "    Ensure that valley resides at the begining of inhalation cycle where inhalation slope is maximum.\n",
    "\n",
    "    :rtype: object\n",
    "    :return valley_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param up_intercepts:\n",
    "    :param data:\n",
    "    :param data_start_time_to_index: hash table for data where start_time is key, index is value\n",
    "    \"\"\"\n",
    "    valley_updated = valleys.copy()\n",
    "    for i in range(len(valleys)):\n",
    "        if valleys[i][0] < up_intercepts[i][0] < peaks[i][0]:\n",
    "            up_intercept = up_intercepts[i]\n",
    "\n",
    "            if valleys[i][0] not in data_start_time_to_index or up_intercept[0] not in data_start_time_to_index:\n",
    "                exception_message = 'Data has no start time for valley or up intercept start time at index ' + str(i)\n",
    "                raise Exception(exception_message)\n",
    "            else:\n",
    "                valley_index = data_start_time_to_index[valleys[i][0]]\n",
    "                up_intercept_index = data_start_time_to_index[up_intercept[0]]\n",
    "                data_valley_to_ui = data[valley_index: up_intercept_index + 1]\n",
    "                sample_valley_to_ui = data_valley_to_ui[:,1]\n",
    "\n",
    "                slope_at_samples = np.diff(sample_valley_to_ui)\n",
    "\n",
    "                consecutive_positive_slopes = [-1] * len(slope_at_samples)\n",
    "\n",
    "                for j in range(len(slope_at_samples)):\n",
    "                    slopes_subset = slope_at_samples[j:]\n",
    "                    if all(slope > 0 for slope in slopes_subset):\n",
    "                        consecutive_positive_slopes[j] = len(slopes_subset)\n",
    "\n",
    "                if any(no_con_slope > 0 for no_con_slope in consecutive_positive_slopes):\n",
    "                    indices_max_pos_slope = []\n",
    "                    for k in range(len(consecutive_positive_slopes)):\n",
    "                        if consecutive_positive_slopes[k] == max(consecutive_positive_slopes):\n",
    "                            indices_max_pos_slope.append(k)\n",
    "                    valley_updated[i] = data_valley_to_ui[indices_max_pos_slope[-1]]\n",
    "\n",
    "        else:\n",
    "            # TODO: discuss whether raise exception or not\n",
    "            # Up intercept at index i is not between valley and peak at index i.\n",
    "            break\n",
    "\n",
    "    return valley_updated\n",
    "\n",
    "\n",
    "def correct_peak_position(peaks: List,\n",
    "                          valleys: List,\n",
    "                          up_intercepts: List,\n",
    "                          data: np.ndarray,\n",
    "                          max_amplitude_change_peak_correction: float,\n",
    "                          min_neg_slope_count_peak_correction: int,\n",
    "                          data_start_time_to_index: dict) -> List:\n",
    "    \"\"\"\n",
    "    Correct peak position by checking if there is a notch in the inspiration branch at left position.\n",
    "    If at least 60% inspiration is done at a notch point, assume that notch as an original peak.\n",
    "    Our hypothesis is most of breathing in done for that cycle. We assume insignificant amount of\n",
    "    breath is taken or some new cycle started after the notch.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param up_intercepts:\n",
    "    :param data:\n",
    "    :param max_amplitude_change_peak_correction:\n",
    "    :param min_neg_slope_count_peak_correction:\n",
    "    :param data_start_time_to_index: hash table for data where start_time is key, index is value\n",
    "\n",
    "    \"\"\"\n",
    "    for i, item in enumerate(peaks):\n",
    "        if valleys[i][0] < up_intercepts[i][0] < peaks[i][0]:\n",
    "            up_intercept = up_intercepts[i]\n",
    "            # points between current valley and UI.\n",
    "            if up_intercept[0] not in data_start_time_to_index or peaks[i][0] not in data_start_time_to_index:\n",
    "                exception_message = 'Data has no start time for peak or up intercept start time at index ' + str(i)\n",
    "#                 raise Exception(exception_message)\n",
    "                return np.array([])\n",
    "            else:\n",
    "                data_up_intercept_index = data_start_time_to_index[up_intercept[0]]\n",
    "                data_peak_index = data_start_time_to_index[peaks[i][0]]\n",
    "\n",
    "                data_ui_to_peak = data[data_up_intercept_index: data_peak_index + 1]\n",
    "\n",
    "                sample_ui_to_peak = data_ui_to_peak[:,1]\n",
    "                slope_at_samples = np.diff(sample_ui_to_peak)\n",
    "\n",
    "                if not all(j >= 0 for j in slope_at_samples):\n",
    "                    indices_neg_slope = [j for j in range(len(slope_at_samples)) if slope_at_samples[j] < 0]\n",
    "                    peak_new = data_ui_to_peak[indices_neg_slope[0]]\n",
    "                    valley_peak_dist_new = peak_new[1] - valleys[i][1]\n",
    "                    valley_peak_dist_prev = peaks[i][1] - valleys[i][1]\n",
    "                    if valley_peak_dist_new == 0:\n",
    "                        return np.array([])\n",
    "                    else:\n",
    "                        amplitude_change = (valley_peak_dist_prev - valley_peak_dist_new) / valley_peak_dist_new * 100.0\n",
    "\n",
    "                        if len(indices_neg_slope) >= min_neg_slope_count_peak_correction:\n",
    "                            if amplitude_change <= max_amplitude_change_peak_correction:\n",
    "                                peaks[i] = peak_new  # 60% inspiration is done at that point.\n",
    "\n",
    "        else:\n",
    "            # TODO: Discuss whether raise exception or not for this scenario.\n",
    "            break  # up intercept at i is not between valley and peak at i\n",
    "\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def remove_close_valley_peak_pair(peaks: List,\n",
    "                                  valleys: List,\n",
    "                                  minimum_peak_to_valley_time_diff: float = 0.31) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out too close valley peak pair.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param minimum_peak_to_valley_time_diff:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = []\n",
    "\n",
    "    for i, item in enumerate(peaks):\n",
    "        time_diff_valley_peak = peaks[i][0] - valleys[i][0]\n",
    "        if time_diff_valley_peak/1000 > minimum_peak_to_valley_time_diff:\n",
    "            peaks_updated.append(peaks[i])\n",
    "            valleys_updated.append(valleys[i])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "def filter_expiration_duration_outlier(peaks: List,\n",
    "                                       valleys: List,\n",
    "                                       threshold_expiration_duration: float) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair for which expiration duration is too small.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param threshold_expiration_duration:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = [valleys[0]]\n",
    "\n",
    "    for i, item in enumerate(peaks):\n",
    "        if i < len(peaks) - 1:\n",
    "            expiration_duration = valleys[i + 1][0] - peaks[i][0]\n",
    "            if expiration_duration/1000 > threshold_expiration_duration:\n",
    "                peaks_updated.append(peaks[i])\n",
    "                valleys_updated.append(valleys[i + 1])\n",
    "\n",
    "    peaks_updated.append(peaks[-1])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def filter_small_amp_expiration_peak_valley(peaks: List,\n",
    "                                            valleys: List,\n",
    "                                            expiration_amplitude_threshold_perc: float) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair if their expiration amplitude is less than or equal to 10% of\n",
    "    average expiration amplitude.\n",
    "\n",
    "    :rtype: object\n",
    "    :return: peaks_updated, valleys_updated:\n",
    "    :param: peaks:\n",
    "    :param: valleys:\n",
    "    :param: expiration_amplitude_threshold_perc:\n",
    "    \"\"\"\n",
    "\n",
    "    expiration_amplitudes = []\n",
    "    peaks_updated = []\n",
    "    valleys_updated = [valleys[0]]\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        if i < len(peaks) - 1:\n",
    "            expiration_amplitudes.append(abs(valleys[i + 1][1] - peak[1]))\n",
    "\n",
    "    mean_expiration_amplitude = np.mean(expiration_amplitudes)\n",
    "\n",
    "    for i, expiration_amplitude in enumerate(expiration_amplitudes):\n",
    "        if expiration_amplitude > expiration_amplitude_threshold_perc * mean_expiration_amplitude:\n",
    "            peaks_updated.append(peaks[i])\n",
    "            valleys_updated.append(valleys[i + 1])\n",
    "\n",
    "    peaks_updated.append(peaks[-1])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def filter_small_amp_inspiration_peak_valley(peaks: List,\n",
    "                                             valleys: List,\n",
    "                                             inspiration_amplitude_threshold_perc: float) -> [List,List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair if their inspiration amplitude is less than or to equal 10% of\n",
    "    average inspiration amplitude.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param inspiration_amplitude_threshold_perc:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = []\n",
    "\n",
    "    inspiration_amplitudes = [(peaks[i][1] - valleys[i][1]) for i, valley in enumerate(valleys)]\n",
    "    mean_inspiration_amplitude = np.mean(inspiration_amplitudes)\n",
    "\n",
    "    for i, inspiration_amplitude in enumerate(inspiration_amplitudes):\n",
    "        if inspiration_amplitude > inspiration_amplitude_threshold_perc * mean_inspiration_amplitude:\n",
    "            valleys_updated.append(valleys[i])\n",
    "            peaks_updated.append(peaks[i])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def compute_peak_valley(rip: np.ndarray,\n",
    "                        fs: float = 21.33,\n",
    "                        smoothing_factor: int = 5,\n",
    "                        time_window: int = 8,\n",
    "                        expiration_amplitude_threshold_perc: float = 0.10,\n",
    "                        threshold_expiration_duration: float = 0.312,\n",
    "                        inspiration_amplitude_threshold_perc: float = 0.10,\n",
    "                        max_amplitude_change_peak_correction: float = 30,\n",
    "                        min_neg_slope_count_peak_correction: int = 4,\n",
    "                        minimum_peak_to_valley_time_diff=0.31) -> [np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute peak and valley from rip data and filter peak and valley.\n",
    "\n",
    "    :rtype: object\n",
    "    :param minimum_peak_to_valley_time_diff:\n",
    "    :param inspiration_amplitude_threshold_perc:\n",
    "    :param smoothing_factor:\n",
    "    :return peak_datastream, valley_datastream:\n",
    "    :param rip:\n",
    "    :param rip_quality:\n",
    "    :param fs:\n",
    "    :param time_window:\n",
    "    :param expiration_amplitude_threshold_perc:\n",
    "    :param threshold_expiration_duration:\n",
    "    :param max_amplitude_change_peak_correction:\n",
    "    :param min_neg_slope_count_peak_correction:\n",
    "    \"\"\"\n",
    "\n",
    "    rip_filtered = rip\n",
    "#     print(rip_filtered.shape)\n",
    "    data_smooth = smooth(data=rip_filtered, span=smoothing_factor)\n",
    "#     print(data_smooth.shape)\n",
    "    window_length = int(round(time_window * fs))\n",
    "    # plt.figure()\n",
    "    # plt.plot(rip_filtered[:,0],rip_filtered[:,1])\n",
    "    # plt.plot(data_smooth[:,0],data_smooth[:,1])\n",
    "    # # plt.plot(data_mac[:,0],data_mac[:,1])\n",
    "    # plt.show()\n",
    "    data_mac = moving_average_curve(deepcopy(data_smooth), window_length=window_length)\n",
    "\n",
    "    data_smooth_start_time_to_index = {}\n",
    "    for index, data in enumerate(data_smooth):\n",
    "        data_smooth_start_time_to_index[data_smooth[index,0]] = index\n",
    "\n",
    "    up_intercepts, down_intercepts = up_down_intercepts(data=data_smooth,\n",
    "                                                        mac=data_mac,\n",
    "                                                        data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "    if len(up_intercepts)<3 or len(down_intercepts)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # print(up_intercepts,down_intercepts)\n",
    "    up_intercepts_filtered, down_intercepts_filtered = filter_intercept_outlier(up_intercepts=up_intercepts,\n",
    "                                                                                down_intercepts=down_intercepts)\n",
    "    if len(up_intercepts_filtered)<3 or len(down_intercepts_filtered)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    peaks, valleys = generate_peak_valley(up_intercepts=up_intercepts_filtered,\n",
    "                                          down_intercepts=down_intercepts_filtered,\n",
    "                                          data=data_smooth)\n",
    "    if len(peaks)<3 or len(valleys)<3:\n",
    "        return np.array([]),np.array([])\n",
    "\n",
    "    valleys_corrected = correct_valley_position(peaks=peaks,\n",
    "                                                valleys=valleys,\n",
    "                                                up_intercepts=up_intercepts_filtered,\n",
    "                                                data=data_smooth,\n",
    "                                                data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "\n",
    "    if len(valleys_corrected)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    peaks_corrected = correct_peak_position(peaks=peaks,\n",
    "                                            valleys=valleys_corrected,\n",
    "                                            up_intercepts=up_intercepts_filtered,\n",
    "                                            data=data_smooth,\n",
    "                                            max_amplitude_change_peak_correction=max_amplitude_change_peak_correction,\n",
    "                                            min_neg_slope_count_peak_correction=min_neg_slope_count_peak_correction,\n",
    "                                            data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "    if len(peaks_corrected)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # remove too close valley peak pair.\n",
    "    peaks_filtered_close, valleys_filtered_close = remove_close_valley_peak_pair(peaks=peaks_corrected,\n",
    "                                                                                 valleys=valleys_corrected,\n",
    "                                                                                 minimum_peak_to_valley_time_diff=minimum_peak_to_valley_time_diff)\n",
    "    if len(peaks_filtered_close)<3 or len(valleys_filtered_close)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # Remove small  Expiration duration < 0.31\n",
    "    peaks_filtered_exp_dur, valleys_filtered_exp_dur = filter_expiration_duration_outlier(peaks=peaks_filtered_close,\n",
    "                                                                                          valleys=valleys_filtered_close,\n",
    "                                                                                          threshold_expiration_duration=threshold_expiration_duration)\n",
    "    if len(peaks_filtered_exp_dur)<3 or len(valleys_filtered_exp_dur)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # filter out peak valley pair of inspiration of small amplitude.\n",
    "    peaks_filtered_insp_amp, valleys_filtered_insp_amp = filter_small_amp_inspiration_peak_valley(\n",
    "        peaks=peaks_filtered_exp_dur,\n",
    "        valleys=valleys_filtered_exp_dur,\n",
    "        inspiration_amplitude_threshold_perc=inspiration_amplitude_threshold_perc)\n",
    "    if len(peaks_filtered_insp_amp)<3 or len(valleys_filtered_insp_amp)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # filter out peak valley pair of expiration of small amplitude.\n",
    "    peaks_filtered_exp_amp, valleys_filtered_exp_amp = filter_small_amp_expiration_peak_valley(\n",
    "        peaks=peaks_filtered_insp_amp,\n",
    "        valleys=valleys_filtered_insp_amp,\n",
    "        expiration_amplitude_threshold_perc=expiration_amplitude_threshold_perc)\n",
    "    if len(peaks_filtered_exp_amp)<3 or len(valleys_filtered_exp_amp)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    \n",
    "    peaks_filtered_exp_amp, valleys_filtered_exp_amp = np.array(peaks_filtered_exp_amp), np.array(valleys_filtered_exp_amp)\n",
    "    # peaks_filtered_exp_amp = np.insert(peaks_filtered_exp_amp,\n",
    "    # peaks_filtered_exp_amp =  np.insert(peaks_filtered_exp_amp, 2, 5, axis=1)\n",
    "    # valleys_filtered_exp_amp =  np.insert(valleys_filtered_exp_amp, 2, 5, axis=1)\n",
    "    # peaks_filtered_exp_amp[:,2] = [data_smooth_start_time_to_index[i[0]] for i in peaks_filtered_exp_amp]\n",
    "    # valleys_filtered_exp_amp[:,2] = [data_smooth_start_time_to_index[i[0]] for i in valleys_filtered_exp_amp]\n",
    "    return np.array(itemgetter(*list(peaks_filtered_exp_amp[:,0]))(data_smooth_start_time_to_index)),\\\n",
    "    np.array(itemgetter(*list(valleys_filtered_exp_amp[:,0]))(data_smooth_start_time_to_index))\n",
    "\n",
    "rip_stream = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "rip_data = CC.get_stream(rip_stream)\n",
    "\n",
    "rip_data.show(5,False)\n",
    "rip_clean_data = rip_data.filter(F.col('quality')=='Acceptable')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"indicator\", StringType())\n",
    "])\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(key,data):\n",
    "    if data.shape[0]>2*21.33*10:\n",
    "        data = data.sort_values('time').reset_index(drop=True)\n",
    "        r = np.zeros((data.shape[0],2))\n",
    "        r[:,0] = 1000*data['time'].values\n",
    "        r[:,1] = data['respiration'].values\n",
    "        peak_index,valley_index = compute_peak_valley(r)\n",
    "        data['indicator'] = 1\n",
    "        data['indicator'].loc[peak_index] = 'p'\n",
    "        data['indicator'].loc[valley_index] = 'v'\n",
    "        data = data[data.indicator.isin(['p','v'])]\n",
    "        return data[['timestamp','localtime','version','user','indicator','respiration']]\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['timestamp','localtime','version','user','indicator','respiration'])\n",
    "rip_clean_data = rip_clean_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "peak_valley_data = rip_clean_data.compute(peak_valley,windowDuration=60,startTime='0 seconds')\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.peak.valley'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Peak Valley of Autosense Respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"indicator\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"Peak and valley indicator in respiration signal\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration amplitude\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','Peak Valley').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "\n",
    "peak_valley_data.metadata = stream_metadata\n",
    "\n",
    "CC.save_stream(peak_valley_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.peak.valley').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rip_cycle_feature_computation(peaks_datastream: np.ndarray,\n",
    "                                  valleys_datastream: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Respiration Feature Implementation. The respiration feature values are\n",
    "    derived from the following paper:\n",
    "    'puffMarker: a multi-sensor approach for pinpointing the timing of first lapse in smoking cessation'\n",
    "    Removed due to lack of current use in the implementation\n",
    "    roc_max = []  # 8. ROC_MAX = max(sample[j]-sample[j-1])\n",
    "    roc_min = []  # 9. ROC_MIN = min(sample[j]-sample[j-1])\n",
    "\n",
    "    :param peaks_datastream: list of peak datapoints\n",
    "    :param valleys_datastream: list of valley datapoints\n",
    "    :return: lists of DataPoints each representing a specific feature calculated from the respiration cycle\n",
    "    found from the peak valley inputs\n",
    "    \"\"\"\n",
    "\n",
    "    inspiration_duration = []  # 1 Inhalation duration\n",
    "    expiration_duration = []  # 2 Exhalation duration\n",
    "    respiration_duration = []  # 3 Respiration duration\n",
    "    inspiration_expiration_ratio = []  # 4 Inhalation and Exhalation ratio\n",
    "    stretch = []  # 5 Stretch\n",
    "    upper_stretch = []  # 6. Upper portion of the stretch calculation\n",
    "    lower_stretch = []  # 7. Lower portion of the stretch calculation\n",
    "    delta_previous_inspiration_duration = []  # 10. BD_INSP = INSP(i)-INSP(i-1)\n",
    "    delta_previous_expiration_duration = []  # 11. BD_EXPR = EXPR(i)-EXPR(i-1)\n",
    "    delta_previous_respiration_duration = []  # 12. BD_RESP = RESP(i)-RESP(i-1)\n",
    "    delta_previous_stretch_duration = []  # 14. BD_Stretch= Stretch(i)-Stretch(i-1)\n",
    "    delta_next_inspiration_duration = []  # 19. FD_INSP = INSP(i)-INSP(i+1)\n",
    "    delta_next_expiration_duration = []  # 20. FD_EXPR = EXPR(i)-EXPR(i+1)\n",
    "    delta_next_respiration_duration = []  # 21. FD_RESP = RESP(i)-RESP(i+1)\n",
    "    delta_next_stretch_duration = []  # 23. FD_Stretch= Stretch(i)-Stretch(i+1)\n",
    "    neighbor_ratio_expiration_duration = []  # 29. D5_EXPR(i) = EXPR(i) / avg(EXPR(i-2)...EXPR(i+2))\n",
    "    neighbor_ratio_stretch_duration = []  # 32. D5_Stretch = Stretch(i) / avg(Stretch(i-2)...Stretch(i+2))\n",
    "\n",
    "    valleys = valleys_datastream\n",
    "    peaks = peaks_datastream[:-1]\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "\n",
    "        delta = peak[0] - valleys[i][0]\n",
    "        inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - peak[0]\n",
    "        expiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - valley_start_time\n",
    "        respiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        ratio = (peak[0] - valley_start_time) / (valleys[i + 1][0] - peak[0])\n",
    "        inspiration_expiration_ratio.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        value = peak[1] - valleys[i + 1][1]\n",
    "        stretch.append(np.array([valley_start_time,valley_end_time,value]))\n",
    "\n",
    "    for i, point in enumerate(inspiration_duration):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "        if i == 0:  # Edge case\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i - 1][2]\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i - 1][2]\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i - 1][2]\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i - 1][2]\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        if i == len(inspiration_duration) - 1:\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i + 1][2]\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i + 1][2]\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i + 1][2]\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i + 1][2]\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        stretch_average = 0\n",
    "        expiration_average = 0\n",
    "        count = 0.0\n",
    "        for j in [-2, -1, 1, 2]:\n",
    "            if i + j < 0 or i + j >= len(inspiration_duration):\n",
    "                continue\n",
    "            stretch_average += stretch[i + j][2]\n",
    "            expiration_average += expiration_duration[i + j][2]\n",
    "            count += 1\n",
    "\n",
    "        stretch_average /= count\n",
    "        expiration_average /= count\n",
    "\n",
    "        ratio = stretch[i][2] / stretch_average\n",
    "        neighbor_ratio_stretch_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        ratio = expiration_duration[i][2] / expiration_average\n",
    "        neighbor_ratio_expiration_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "    # Begin assembling datastream for output\n",
    "    inspiration_duration_datastream = np.array(inspiration_duration)[1:-1]\n",
    "\n",
    "    expiration_duration_datastream = np.array(expiration_duration)[1:-1]\n",
    "\n",
    "    respiration_duration_datastream = np.array(respiration_duration)[1:-1]\n",
    "\n",
    "    inspiration_expiration_ratio_datastream = np.array(inspiration_expiration_ratio)[1:-1]\n",
    "\n",
    "    stretch_datastream = np.array(stretch)[1:-1]\n",
    "\n",
    "    delta_previous_inspiration_duration_datastream = np.array(delta_previous_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_expiration_duration_datastream = np.array(delta_previous_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_respiration_duration_datastream = np.array(delta_previous_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_stretch_duration_datastream = np.array(delta_previous_stretch_duration)[1:-1]\n",
    "\n",
    "    delta_next_inspiration_duration_datastream = np.array(delta_next_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_expiration_duration_datastream = np.array(delta_next_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_respiration_duration_datastream = np.array(delta_next_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_stretch_duration_datastream = np.array(delta_next_stretch_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_expiration_datastream = np.array(neighbor_ratio_expiration_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_stretch_datastream = np.array(neighbor_ratio_stretch_duration)[1:-1]\n",
    "\n",
    "    return np.concatenate([inspiration_duration_datastream,\n",
    "                           expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           inspiration_expiration_ratio_datastream[:,2].reshape(-1,1),\n",
    "                           stretch_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_expiration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_stretch_datastream[:,2].reshape(-1,1)],axis=1)\n",
    "stream_name = 'org.md2k.autosense.rip.peak.valley'\n",
    "\n",
    "data = CC.get_stream(stream_name)\n",
    "\n",
    "data_time = data.withColumn('time',1000*F.col('timestamp').cast('double'))\n",
    "\n",
    "# data_time.sort('timestamp').show(5,False)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"inspiration\", DoubleType()),\n",
    "    StructField(\"expiration\", DoubleType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"ieratio\", DoubleType()),\n",
    "    StructField(\"stretch\", DoubleType()),\n",
    "    StructField(\"pinspiration\", DoubleType()),\n",
    "    StructField(\"pexpiration\", DoubleType()),\n",
    "    StructField(\"prespiration\", DoubleType()),\n",
    "    StructField(\"pstretch\", DoubleType()),\n",
    "    StructField(\"ninspiration\", DoubleType()),\n",
    "    StructField(\"nexpiration\", DoubleType()),\n",
    "    StructField(\"nrespiration\", DoubleType()),\n",
    "    StructField(\"nstretch\", DoubleType()),\n",
    "    StructField(\"rexpiration\", DoubleType()),\n",
    "    StructField(\"rstretch\", DoubleType())    \n",
    "])\n",
    "\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(key,data):\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "    if data.shape[0]>8:\n",
    "        index_dict = {}\n",
    "        data = data.sort_values('time').reset_index(drop=True)\n",
    "        for i in range(data.shape[0]):\n",
    "            index_dict[data['time'].loc[i]] = i\n",
    "        peak_data = data[data.indicator.isin(['p'])]\n",
    "        valley_data = data[data.indicator.isin(['v'])]\n",
    "        peaks = peak_data[['time','respiration']].values\n",
    "        valleys = valley_data[['time','respiration']].values\n",
    "        features = rip_cycle_feature_computation(peaks,valleys)\n",
    "        timestamp_col = data['timestamp'].values\n",
    "        localtime_col = data['localtime'].values\n",
    "        user_id = data['user'].values[0]\n",
    "        version_id = data['version'].values[0]\n",
    "        data1 = pd.DataFrame()\n",
    "        ind_col_1 = np.array([index_dict[features[i][0]] for i in range(features.shape[0])])\n",
    "        ind_col_2 = np.array([index_dict[features[i][1]] for i in range(features.shape[0])])\n",
    "        timestamp_col_s = timestamp_col[ind_col_1]\n",
    "        timestamp_col_e = timestamp_col[ind_col_2]\n",
    "        localtime_col = localtime_col[ind_col_1]\n",
    "        data1['timestamp'] = timestamp_col_s\n",
    "        data1['start'] = timestamp_col_s\n",
    "        data1['end'] = timestamp_col_e\n",
    "        data1['localtime'] = localtime_col\n",
    "        data1['user'] = [user_id]*timestamp_col_e.reshape(-1).shape[0]\n",
    "        data1['version'] = [version_id]*timestamp_col_e.reshape(-1).shape[0]\n",
    "        for i in range(2,features.shape[1],1):\n",
    "            data1[columns[i-2]] = list(features[:,i])\n",
    "        return data1\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=columns+['user','version','timestamp','localtime','start','end'])\n",
    "\n",
    "\n",
    "respiration_features = data_time.compute(peak_valley,windowDuration=240,startTime='0 seconds')\n",
    "\n",
    "# respiration_features.sort('timestamp').show(5,False)\n",
    "respiration_features.printSchema()\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.cycle.features'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Features from autosense respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"start\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"start time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"end\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"end time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"inspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"expiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"expiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ieratio\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration to expiration ratio\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"stretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pinspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"prespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ninspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nrespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor stretch\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','cycle features').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "respiration_features.metadata = stream_metadata\n",
    "CC.save_stream(respiration_features,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.cycle.features').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_stream = 'org.md2k.autosense.rip.cycle.features'\n",
    "rr_intervals = CC.get_stream(rr_stream)\n",
    "rr_intervals.show(5,False)\n",
    "rr_intervals = rr_intervals.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_90 = []\n",
    "for c in columns:\n",
    "    percentile_90 = F.expr('percentile_approx('+str(c)+',0.90)')\n",
    "    temp = rr_intervals.groupBy(['user','day']).agg(percentile_90.alias(str(c)))\n",
    "    df_90.append(temp.toPandas())\n",
    "\n",
    "from copy import deepcopy\n",
    "df_90_final = deepcopy(df_90[0])\n",
    "for i in range(1,len(df_90),1):\n",
    "    df_90_final = df_90_final.join(df_90[i].set_index(['user','day']),on=['user','day'], how='left')\n",
    "\n",
    "df_90_final.set_index(['user','day'],inplace=True)\n",
    "\n",
    "df_10 = []\n",
    "for c in columns:\n",
    "    percentile_10 = F.expr('percentile_approx('+str(c)+',0.10)')\n",
    "    temp = rr_intervals.groupBy(['user','day']).agg(percentile_10.alias(str(c)))\n",
    "    df_10.append(temp.toPandas())\n",
    "\n",
    "from copy import deepcopy\n",
    "df_10_final = deepcopy(df_10[0])\n",
    "for i in range(1,len(df_10),1):\n",
    "    df_10_final = df_10_final.join(df_10[i].set_index(['user','day']),on=['user','day'], how='left')\n",
    "\n",
    "df_10_final.set_index(['user','day'],inplace=True)\n",
    "\n",
    "# df_10_final.loc['c64ca471-369e-43fa-a07b-8260fd1c745c','20190612']['inspiration']\n",
    "\n",
    "df_90_final.head()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"inspiration\", DoubleType()),\n",
    "    StructField(\"expiration\", DoubleType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"ieratio\", DoubleType()),\n",
    "    StructField(\"stretch\", DoubleType()),\n",
    "    StructField(\"pinspiration\", DoubleType()),\n",
    "    StructField(\"pexpiration\", DoubleType()),\n",
    "    StructField(\"prespiration\", DoubleType()),\n",
    "    StructField(\"pstretch\", DoubleType()),\n",
    "    StructField(\"ninspiration\", DoubleType()),\n",
    "    StructField(\"nexpiration\", DoubleType()),\n",
    "    StructField(\"nrespiration\", DoubleType()),\n",
    "    StructField(\"nstretch\", DoubleType()),\n",
    "    StructField(\"rexpiration\", DoubleType()),\n",
    "    StructField(\"rstretch\", DoubleType()),\n",
    "    StructField(\"day\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(data):\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "    if data.shape[0]>1:\n",
    "        user = data['user'].loc[0]\n",
    "        day = data['day'].loc[0]\n",
    "        for c in columns:\n",
    "            upper = df_90_final.loc[user,day][c]\n",
    "            lower = df_10_final.loc[user,day][c]\n",
    "            data[c][data[c]>upper] = upper\n",
    "            data[c][data[c]<lower] = lower\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=columns+['user','version','timestamp','localtime','start','end','day'])\n",
    "\n",
    "rr_intervals_winsorized = rr_intervals.groupBy(['user','day']).apply(peak_valley)\n",
    "\n",
    "rr_intervals_winsorized.show(5,False)\n",
    "stream_name = 'org.md2k.autosense.rip.cycle.features.winsorized'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Features from autosense respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"start\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"start time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"end\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"end time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"inspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"expiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"expiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ieratio\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration to expiration ratio\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"stretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pinspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"prespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ninspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nrespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"day\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"day localtime\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_percentiles.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','cycle features winsorized').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "rr_intervals_winsorized_final =DataStream(data=rr_intervals_winsorized,metadata=stream_metadata)\n",
    "CC.save_stream(rr_intervals_winsorized_final,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.cycle.features.winsorized').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_name = 'org.md2k.autosense.rip.cycle.features.winsorized'\n",
    "df = CC.get_stream(stream_name)\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"m_inspiration\", DoubleType()),\n",
    "    StructField(\"s_inspiration\", DoubleType()),\n",
    "    StructField(\"80_inspiration\", DoubleType()),\n",
    "    StructField(\"20_inspiration\", DoubleType()),\n",
    "    StructField(\"m_expiration\", DoubleType()),\n",
    "    StructField(\"s_expiration\", DoubleType()),\n",
    "    StructField(\"80_expiration\", DoubleType()),\n",
    "    StructField(\"20_expiration\", DoubleType()),\n",
    "    StructField(\"m_respiration\", DoubleType()),\n",
    "    StructField(\"s_respiration\", DoubleType()),\n",
    "    StructField(\"80_respiration\", DoubleType()),\n",
    "    StructField(\"20_respiration\", DoubleType()),\n",
    "    StructField(\"m_ieratio\", DoubleType()),\n",
    "    StructField(\"s_ieratio\", DoubleType()),\n",
    "    StructField(\"80_ieratio\", DoubleType()),\n",
    "    StructField(\"20_ieratio\", DoubleType()),\n",
    "    StructField(\"m_stretch\", DoubleType()),\n",
    "    StructField(\"s_stretch\", DoubleType()),\n",
    "    StructField(\"80_stretch\", DoubleType()),\n",
    "    StructField(\"20_stretch\", DoubleType()),\n",
    "    StructField(\"m_pinspiration\", DoubleType()),\n",
    "    StructField(\"s_pinspiration\", DoubleType()),\n",
    "    StructField(\"80_pinspiration\", DoubleType()),\n",
    "    StructField(\"20_pinspiration\", DoubleType()),\n",
    "    StructField(\"m_pexpiration\", DoubleType()),\n",
    "    StructField(\"s_pexpiration\", DoubleType()),\n",
    "    StructField(\"80_pexpiration\", DoubleType()),\n",
    "    StructField(\"20_pexpiration\", DoubleType()),\n",
    "    StructField(\"m_rexpiration\", DoubleType()),\n",
    "    StructField(\"s_rexpiration\", DoubleType()),\n",
    "    StructField(\"80_rexpiration\", DoubleType()),\n",
    "    StructField(\"20_rexpiration\", DoubleType()),\n",
    "    StructField(\"m_rstretch\", DoubleType()),\n",
    "    StructField(\"s_rstretch\", DoubleType()),\n",
    "    StructField(\"80_rstretch\", DoubleType()),\n",
    "    StructField(\"20_rstretch\", DoubleType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def minute_level_rip_features(key,data):\n",
    "    ms82 = ['m_','s_','80_','20_']\n",
    "    all_columns = ['start','end','timestamp','localtime','user','version','day']\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "    if data.shape[0]>5:\n",
    "        all_data = []\n",
    "        all_data.extend([key[2]['start'],key[2]['end'],data['timestamp'].loc[0],\n",
    "                        data['localtime'].loc[0],data['user'].loc[0],data['version'].loc[0],\n",
    "                        data['day'].loc[0]])\n",
    "        data = data[columns]\n",
    "        list_col = [list(data.mean()),list(data.std()),\n",
    "                    list(data.quantile(.8)),list(data.quantile(.2))]\n",
    "        for i,c in enumerate(ms82):\n",
    "            all_columns.extend([c+j for j in columns])\n",
    "            all_data.extend(list_col[i])\n",
    "        return pd.DataFrame([all_data],columns=all_columns)\n",
    "    else:\n",
    "        for i,c in enumerate(ms82):\n",
    "            all_columns.extend([c+j for j in columns])\n",
    "        return pd.DataFrame([],columns=all_columns)\n",
    "df_minutewise = df.compute(minute_level_rip_features,windowDuration=60,startTime='0 seconds',slideDuration=5)\n",
    "ms82 = ['m_','s_','80_','20_']\n",
    "all_columns = [F.struct('start', 'end').alias('window'),'timestamp',\n",
    "               'localtime','user','version','day']\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "      'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "for i,c in enumerate(ms82):\n",
    "    all_columns.extend([c+j for j in columns])\n",
    "df_minutewise = df_minutewise.select(*all_columns)\n",
    "df_minutewise.show(2)\n",
    "schema = df_minutewise._data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.minute.features').set_description(\"Respiration Minute Features\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration Minute Level Features\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "print(stream_metadata.is_valid(),'metadata')\n",
    "ds = DataStream(data=df_minutewise._data,metadata=stream_metadata)\n",
    "ds.printSchema()\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features').sort('window').select('window','timestamp').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features').sort('window').select('window','timestamp').show(3,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rip_data = CC.get_stream('org.md2k.autosense.rip.minute.features')\n",
    "activity_data = CC.get_stream('org.md2k.autosense.accel.activity.60seconds')\n",
    "\n",
    "df = rip_data.join(activity_data.drop(*['timestamp','localtime','version']),on=['user','window'],how='left')\n",
    "df = df.filter(F.col('activity')!=1)\n",
    "df = df.drop(*['activity'])\n",
    "df = df.withColumn('start',F.col('window').start)\n",
    "df = df.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "\n",
    "# df.printSchema()\n",
    "ms82 = ['m_','s_','80_','20_']\n",
    "all_columns = ['start','end','timestamp','localtime','user','version','day']\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "feature_columns = []\n",
    "for i,c in enumerate(ms82):\n",
    "    feature_columns.extend([c+j for j in columns])\n",
    "    all_columns.extend([c+j for j in columns])\n",
    "\n",
    "df = df.select(*all_columns)\n",
    "# all_columns\n",
    "# df.count()\n",
    "# CC.get_stream('org.md2k.autosense.rip.minute.features').count()\n",
    "\n",
    "feature_columns_action_std = [F.stddev(i).alias(i) for i in feature_columns]\n",
    "feature_columns_action_mean = [F.mean(i).alias(i) for i in feature_columns]\n",
    "\n",
    "std_all = df.groupBy(['user','day']).agg(*feature_columns_action_std)\n",
    "mean_all = df.groupBy(['user','day']).agg(*feature_columns_action_mean)\n",
    "\n",
    "mean_all = mean_all.toPandas()\n",
    "std_all = std_all.toPandas()\n",
    "mean_all.set_index(['user','day'],inplace=True)\n",
    "std_all.set_index(['user','day'],inplace=True)\n",
    "\n",
    "basic_schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType()),\n",
    "            StructField(\"localtime\", TimestampType()),\n",
    "            StructField(\"user\", StringType()),\n",
    "            StructField(\"day\", StringType()),\n",
    "            StructField(\"version\", IntegerType()),\n",
    "            StructField(\"start\", TimestampType()),\n",
    "            StructField(\"end\", TimestampType())\n",
    "])\n",
    "\n",
    "features_list = []\n",
    "for c in feature_columns:\n",
    "    features_list.append(StructField(c, DoubleType(), True))\n",
    "features_schema = StructType(basic_schema.fields + features_list)\n",
    "\n",
    "@pandas_udf(features_schema, PandasUDFType.GROUPED_MAP)\n",
    "def standardize_rip_feature(data):\n",
    "    if data.shape[0]>10:\n",
    "        for c in feature_columns:\n",
    "            m = mean_all.loc[data.loc[0]['user'],data.loc[0]['day']][c]\n",
    "            s = std_all.loc[data.loc[0]['user'],data.loc[0]['day']][c]\n",
    "            data[c] = (data[c] - m)/s\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=all_columns)\n",
    "\n",
    "df_standardized = df.groupBy(['user','day']).apply(standardize_rip_feature)\n",
    "columns = ['m_inspiration', 'm_expiration', 'm_respiration',\n",
    "       'm_ieratio', 'm_stretch', 'm_pinspiration', 'm_pexpiration',\n",
    "       'm_rexpiration', 'm_rstretch', 's_inspiration', 's_expiration',\n",
    "       's_respiration', 's_ieratio', 's_stretch', 's_pinspiration',\n",
    "       's_pexpiration', 's_rexpiration', 's_rstretch', '80_inspiration',\n",
    "       '80_expiration', '80_respiration', '80_ieratio', '80_stretch',\n",
    "       '80_pinspiration', '80_pexpiration', '80_rexpiration', '80_rstretch',\n",
    "       '20_inspiration', '20_expiration', '20_respiration', '20_ieratio',\n",
    "       '20_stretch', '20_pinspiration', '20_pexpiration', '20_rexpiration',\n",
    "       '20_rstretch']\n",
    "print(len(feature_columns),len(columns))\n",
    "df_standardized_array = df_standardized.withColumn('features',F.array(*columns)).drop(*feature_columns)\n",
    "\n",
    "df_standardized_array.show(5,False)\n",
    "\n",
    "df_standardized_array = df_standardized_array.select(\"user\",'version','timestamp','localtime','day',\n",
    "              F.struct('start', 'end').alias('window'),'features')\n",
    "\n",
    "schema = df_standardized_array.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.minute.features.standardized.final').set_description(\"RIP features standardized\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration features standardized\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=df_standardized_array,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features.standardized.final').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features.standardized.final').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CC.get_stream('org.md2k.autosense.rip.minute.features.standardized.final')\n",
    "\n",
    "# df.show(5,False)\n",
    "\n",
    "df = df.withColumn('start',F.col('window').start)\n",
    "df = df.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "\n",
    "schema = StructType([StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"stress_likelihood\", DoubleType())\n",
    "])\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "ecg_model = pickle.load(open('./models/rip_model.p','rb'))\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def ecg_r_peak(key,data):\n",
    "    if data.shape[0]>0:\n",
    "        features = []\n",
    "        for i in range(data.shape[0]):\n",
    "            features.append(np.array(data['features'][i]))\n",
    "        features = np.array(features)\n",
    "        features[features>10] = 10\n",
    "        features[features<-10] = -10\n",
    "        probs = ecg_model.predict_proba(np.nan_to_num(features))[:,1]\n",
    "        data['stress_likelihood'] = probs\n",
    "        data = data[['timestamp','start','end','version','user','day',\n",
    "                     'localtime','stress_likelihood']]\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['timestamp','version','user','day',\n",
    "                                        'localtime','stress_likelihood','start','end'])\n",
    "\n",
    "df_stress = df.compute(ecg_r_peak,windowDuration=6000,startTime='0 seconds')\n",
    "\n",
    "df_stress.show(4,False)\n",
    "\n",
    "df_stress = df_stress.select(\"user\",'version','timestamp','localtime','day',\n",
    "              F.struct('start', 'end').alias('window'),'stress_likelihood')\n",
    "\n",
    "schema = df_stress._data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.stress.likelihood').set_description(\"Stress from Respiration\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration stress likelihood\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=df_stress._data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_data = CC.get_stream('org.md2k.autosense.rip.stress.likelihood')\n",
    "stress_data = stress_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "stress_data = stress_data.withColumn('start',F.col('window').start)\n",
    "stress_data = stress_data.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "stress_data = stress_data.withColumn('start',F.col('start').cast('double'))\n",
    "stress_data = stress_data.withColumn('end',F.col('end').cast('double'))\n",
    "stress_data = stress_data.withColumn('localtime',F.col('localtime').cast('double'))\n",
    "stress_data = stress_data.withColumn('timestamp',F.col('timestamp').cast('double'))\n",
    "stress_data  = stress_data.withColumn('likelihood_mean',F.lit(1))\n",
    "stress_data.printSchema()\n",
    "\n",
    "schema = StructType([StructField(\"timestamp\", DoubleType()),\n",
    "    StructField(\"start\", DoubleType()),\n",
    "    StructField(\"end\", DoubleType()),\n",
    "    StructField(\"localtime\", DoubleType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"stress_likelihood\", DoubleType()),\n",
    "    StructField(\"imputed\", IntegerType()),\n",
    "    StructField(\"likelihood_mean\", DoubleType())\n",
    "])\n",
    "step = 5\n",
    "smoothing = int(60*2/step)\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def impute_forwardfill(data):\n",
    "    data = data.sort_values('start').reset_index(drop=True)\n",
    "    start = data['start'][0]\n",
    "    all_rows = []\n",
    "    for i,row in data.iterrows():\n",
    "        if row['start']==start:\n",
    "            all_rows.append([row['timestamp'],row['localtime'],row['start'],row['end'],\n",
    "                             row['version'],row['user'],row['day'],row['stress_likelihood'],\n",
    "                             row['likelihood_mean'],0])\n",
    "            start = row['end']\n",
    "        else:\n",
    "            k = 1\n",
    "            while (start+k*step)<=row['start']:\n",
    "                all_rows.append([data.loc[i-1]['timestamp']+k*step,data.loc[i-1]['localtime']+k*step,\n",
    "                                 data.loc[i-1]['start']+k*step,data.loc[i-1]['end']+k*step,\n",
    "                                 row['version'],row['user'],row['day'],\n",
    "                                 data.loc[i-1]['stress_likelihood'],data.loc[i-1]['likelihood_mean'],1])\n",
    "                k+=1\n",
    "            all_rows.append([row['timestamp'],row['localtime'],row['start'],row['end'],\n",
    "                             row['version'],row['user'],row['day'],row['stress_likelihood'],\n",
    "                             row['likelihood_mean'],0])\n",
    "            start = row['end']    \n",
    "    return pd.DataFrame(all_rows,columns=['timestamp','localtime','start','end',\n",
    "                                          'version','user','day','stress_likelihood','likelihood_mean','imputed'])\n",
    "\n",
    "stress_imputed_data = stress_data.groupBy(['user','day']).apply(impute_forwardfill)\n",
    "stress_imputed_data = stress_imputed_data.withColumn('start',F.col('start').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('end',F.col('end').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('localtime',F.col('localtime').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('timestamp',F.col('timestamp').cast('timestamp'))\n",
    "cols = list(stress_imputed_data.columns)\n",
    "cols.append(F.struct('start', 'end').alias('window'))\n",
    "cols.remove('start')\n",
    "cols.remove('end')\n",
    "stress_imputed_data = stress_imputed_data.select(*cols)\n",
    "stress_imputed_data.show(5,False)\n",
    "schema = stress_imputed_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.stress.likelihood.ffill').set_description(\"rip stress imputed\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"stress forward fill imputed\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=stress_imputed_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.stress.likelihood.ffill').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------+-------------------+-------+------------------------------------+-------+-------------+-------------+----+--------+\n",
      "|timestamp       |localtime       |day     |stress_probability |imputed|user                                |version|start        |end          |hour|weekday |\n",
      "+----------------+----------------+--------+-------------------+-------+------------------------------------+-------+-------------+-------------+----+--------+\n",
      "|1.561184769619E9|1.561163169619E9|20190622|0.21555789087732136|0      |9197be51-f220-4c63-a6a8-3ec1bbd50810|1      |1.561184735E9|1.561184795E9|0   |Saturday|\n",
      "|1.561184769619E9|1.561163169619E9|20190622|0.21555789087732136|0      |9197be51-f220-4c63-a6a8-3ec1bbd50810|1      |1.56118474E9 |1.5611848E9  |0   |Saturday|\n",
      "|1.561184769619E9|1.561163169619E9|20190622|0.21555789087732136|0      |9197be51-f220-4c63-a6a8-3ec1bbd50810|1      |1.561184745E9|1.561184805E9|0   |Saturday|\n",
      "|1.561184769619E9|1.561163169619E9|20190622|0.21555716711897396|0      |9197be51-f220-4c63-a6a8-3ec1bbd50810|1      |1.56118475E9 |1.56118481E9 |0   |Saturday|\n",
      "|1.561184769619E9|1.561163169619E9|20190622|0.21555726903369546|0      |9197be51-f220-4c63-a6a8-3ec1bbd50810|1      |1.561184755E9|1.561184815E9|0   |Saturday|\n",
      "+----------------+----------------+--------+-------------------+-------+------------------------------------+-------+-------------+-------------+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,GridSearchCV,KFold,train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "def best_fit_slope(ys):\n",
    "    return np.mean(np.diff(ys))\n",
    "\n",
    "def get_features(y):\n",
    "    tmp = y[-1,0]\n",
    "    return [tmp,\n",
    "#             np.median(y),\n",
    "#             np.std(y),\n",
    "#             np.percentile(y,80),\n",
    "#             np.percentile(y,20),\n",
    "            best_fit_slope(y[:,0])]\n",
    "\n",
    "def get_trained_model(X_train,y_train):\n",
    "    paramGrid = {'rf__n_neighbors':[3,4,5,6,7,8,9],\n",
    "                 }\n",
    "    clf = Pipeline([('rf',KNeighborsRegressor())])\n",
    "    gkf = KFold(n_splits=5)\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=gkf.split(X_train),\n",
    "                               scoring='r2',verbose=5)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf\n",
    "\n",
    "weekday_dict = {'Wednesday':5,\n",
    "                'Saturday':1,\n",
    "                'Thursday':6,\n",
    "                'Tuesday':4,\n",
    "                'Friday':0,\n",
    "                'Sunday':2,\n",
    "                'Monday':3}\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", DoubleType()),\n",
    "    StructField(\"start\", DoubleType()),\n",
    "    StructField(\"end\", DoubleType()),\n",
    "    StructField(\"localtime\", DoubleType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"weekday\", StringType()),\n",
    "    StructField(\"hour\", IntegerType()),\n",
    "    StructField(\"stress_probability\", DoubleType()),\n",
    "    StructField(\"imputed\", IntegerType()),\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fillup_imputation(data):\n",
    "    data = data.sort_values('start').reset_index(drop=True)\n",
    "    data['stress_probability'] = data['stress_probability'].rolling(window=lookback).mean()\n",
    "    data = data.dropna().sort_values('start').reset_index(drop=True)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i,row in data.iterrows():\n",
    "        if i<lookback:\n",
    "            continue\n",
    "        if row['imputed'] in [1]:\n",
    "            continue\n",
    "        prev_imputed = list(data['imputed'].iloc[i-lookback:i].values)\n",
    "        if prev_imputed.count(1)<=.33*lookback:\n",
    "            feature = data[['stress_probability','imputed','timestamp']].iloc[i-lookback:i]\n",
    "#             .sort_values('timestamp').reset_index(drop=True)\n",
    "            feature = feature[feature.imputed.isin([0,2])][['stress_probability','timestamp']].values\n",
    "            X.append(np.array(get_features(feature)+[row['hour'],weekday_dict[row['weekday']]]))\n",
    "            y.append(row['stress_probability'])\n",
    "    if len(X)<100:\n",
    "        return data\n",
    "    X = np.array(X)\n",
    "    X_s = X[:,:-2]\n",
    "    X_hour = X[:,-2:-1].reshape(-1,1)\n",
    "    X_weekday = X[:,-1:].reshape(-1,1)\n",
    "    clf_hour = OneHotEncoder().fit(np.arange(0,25,1).reshape(-1,1))\n",
    "    clf_week_day = OneHotEncoder().fit(np.arange(0,7,1).reshape(-1,1))\n",
    "    X_hour = clf_hour.transform(X_hour).todense().reshape(X.shape[0],-1)\n",
    "    X_weekday = clf_week_day.transform(X_weekday).todense().reshape(X.shape[0],-1)\n",
    "    X = np.concatenate([X_s,X_hour,X_weekday],axis=1)\n",
    "    y = np.array(y)\n",
    "    print(X.shape)\n",
    "    clf = get_trained_model(X,y)\n",
    "    start = data[data.imputed==1].shape[0]+1\n",
    "#     return data\n",
    "    count = 0\n",
    "    while data[data.imputed==1].shape[0]<start and data[data.imputed==1].shape[0]>0:\n",
    "        start = data[data.imputed==1].shape[0]\n",
    "        print(start,count)\n",
    "        X = []\n",
    "        y = []\n",
    "        index = []\n",
    "        for i,row in data.iterrows():\n",
    "            if i<lookback:\n",
    "                continue\n",
    "            if row['imputed'] in [0,2]:\n",
    "                continue\n",
    "            prev_imputed = list(data['imputed'].iloc[i-lookback:i].values)\n",
    "            if prev_imputed.count(1)<=.33*lookback:\n",
    "                feature = data[['stress_probability','imputed','timestamp']].iloc[i-lookback:i]\n",
    "#                 .sort_values('timestamp').reset_index(drop=True)\n",
    "                feature = feature[feature.imputed.isin([0,2])][['stress_probability','timestamp']].values\n",
    "                X.append(np.array(get_features(feature)+[row['hour'],weekday_dict[row['weekday']]]))\n",
    "                y.append(row['stress_probability'])\n",
    "                index.append(i)\n",
    "        count+=1\n",
    "        if len(X)==0:\n",
    "            break\n",
    "        X = np.array(X)\n",
    "        X_s = X[:,:-2]\n",
    "        X_hour = X[:,-2:-1].reshape(-1,1)\n",
    "        X_weekday = X[:,-1:].reshape(-1,1)\n",
    "        X_hour = clf_hour.transform(X_hour).todense().reshape(X.shape[0],-1)\n",
    "        X_weekday = clf_week_day.transform(X_weekday).todense().reshape(X.shape[0],-1)\n",
    "        X = np.concatenate([X_s,X_hour,X_weekday],axis=1)\n",
    "#         break\n",
    "        data.loc[index,'stress_probability'] = clf.predict(X).reshape(-1)\n",
    "        data.loc[index,'imputed'] = 2\n",
    "    return data\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fillup_imputation_simple(data):\n",
    "    data = data.sort_values('start').reset_index(drop=True)\n",
    "    data['stress_probability'] = data['stress_probability'].rolling(window=lookback).mean()\n",
    "    data = data.dropna().sort_values('start').reset_index(drop=True)\n",
    "    if data.shape[0]<100:\n",
    "        return data\n",
    "    Xs = []\n",
    "    Xweekday = []\n",
    "    Xhour = []\n",
    "    index = []\n",
    "    for i,row in data.iterrows():\n",
    "        if row['imputed'] in [1]:\n",
    "            Xs.append(np.nan)\n",
    "            index.append(i)\n",
    "        else:\n",
    "            Xs.append(row['stress_probability'])\n",
    "        Xweekday.append(weekday_dict[row['weekday']])\n",
    "        Xhour.append(row['hour'])\n",
    "    X_s = np.array(Xs).reshape(-1,1)\n",
    "    clf_hour = OneHotEncoder().fit(np.arange(0,25,1).reshape(-1,1))\n",
    "    clf_week_day = OneHotEncoder().fit(np.arange(0,7,1).reshape(-1,1))\n",
    "    X_hour = clf_hour.transform(np.array(Xhour).reshape(-1,1)).todense().reshape(X_s.shape[0],-1)\n",
    "    X_weekday = clf_week_day.transform(np.array(Xweekday).reshape(-1,1)).todense().reshape(X_s.shape[0],-1)\n",
    "    X = np.concatenate([X_hour,X_weekday,X_s],axis=1)\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X = imputer.fit_transform(X)[np.array(index)]\n",
    "    data.loc[index,'stress_probability'] = X[:,-1]\n",
    "    data.loc[index,'imputed'] = 2\n",
    "    return data\n",
    "\n",
    "step = 5\n",
    "lookback = int(3*(60/5))\n",
    "stream_name = 'org.md2k.autosense.rip.stress.likelihood.ffill'\n",
    "stress_data = CC.get_stream(stream_name)\n",
    "stress_data = stress_data.withColumnRenamed('stress_likelihood','stress_probability').drop('likelihood_mean')\n",
    "# stress_data = stress_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "stress_data = stress_data.withColumn('start',F.col('window').start)\n",
    "stress_data = stress_data.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "stress_data = stress_data.withColumn('start',F.col('start').cast('double'))\n",
    "stress_data = stress_data.withColumn('end',F.col('end').cast('double'))\n",
    "stress_data = stress_data.withColumn('hour',F.hour('localtime'))\n",
    "stress_data = stress_data.withColumn('weekday',F.date_format('localtime','EEEE'))\n",
    "stress_data = stress_data.withColumn('localtime',F.col('localtime').cast('double'))\n",
    "stress_data = stress_data.withColumn('timestamp',F.col('timestamp').cast('double'))\n",
    "stress_data.show(5,False)\n",
    "data_1 = stress_data._data.toPandas()\n",
    "stress_imputed_data = stress_data.groupBy(['user','version']).apply(fillup_imputation_simple)\n",
    "stress_imputed_data = stress_imputed_data.withColumn('start',F.col('start').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('end',F.col('end').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('localtime',F.col('localtime').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('timestamp',F.col('timestamp').cast('timestamp'))\n",
    "cols = list(stress_imputed_data.columns)\n",
    "cols.append(F.struct('start', 'end').alias('window'))\n",
    "cols.remove('start')\n",
    "cols.remove('end')\n",
    "cols.remove('hour')\n",
    "cols.remove('weekday')\n",
    "cols.remove('day')\n",
    "stress_imputed_data = stress_imputed_data.select(*cols)\n",
    "# stress_imputed_data.show(100,False) \n",
    "schema = stress_imputed_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.stress.likelihood.imputed').set_description(\"stress imputed\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"stress imputed in KNN\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=stress_imputed_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)\n",
    "# stream_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa1 = fillup_imputation_simple(data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 8705, 2: 21295})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(aa1['imputed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2505., 2481., 1942.,  919.,  554.,  224.,   48.,   12.,    6.,\n",
       "          14.]),\n",
       " array([0.01019088, 0.09361269, 0.1770345 , 0.26045632, 0.34387813,\n",
       "        0.42729994, 0.51072175, 0.59414357, 0.67756538, 0.76098719,\n",
       "        0.84440901]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(aa1['stress_probability'][18000:19900])\n",
    "# plt.plot(aa1['imputed'][18000:19900])\n",
    "plt.hist(aa1[aa1.imputed.isin([0])]['stress_probability'],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream('org.md2k.autosense.rip.stress.likelihood.imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(10,3),columns=['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['a','b']].iloc[8-3:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in df.iterrows():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_data = CC.get_stream('org.md2k.autosense.rip.stress.likelihood')._data.toPandas()\n",
    "# quality_data = CC.get_stream('org.md2k.autosense.rip.quality.60seconds')._data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,6))\n",
    "# plt.plot(stress_data['timestamp'][stress_data.user==stress_data['user'].iloc[20000]][:300],\n",
    "#          stress_data['stress_likelihood'][stress_data.user==stress_data['user'].iloc[20000]][:300])\n",
    "plt.hist(stress_data['stress_likelihood'],20)\n",
    "plt.show()\n",
    "stress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(stress_data,open('./rice_data/stress_rip_only_no_c6_nw.p','wb'))\n",
    "pickle.dump(quality_data,open('./rice_data/rip_quality_60_seconds_nw.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CC.get_stream('org.md2k.motionsensehrv.ppg.left.stress.md2k_aa_rice.imputed.ffill')\n",
    "emas = CC.get_stream(\"perceived.stress.score--org.md2k.ema_scheduler--phone\").drop(*['timestamp',\n",
    "                                                                                    'localtime',\n",
    "                                                                                    'version'])\n",
    "all_stress = ds.join(emas,on=['user','window'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
