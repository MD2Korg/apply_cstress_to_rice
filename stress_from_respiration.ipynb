{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/cerebralcortex/core/data_manager/raw/data.py:67: DeprecationWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n",
      "  self.fs = pa.hdfs.connect(self.hdfs_ip, self.hdfs_port)\n"
     ]
    }
   ],
   "source": [
    "# from cerebralcortex.util.helper_methods import get_study_names\n",
    "# sn = get_study_names(\"/home/jupyter/cc3_conf/\")\n",
    "# print(sn)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, LongType, \\\n",
    "DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "ModuleMetadata\n",
    "import pandas as pd\n",
    "from cerebralcortex import Kernel\n",
    "study_name = 'md2k_aa_rice'\n",
    "CC = Kernel(\"/home/jupyter/cc3_conf/\", study_name=study_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_respiration_quality(data,\n",
    "                                window_size=3,\n",
    "                                outlier_threshold_high = 4000,\n",
    "                                outlier_threshold_low = 20,\n",
    "                                slope_threshold = 300,\n",
    "                                eck_threshold_band_loose = 175,\n",
    "                                eck_threshold_band_off = 20,\n",
    "                                minimum_expected_samples = 3*(0.33)*21.33,\n",
    "                                acceptable_outlier_percent = 34):\n",
    "    \n",
    "    def get_metadata(stream_name):\n",
    "        stream_metadata = Metadata()\n",
    "        stream_metadata.set_name(stream_name).set_description(\"Chest Respiration quality per sample 3 seconds\") \\\n",
    "            .add_dataDescriptor(\n",
    "            DataDescriptor().set_name(\"quality\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "            \"Respiration data quality\").set_attribute('Loose/Improper Attachment','Electrode Displacement').set_attribute('Sensor off Body',\\\n",
    "            'Autosense not worn').set_attribute('Battery down/Disconnected', \\\n",
    "            'No data is present - Can be due to battery down or sensor disconnection').set_attribute('Interittent Data Loss', \\\n",
    "             'Not enough samples are present').set_attribute('Acceptable','Good Quality')) \\\n",
    "            .add_dataDescriptor(\n",
    "            DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "            \"respiration sample value\")) \\\n",
    "            .add_module(\n",
    "            ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/rip_quality.ipynb\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "                \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "        return stream_metadata\n",
    "    \n",
    "    def get_quality(data):\n",
    "        data_quality_band_loose = 'Loose/Improper Attachment'\n",
    "        data_quality_not_worn = 'Sensor off Body'\n",
    "        data_quality_band_off = 'Battery down/Disconnected'\n",
    "        data_quality_missing = 'Interittent Data Loss' \n",
    "        data_quality_good = 'Acceptable'\n",
    "        if (len(data)== 0):\n",
    "            return data_quality_band_off\n",
    "        range_data = max(data)-min(data)\n",
    "        if range_data<=eck_threshold_band_off:\n",
    "            return data_quality_not_worn\n",
    "        if (len(data)<=minimum_expected_samples) :\n",
    "            return data_quality_missing\n",
    "        if range_data<=eck_threshold_band_loose:\n",
    "            return data_quality_band_loose\n",
    "\n",
    "        outlier_counts = 0 \n",
    "        for i in range(0,len(data)):\n",
    "            im,ip  = i,i\n",
    "            if i==0:\n",
    "                im = len(data)-1\n",
    "            else:\n",
    "                im = i-1\n",
    "            if i == len(data)-1:\n",
    "                ip = 0\n",
    "            else:\n",
    "                ip = ip+1\n",
    "            stuck = ((data[i]==data[im]) and (data[i]==data[ip]))\n",
    "            flip = ((abs(data[i]-data[im])>((int(outlier_threshold_high)))) or (abs(data[i]-data[ip])>((int(outlier_threshold_high)))))\n",
    "            disc = ((abs(data[i]-data[im])>((int(slope_threshold)))) and (abs(data[i]-data[ip])>((int(slope_threshold)))))\n",
    "            if disc:\n",
    "                outlier_counts += 1\n",
    "            elif stuck:\n",
    "                outlier_counts +=1\n",
    "            elif flip:\n",
    "                outlier_counts +=1\n",
    "            elif data[i] >= outlier_threshold_high:\n",
    "                outlier_counts +=1\n",
    "            elif data[i]<= outlier_threshold_low:\n",
    "                outlier_counts +=1\n",
    "        if (100*outlier_counts>acceptable_outlier_percent*len(data)):\n",
    "            return data_quality_band_loose\n",
    "        return data_quality_good\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType()),\n",
    "        StructField(\"localtime\", TimestampType()),\n",
    "        StructField(\"version\", IntegerType()),\n",
    "        StructField(\"user\", StringType()),\n",
    "        StructField(\"quality\", StringType()),\n",
    "        StructField(\"respiration\", DoubleType())\n",
    "    ])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def ecg_quality(key,data):\n",
    "        data['quality'] = ''\n",
    "        if data.shape[0]>0:\n",
    "            data = data.sort_values('timestamp')\n",
    "            data['quality'] = get_quality(list(data['respiration']))\n",
    "        return data\n",
    "\n",
    "    stream_name = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "    rip_quality_stream = rip.compute(ecg_quality,windowDuration=window_size,startTime='0 seconds')\n",
    "    data = rip_quality_stream._data\n",
    "    ds = DataStream(data=data,metadata=get_metadata(stream_name))\n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rip_stream = 'respiration--org.md2k.autosense--autosense_chest--chest'\n",
    "rip = CC.get_stream(rip_stream)\n",
    "ds = compute_respiration_quality(rip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o127.load.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:116)\n\tat org.apache.spark.SparkContext.$anonfun$parallelize$1(SparkContext.scala:774)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:756)\n\tat org.apache.spark.SparkContext.parallelize(SparkContext.scala:773)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.listLeafFiles(InMemoryFileIndex.scala:360)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.$anonfun$bulkListLeafFiles$1(InMemoryFileIndex.scala:195)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:187)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:568)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:406)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2ccfdec4b2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.md2k.autosense.rip.quality.per.sample'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/cerebralcortex/kernel.py\u001b[0m in \u001b[0;36mget_stream\u001b[0;34m(self, stream_name, version, user_id, data_type)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m###########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/cerebralcortex/core/data_manager/raw/stream_handler.py\u001b[0m in \u001b[0;36mget_stream\u001b[0;34m(self, stream_name, version, user_id, data_type)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;31m#df = df.dropDuplicates(subset=['timestamp'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/cerebralcortex/core/data_manager/raw/filebased_storage.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(self, stream_name, version, user_id)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_storage_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o127.load.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:116)\n\tat org.apache.spark.SparkContext.$anonfun$parallelize$1(SparkContext.scala:774)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.SparkContext.withScope(SparkContext.scala:756)\n\tat org.apache.spark.SparkContext.parallelize(SparkContext.scala:773)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.listLeafFiles(InMemoryFileIndex.scala:360)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.$anonfun$bulkListLeafFiles$1(InMemoryFileIndex.scala:195)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:187)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:568)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:406)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.quality.per.sample').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"quality\", StringType())\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def ecg_quality_60(key,data):\n",
    "    if data.shape[0]>0:\n",
    "        quals = list(data['quality'].values)\n",
    "        qual = Counter(quals).most_common()[0][0]\n",
    "        data = data[:1].reset_index(drop=True)\n",
    "        data['quality'].set_value(0,qual)\n",
    "        data['start'] = [key[2]['start']]\n",
    "        data['end'] = [key[2]['end']]\n",
    "        return data[['start','end','timestamp','localtime','version','user','quality']]\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['start','end','timestamp','localtime','version','user','quality'])\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.quality.60seconds'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Chest RIP quality 60 seconds\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"quality\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"ECG data quality\").set_attribute('Loose/Improper Attachment','Electrode Displacement').set_attribute('Sensor off Body',\\\n",
    "    'Autosense not worn').set_attribute('Battery down/Disconnected', \\\n",
    "    'No data is present - Can be due to battery down or sensor disconnection').set_attribute('Interittent Data Loss', \\\n",
    "     'Not enough samples are present').set_attribute('Acceptable','Good Quality')) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"window\").set_type(\"struct\").set_attribute(\"description\", \\\n",
    "    \"window start and end time in UTC\").set_attribute('start', \\\n",
    "    'start of 1 minute window').set_attribute('end','end of 1 minute window')) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "\n",
    "stream_metadata.is_valid()\n",
    "ecg_stream = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "ecg = CC.get_stream(ecg_stream)\n",
    "ecg_quality_stream = ecg.compute(ecg_quality_60,windowDuration=60,startTime='0 seconds')\n",
    "ecg_quality_stream = ecg_quality_stream.select('timestamp', F.struct('start', 'end').alias('window'),\n",
    "                                   'localtime','quality','user','version')\n",
    "ecg_quality_stream.printSchema()\n",
    "data = ecg_quality_stream._data\n",
    "ds = DataStream(data=data,metadata=stream_metadata)\n",
    "CC.save_stream(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.quality.60seconds').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "# TODO: What is this?\n",
    "class Quality(Enum):\n",
    "    ACCEPTABLE = 1\n",
    "    UNACCEPTABLE = 0\n",
    "from operator import itemgetter\n",
    "\n",
    "def smooth(data:np.ndarray,\n",
    "           span: int = 5)-> np.ndarray:\n",
    "    \"\"\"\n",
    "\n",
    "    :rtype: object\n",
    "    :param data:\n",
    "    :param span:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return []\n",
    "    # plt.figure()\n",
    "    # plt.plot(data[:,1])\n",
    "    sample = data[:,1]\n",
    "    sample_middle = np.convolve(sample, np.ones(span, dtype=int), 'valid') / span\n",
    "    divisor = np.arange(1, span - 1, 2)\n",
    "    sample_start = np.cumsum(sample[:span - 1])[::2] / divisor\n",
    "    sample_end = (np.cumsum(sample[:-span:-1])[::2] / divisor)[::-1]\n",
    "    sample_smooth = np.concatenate((sample_start, sample_middle, sample_end))\n",
    "    data[:,1] = sample_smooth\n",
    "\n",
    "    # plt.plot(data[:,1])\n",
    "    # plt.show()\n",
    "    return data\n",
    "\n",
    "\n",
    "def moving_average_curve(data: np.ndarray,\n",
    "                         window_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Moving average curve from filtered (using moving average) samples.\n",
    "\n",
    "    :rtype: object\n",
    "    :return: mac\n",
    "    :param data:\n",
    "    :param window_length:\n",
    "    \"\"\"\n",
    "    if data is None or len(data) == 0:\n",
    "        return []\n",
    "    # plt.figure()\n",
    "    # plt.plot(data[:,1])\n",
    "    sample = data[:,1]\n",
    "    for i in range(window_length, len(sample) - (window_length + 1)):\n",
    "        sample_avg = np.mean(sample[i - window_length:i + window_length + 1])\n",
    "        data[i,1] = sample_avg\n",
    "\n",
    "    # plt.plot(data[:,1])\n",
    "    # plt.show()\n",
    "    return data[np.array(range(window_length, len(sample) - (window_length + 1)))]\n",
    "\n",
    "\n",
    "def up_down_intercepts(data: np.ndarray,\n",
    "                       mac: np.ndarray,\n",
    "                       data_start_time_to_index: dict) -> [np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns Up and Down Intercepts.\n",
    "    Moving Average Centerline curve intersects breath cycle twice. Once in the inhalation branch\n",
    "    (Up intercept) and in the exhalation branch (Down intercept).\n",
    "\n",
    "    :param data_start_time_to_index:\n",
    "    :rtype: object\n",
    "    :return up_intercepts, down_intercepts:\n",
    "    :param data:\n",
    "    :param mac:\n",
    "    \"\"\"\n",
    "\n",
    "    up_intercepts = []\n",
    "    down_intercepts = []\n",
    "\n",
    "    subsets = []\n",
    "    for i in range(len(mac)):\n",
    "        data_index = data_start_time_to_index[mac[i,0]]\n",
    "        subsets.append(data[data_index,1])\n",
    "    # plt.plot(data[:,0],data[:,1])\n",
    "    # plt.plot(mac[:,0],mac[:,1])\n",
    "    # # plt.plot(mac[:,0],subsets)\n",
    "    # plt.show()\n",
    "    if len(subsets) == len(mac):\n",
    "        for i in range(len(mac) - 1):\n",
    "            if subsets[i] <= mac[i,1] and mac[i + 1,1] <= subsets[i + 1]:\n",
    "                up_intercepts.append(mac[i + 1])\n",
    "            elif subsets[i] >= mac[i,1] and mac[i + 1,1] >= subsets[i + 1]:\n",
    "                down_intercepts.append(mac[i + 1])\n",
    "    else:\n",
    "        raise Exception(\"Data sample not found at Moving Average Curve.\")\n",
    "\n",
    "    return up_intercepts, down_intercepts\n",
    "\n",
    "def filter_intercept_outlier(up_intercepts: List,\n",
    "                             down_intercepts: List) -> [List,List]:\n",
    "    \"\"\"\n",
    "    Remove two or more consecutive up or down intercepts.\n",
    "\n",
    "    :rtype: object\n",
    "    :return up_intercepts_updated, down_intercepts_updated:\n",
    "    :param up_intercepts:\n",
    "    :param down_intercepts:\n",
    "    \"\"\"\n",
    "\n",
    "    up_intercepts_filtered = []\n",
    "    down_intercepts_filtered = []\n",
    "\n",
    "    for index in range(len(down_intercepts) - 1):\n",
    "        up_intercepts_between_down_intercepts = []\n",
    "        for ui in up_intercepts:\n",
    "            if down_intercepts[index][0] <= ui[0] <= down_intercepts[index + 1][0]:\n",
    "                up_intercepts_between_down_intercepts.append(ui)\n",
    "\n",
    "        if len(up_intercepts_between_down_intercepts) > 0:\n",
    "            up_intercepts_filtered.append(up_intercepts_between_down_intercepts[-1])\n",
    "\n",
    "    up_intercepts_after_down = [ui for ui in up_intercepts if ui[0] > down_intercepts[-1][0]]\n",
    "    if len(up_intercepts_after_down) > 0:\n",
    "        up_intercepts_filtered.append(up_intercepts_after_down[-1])\n",
    "\n",
    "    down_intercepts_before_up = [di for di in down_intercepts if di[0] < up_intercepts_filtered[0][0]]\n",
    "    if len(down_intercepts_before_up) > 0:\n",
    "        down_intercepts_filtered.append(down_intercepts_before_up[-1])\n",
    "\n",
    "    for index in range(len(up_intercepts_filtered) - 1):\n",
    "        down_intercepts_between_up_intercepts = []\n",
    "        for di in down_intercepts:\n",
    "            if up_intercepts_filtered[index][0] <= di[0] <= up_intercepts_filtered[index + 1][0]:\n",
    "                down_intercepts_between_up_intercepts.append(di)\n",
    "\n",
    "        if len(down_intercepts_between_up_intercepts) > 0:\n",
    "            down_intercepts_filtered.append(down_intercepts_between_up_intercepts[-1])\n",
    "\n",
    "    down_intercepts_after_up = [di for di in down_intercepts if di[0] > up_intercepts_filtered[-1][0]]\n",
    "    if len(down_intercepts_after_up) > 0:\n",
    "        down_intercepts_filtered.append(down_intercepts_after_up[-1])\n",
    "\n",
    "    up_intercepts_truncated = []\n",
    "    for ui in up_intercepts_filtered:\n",
    "        if ui[0] >= down_intercepts_filtered[0][0]:\n",
    "            up_intercepts_truncated.append(ui)\n",
    "\n",
    "    min_length = min(len(up_intercepts_truncated), len(down_intercepts_filtered))\n",
    "\n",
    "    up_intercepts_updated = up_intercepts_truncated[:min_length]\n",
    "    down_intercepts_updated = down_intercepts_filtered[:min_length]\n",
    "\n",
    "    return up_intercepts_updated, down_intercepts_updated\n",
    "\n",
    "\n",
    "\n",
    "def generate_peak_valley(up_intercepts: List,\n",
    "                         down_intercepts: List,\n",
    "                         data: np.ndarray) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Compute peak valley from up intercepts and down intercepts indices.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks, valleys:\n",
    "    :param up_intercepts:\n",
    "    :param down_intercepts:\n",
    "    :param data:\n",
    "    \"\"\"\n",
    "    peaks = []\n",
    "    valleys = []\n",
    "\n",
    "    last_iterated_index = 0\n",
    "    for i in range(len(down_intercepts) - 1):\n",
    "        peak = None\n",
    "        valley = None\n",
    "\n",
    "        for j in range(last_iterated_index, len(data)):\n",
    "            if down_intercepts[i][0] <= data[j][0] <= up_intercepts[i][0]:\n",
    "                if valley is None or data[j][1] < valley[1]:\n",
    "                    valley = data[j]\n",
    "            elif up_intercepts[i][0] <= data[j][0] <= down_intercepts[i + 1][0]:\n",
    "                if peak is None or data[j][1] > peak[1]:\n",
    "                    peak = data[j]\n",
    "            elif data[j][0] > down_intercepts[i + 1][0]:\n",
    "                last_iterated_index = j\n",
    "                break\n",
    "        if peak is None or valley is None:\n",
    "            continue\n",
    "\n",
    "        valleys.append(valley)\n",
    "        peaks.append(peak)\n",
    "\n",
    "    return peaks, valleys\n",
    "\n",
    "\n",
    "def correct_valley_position(peaks: List,\n",
    "                            valleys: List,\n",
    "                            up_intercepts: List,\n",
    "                            data: np.ndarray,\n",
    "                            data_start_time_to_index: dict) -> List:\n",
    "    \"\"\"\n",
    "    Correct Valley position by locating actual valley using maximum slope algorithm which is\n",
    "    located between current valley and following peak.\n",
    "\n",
    "    Algorithm - push valley towards right:\n",
    "    Search for points lies in between current valley and following Up intercept.\n",
    "    Calculate slopes at those points.\n",
    "    Ensure that valley resides at the begining of inhalation cycle where inhalation slope is maximum.\n",
    "\n",
    "    :rtype: object\n",
    "    :return valley_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param up_intercepts:\n",
    "    :param data:\n",
    "    :param data_start_time_to_index: hash table for data where start_time is key, index is value\n",
    "    \"\"\"\n",
    "    valley_updated = valleys.copy()\n",
    "    for i in range(len(valleys)):\n",
    "        if valleys[i][0] < up_intercepts[i][0] < peaks[i][0]:\n",
    "            up_intercept = up_intercepts[i]\n",
    "\n",
    "            if valleys[i][0] not in data_start_time_to_index or up_intercept[0] not in data_start_time_to_index:\n",
    "                exception_message = 'Data has no start time for valley or up intercept start time at index ' + str(i)\n",
    "                raise Exception(exception_message)\n",
    "            else:\n",
    "                valley_index = data_start_time_to_index[valleys[i][0]]\n",
    "                up_intercept_index = data_start_time_to_index[up_intercept[0]]\n",
    "                data_valley_to_ui = data[valley_index: up_intercept_index + 1]\n",
    "                sample_valley_to_ui = data_valley_to_ui[:,1]\n",
    "\n",
    "                slope_at_samples = np.diff(sample_valley_to_ui)\n",
    "\n",
    "                consecutive_positive_slopes = [-1] * len(slope_at_samples)\n",
    "\n",
    "                for j in range(len(slope_at_samples)):\n",
    "                    slopes_subset = slope_at_samples[j:]\n",
    "                    if all(slope > 0 for slope in slopes_subset):\n",
    "                        consecutive_positive_slopes[j] = len(slopes_subset)\n",
    "\n",
    "                if any(no_con_slope > 0 for no_con_slope in consecutive_positive_slopes):\n",
    "                    indices_max_pos_slope = []\n",
    "                    for k in range(len(consecutive_positive_slopes)):\n",
    "                        if consecutive_positive_slopes[k] == max(consecutive_positive_slopes):\n",
    "                            indices_max_pos_slope.append(k)\n",
    "                    valley_updated[i] = data_valley_to_ui[indices_max_pos_slope[-1]]\n",
    "\n",
    "        else:\n",
    "            # TODO: discuss whether raise exception or not\n",
    "            # Up intercept at index i is not between valley and peak at index i.\n",
    "            break\n",
    "\n",
    "    return valley_updated\n",
    "\n",
    "\n",
    "def correct_peak_position(peaks: List,\n",
    "                          valleys: List,\n",
    "                          up_intercepts: List,\n",
    "                          data: np.ndarray,\n",
    "                          max_amplitude_change_peak_correction: float,\n",
    "                          min_neg_slope_count_peak_correction: int,\n",
    "                          data_start_time_to_index: dict) -> List:\n",
    "    \"\"\"\n",
    "    Correct peak position by checking if there is a notch in the inspiration branch at left position.\n",
    "    If at least 60% inspiration is done at a notch point, assume that notch as an original peak.\n",
    "    Our hypothesis is most of breathing in done for that cycle. We assume insignificant amount of\n",
    "    breath is taken or some new cycle started after the notch.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param up_intercepts:\n",
    "    :param data:\n",
    "    :param max_amplitude_change_peak_correction:\n",
    "    :param min_neg_slope_count_peak_correction:\n",
    "    :param data_start_time_to_index: hash table for data where start_time is key, index is value\n",
    "\n",
    "    \"\"\"\n",
    "    for i, item in enumerate(peaks):\n",
    "        if valleys[i][0] < up_intercepts[i][0] < peaks[i][0]:\n",
    "            up_intercept = up_intercepts[i]\n",
    "            # points between current valley and UI.\n",
    "            if up_intercept[0] not in data_start_time_to_index or peaks[i][0] not in data_start_time_to_index:\n",
    "                exception_message = 'Data has no start time for peak or up intercept start time at index ' + str(i)\n",
    "#                 raise Exception(exception_message)\n",
    "                return np.array([])\n",
    "            else:\n",
    "                data_up_intercept_index = data_start_time_to_index[up_intercept[0]]\n",
    "                data_peak_index = data_start_time_to_index[peaks[i][0]]\n",
    "\n",
    "                data_ui_to_peak = data[data_up_intercept_index: data_peak_index + 1]\n",
    "\n",
    "                sample_ui_to_peak = data_ui_to_peak[:,1]\n",
    "                slope_at_samples = np.diff(sample_ui_to_peak)\n",
    "\n",
    "                if not all(j >= 0 for j in slope_at_samples):\n",
    "                    indices_neg_slope = [j for j in range(len(slope_at_samples)) if slope_at_samples[j] < 0]\n",
    "                    peak_new = data_ui_to_peak[indices_neg_slope[0]]\n",
    "                    valley_peak_dist_new = peak_new[1] - valleys[i][1]\n",
    "                    valley_peak_dist_prev = peaks[i][1] - valleys[i][1]\n",
    "                    if valley_peak_dist_new == 0:\n",
    "                        return np.array([])\n",
    "                    else:\n",
    "                        amplitude_change = (valley_peak_dist_prev - valley_peak_dist_new) / valley_peak_dist_new * 100.0\n",
    "\n",
    "                        if len(indices_neg_slope) >= min_neg_slope_count_peak_correction:\n",
    "                            if amplitude_change <= max_amplitude_change_peak_correction:\n",
    "                                peaks[i] = peak_new  # 60% inspiration is done at that point.\n",
    "\n",
    "        else:\n",
    "            # TODO: Discuss whether raise exception or not for this scenario.\n",
    "            break  # up intercept at i is not between valley and peak at i\n",
    "\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def remove_close_valley_peak_pair(peaks: List,\n",
    "                                  valleys: List,\n",
    "                                  minimum_peak_to_valley_time_diff: float = 0.31) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out too close valley peak pair.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param minimum_peak_to_valley_time_diff:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = []\n",
    "\n",
    "    for i, item in enumerate(peaks):\n",
    "        time_diff_valley_peak = peaks[i][0] - valleys[i][0]\n",
    "        if time_diff_valley_peak/1000 > minimum_peak_to_valley_time_diff:\n",
    "            peaks_updated.append(peaks[i])\n",
    "            valleys_updated.append(valleys[i])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "def filter_expiration_duration_outlier(peaks: List,\n",
    "                                       valleys: List,\n",
    "                                       threshold_expiration_duration: float) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair for which expiration duration is too small.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param threshold_expiration_duration:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = [valleys[0]]\n",
    "\n",
    "    for i, item in enumerate(peaks):\n",
    "        if i < len(peaks) - 1:\n",
    "            expiration_duration = valleys[i + 1][0] - peaks[i][0]\n",
    "            if expiration_duration/1000 > threshold_expiration_duration:\n",
    "                peaks_updated.append(peaks[i])\n",
    "                valleys_updated.append(valleys[i + 1])\n",
    "\n",
    "    peaks_updated.append(peaks[-1])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def filter_small_amp_expiration_peak_valley(peaks: List,\n",
    "                                            valleys: List,\n",
    "                                            expiration_amplitude_threshold_perc: float) -> [List, List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair if their expiration amplitude is less than or equal to 10% of\n",
    "    average expiration amplitude.\n",
    "\n",
    "    :rtype: object\n",
    "    :return: peaks_updated, valleys_updated:\n",
    "    :param: peaks:\n",
    "    :param: valleys:\n",
    "    :param: expiration_amplitude_threshold_perc:\n",
    "    \"\"\"\n",
    "\n",
    "    expiration_amplitudes = []\n",
    "    peaks_updated = []\n",
    "    valleys_updated = [valleys[0]]\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        if i < len(peaks) - 1:\n",
    "            expiration_amplitudes.append(abs(valleys[i + 1][1] - peak[1]))\n",
    "\n",
    "    mean_expiration_amplitude = np.mean(expiration_amplitudes)\n",
    "\n",
    "    for i, expiration_amplitude in enumerate(expiration_amplitudes):\n",
    "        if expiration_amplitude > expiration_amplitude_threshold_perc * mean_expiration_amplitude:\n",
    "            peaks_updated.append(peaks[i])\n",
    "            valleys_updated.append(valleys[i + 1])\n",
    "\n",
    "    peaks_updated.append(peaks[-1])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def filter_small_amp_inspiration_peak_valley(peaks: List,\n",
    "                                             valleys: List,\n",
    "                                             inspiration_amplitude_threshold_perc: float) -> [List,List]:\n",
    "    \"\"\"\n",
    "    Filter out peak valley pair if their inspiration amplitude is less than or to equal 10% of\n",
    "    average inspiration amplitude.\n",
    "\n",
    "    :rtype: object\n",
    "    :return peaks_updated, valleys_updated:\n",
    "    :param peaks:\n",
    "    :param valleys:\n",
    "    :param inspiration_amplitude_threshold_perc:\n",
    "    \"\"\"\n",
    "\n",
    "    peaks_updated = []\n",
    "    valleys_updated = []\n",
    "\n",
    "    inspiration_amplitudes = [(peaks[i][1] - valleys[i][1]) for i, valley in enumerate(valleys)]\n",
    "    mean_inspiration_amplitude = np.mean(inspiration_amplitudes)\n",
    "\n",
    "    for i, inspiration_amplitude in enumerate(inspiration_amplitudes):\n",
    "        if inspiration_amplitude > inspiration_amplitude_threshold_perc * mean_inspiration_amplitude:\n",
    "            valleys_updated.append(valleys[i])\n",
    "            peaks_updated.append(peaks[i])\n",
    "\n",
    "    return peaks_updated, valleys_updated\n",
    "\n",
    "\n",
    "def compute_peak_valley(rip: np.ndarray,\n",
    "                        fs: float = 21.33,\n",
    "                        smoothing_factor: int = 5,\n",
    "                        time_window: int = 8,\n",
    "                        expiration_amplitude_threshold_perc: float = 0.10,\n",
    "                        threshold_expiration_duration: float = 0.312,\n",
    "                        inspiration_amplitude_threshold_perc: float = 0.10,\n",
    "                        max_amplitude_change_peak_correction: float = 30,\n",
    "                        min_neg_slope_count_peak_correction: int = 4,\n",
    "                        minimum_peak_to_valley_time_diff=0.31) -> [np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute peak and valley from rip data and filter peak and valley.\n",
    "\n",
    "    :rtype: object\n",
    "    :param minimum_peak_to_valley_time_diff:\n",
    "    :param inspiration_amplitude_threshold_perc:\n",
    "    :param smoothing_factor:\n",
    "    :return peak_datastream, valley_datastream:\n",
    "    :param rip:\n",
    "    :param rip_quality:\n",
    "    :param fs:\n",
    "    :param time_window:\n",
    "    :param expiration_amplitude_threshold_perc:\n",
    "    :param threshold_expiration_duration:\n",
    "    :param max_amplitude_change_peak_correction:\n",
    "    :param min_neg_slope_count_peak_correction:\n",
    "    \"\"\"\n",
    "\n",
    "    rip_filtered = rip\n",
    "#     print(rip_filtered.shape)\n",
    "    data_smooth = smooth(data=rip_filtered, span=smoothing_factor)\n",
    "#     print(data_smooth.shape)\n",
    "    window_length = int(round(time_window * fs))\n",
    "    # plt.figure()\n",
    "    # plt.plot(rip_filtered[:,0],rip_filtered[:,1])\n",
    "    # plt.plot(data_smooth[:,0],data_smooth[:,1])\n",
    "    # # plt.plot(data_mac[:,0],data_mac[:,1])\n",
    "    # plt.show()\n",
    "    data_mac = moving_average_curve(deepcopy(data_smooth), window_length=window_length)\n",
    "\n",
    "    data_smooth_start_time_to_index = {}\n",
    "    for index, data in enumerate(data_smooth):\n",
    "        data_smooth_start_time_to_index[data_smooth[index,0]] = index\n",
    "\n",
    "    up_intercepts, down_intercepts = up_down_intercepts(data=data_smooth,\n",
    "                                                        mac=data_mac,\n",
    "                                                        data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "    if len(up_intercepts)<3 or len(down_intercepts)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # print(up_intercepts,down_intercepts)\n",
    "    up_intercepts_filtered, down_intercepts_filtered = filter_intercept_outlier(up_intercepts=up_intercepts,\n",
    "                                                                                down_intercepts=down_intercepts)\n",
    "    if len(up_intercepts_filtered)<3 or len(down_intercepts_filtered)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    peaks, valleys = generate_peak_valley(up_intercepts=up_intercepts_filtered,\n",
    "                                          down_intercepts=down_intercepts_filtered,\n",
    "                                          data=data_smooth)\n",
    "    if len(peaks)<3 or len(valleys)<3:\n",
    "        return np.array([]),np.array([])\n",
    "\n",
    "    valleys_corrected = correct_valley_position(peaks=peaks,\n",
    "                                                valleys=valleys,\n",
    "                                                up_intercepts=up_intercepts_filtered,\n",
    "                                                data=data_smooth,\n",
    "                                                data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "\n",
    "    if len(valleys_corrected)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    peaks_corrected = correct_peak_position(peaks=peaks,\n",
    "                                            valleys=valleys_corrected,\n",
    "                                            up_intercepts=up_intercepts_filtered,\n",
    "                                            data=data_smooth,\n",
    "                                            max_amplitude_change_peak_correction=max_amplitude_change_peak_correction,\n",
    "                                            min_neg_slope_count_peak_correction=min_neg_slope_count_peak_correction,\n",
    "                                            data_start_time_to_index=data_smooth_start_time_to_index)\n",
    "    if len(peaks_corrected)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # remove too close valley peak pair.\n",
    "    peaks_filtered_close, valleys_filtered_close = remove_close_valley_peak_pair(peaks=peaks_corrected,\n",
    "                                                                                 valleys=valleys_corrected,\n",
    "                                                                                 minimum_peak_to_valley_time_diff=minimum_peak_to_valley_time_diff)\n",
    "    if len(peaks_filtered_close)<3 or len(valleys_filtered_close)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # Remove small  Expiration duration < 0.31\n",
    "    peaks_filtered_exp_dur, valleys_filtered_exp_dur = filter_expiration_duration_outlier(peaks=peaks_filtered_close,\n",
    "                                                                                          valleys=valleys_filtered_close,\n",
    "                                                                                          threshold_expiration_duration=threshold_expiration_duration)\n",
    "    if len(peaks_filtered_exp_dur)<3 or len(valleys_filtered_exp_dur)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # filter out peak valley pair of inspiration of small amplitude.\n",
    "    peaks_filtered_insp_amp, valleys_filtered_insp_amp = filter_small_amp_inspiration_peak_valley(\n",
    "        peaks=peaks_filtered_exp_dur,\n",
    "        valleys=valleys_filtered_exp_dur,\n",
    "        inspiration_amplitude_threshold_perc=inspiration_amplitude_threshold_perc)\n",
    "    if len(peaks_filtered_insp_amp)<3 or len(valleys_filtered_insp_amp)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    # filter out peak valley pair of expiration of small amplitude.\n",
    "    peaks_filtered_exp_amp, valleys_filtered_exp_amp = filter_small_amp_expiration_peak_valley(\n",
    "        peaks=peaks_filtered_insp_amp,\n",
    "        valleys=valleys_filtered_insp_amp,\n",
    "        expiration_amplitude_threshold_perc=expiration_amplitude_threshold_perc)\n",
    "    if len(peaks_filtered_exp_amp)<3 or len(valleys_filtered_exp_amp)<3:\n",
    "        return np.array([]),np.array([])\n",
    "    \n",
    "    peaks_filtered_exp_amp, valleys_filtered_exp_amp = np.array(peaks_filtered_exp_amp), np.array(valleys_filtered_exp_amp)\n",
    "    # peaks_filtered_exp_amp = np.insert(peaks_filtered_exp_amp,\n",
    "    # peaks_filtered_exp_amp =  np.insert(peaks_filtered_exp_amp, 2, 5, axis=1)\n",
    "    # valleys_filtered_exp_amp =  np.insert(valleys_filtered_exp_amp, 2, 5, axis=1)\n",
    "    # peaks_filtered_exp_amp[:,2] = [data_smooth_start_time_to_index[i[0]] for i in peaks_filtered_exp_amp]\n",
    "    # valleys_filtered_exp_amp[:,2] = [data_smooth_start_time_to_index[i[0]] for i in valleys_filtered_exp_amp]\n",
    "    return np.array(itemgetter(*list(peaks_filtered_exp_amp[:,0]))(data_smooth_start_time_to_index)),\\\n",
    "    np.array(itemgetter(*list(valleys_filtered_exp_amp[:,0]))(data_smooth_start_time_to_index))\n",
    "\n",
    "rip_stream = 'org.md2k.autosense.rip.quality.per.sample'\n",
    "rip_data = CC.get_stream(rip_stream)\n",
    "\n",
    "rip_data.show(5,False)\n",
    "rip_clean_data = rip_data.filter(F.col('quality')=='Acceptable')\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"indicator\", StringType())\n",
    "])\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(key,data):\n",
    "    if data.shape[0]>2*21.33*10:\n",
    "        data = data.sort_values('time').reset_index(drop=True)\n",
    "        r = np.zeros((data.shape[0],2))\n",
    "        r[:,0] = 1000*data['time'].values\n",
    "        r[:,1] = data['respiration'].values\n",
    "        peak_index,valley_index = compute_peak_valley(r)\n",
    "        data['indicator'] = 1\n",
    "        data['indicator'].loc[peak_index] = 'p'\n",
    "        data['indicator'].loc[valley_index] = 'v'\n",
    "        data = data[data.indicator.isin(['p','v'])]\n",
    "        return data[['timestamp','localtime','version','user','indicator','respiration']]\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['timestamp','localtime','version','user','indicator','respiration'])\n",
    "rip_clean_data = rip_clean_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "peak_valley_data = rip_clean_data.compute(peak_valley,windowDuration=60,startTime='0 seconds')\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.peak.valley'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Peak Valley of Autosense Respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"indicator\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"Peak and valley indicator in respiration signal\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration amplitude\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','Peak Valley').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "\n",
    "peak_valley_data.metadata = stream_metadata\n",
    "\n",
    "CC.save_stream(peak_valley_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.peak.valley').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rip_cycle_feature_computation(peaks_datastream: np.ndarray,\n",
    "                                  valleys_datastream: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Respiration Feature Implementation. The respiration feature values are\n",
    "    derived from the following paper:\n",
    "    'puffMarker: a multi-sensor approach for pinpointing the timing of first lapse in smoking cessation'\n",
    "    Removed due to lack of current use in the implementation\n",
    "    roc_max = []  # 8. ROC_MAX = max(sample[j]-sample[j-1])\n",
    "    roc_min = []  # 9. ROC_MIN = min(sample[j]-sample[j-1])\n",
    "\n",
    "    :param peaks_datastream: list of peak datapoints\n",
    "    :param valleys_datastream: list of valley datapoints\n",
    "    :return: lists of DataPoints each representing a specific feature calculated from the respiration cycle\n",
    "    found from the peak valley inputs\n",
    "    \"\"\"\n",
    "\n",
    "    inspiration_duration = []  # 1 Inhalation duration\n",
    "    expiration_duration = []  # 2 Exhalation duration\n",
    "    respiration_duration = []  # 3 Respiration duration\n",
    "    inspiration_expiration_ratio = []  # 4 Inhalation and Exhalation ratio\n",
    "    stretch = []  # 5 Stretch\n",
    "    upper_stretch = []  # 6. Upper portion of the stretch calculation\n",
    "    lower_stretch = []  # 7. Lower portion of the stretch calculation\n",
    "    delta_previous_inspiration_duration = []  # 10. BD_INSP = INSP(i)-INSP(i-1)\n",
    "    delta_previous_expiration_duration = []  # 11. BD_EXPR = EXPR(i)-EXPR(i-1)\n",
    "    delta_previous_respiration_duration = []  # 12. BD_RESP = RESP(i)-RESP(i-1)\n",
    "    delta_previous_stretch_duration = []  # 14. BD_Stretch= Stretch(i)-Stretch(i-1)\n",
    "    delta_next_inspiration_duration = []  # 19. FD_INSP = INSP(i)-INSP(i+1)\n",
    "    delta_next_expiration_duration = []  # 20. FD_EXPR = EXPR(i)-EXPR(i+1)\n",
    "    delta_next_respiration_duration = []  # 21. FD_RESP = RESP(i)-RESP(i+1)\n",
    "    delta_next_stretch_duration = []  # 23. FD_Stretch= Stretch(i)-Stretch(i+1)\n",
    "    neighbor_ratio_expiration_duration = []  # 29. D5_EXPR(i) = EXPR(i) / avg(EXPR(i-2)...EXPR(i+2))\n",
    "    neighbor_ratio_stretch_duration = []  # 32. D5_Stretch = Stretch(i) / avg(Stretch(i-2)...Stretch(i+2))\n",
    "\n",
    "    valleys = valleys_datastream\n",
    "    peaks = peaks_datastream[:-1]\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "\n",
    "        delta = peak[0] - valleys[i][0]\n",
    "        inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - peak[0]\n",
    "        expiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - valley_start_time\n",
    "        respiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        ratio = (peak[0] - valley_start_time) / (valleys[i + 1][0] - peak[0])\n",
    "        inspiration_expiration_ratio.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        value = peak[1] - valleys[i + 1][1]\n",
    "        stretch.append(np.array([valley_start_time,valley_end_time,value]))\n",
    "\n",
    "    for i, point in enumerate(inspiration_duration):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "        if i == 0:  # Edge case\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i - 1][2]\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i - 1][2]\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i - 1][2]\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i - 1][2]\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        if i == len(inspiration_duration) - 1:\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i + 1][2]\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i + 1][2]\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i + 1][2]\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i + 1][2]\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        stretch_average = 0\n",
    "        expiration_average = 0\n",
    "        count = 0.0\n",
    "        for j in [-2, -1, 1, 2]:\n",
    "            if i + j < 0 or i + j >= len(inspiration_duration):\n",
    "                continue\n",
    "            stretch_average += stretch[i + j][2]\n",
    "            expiration_average += expiration_duration[i + j][2]\n",
    "            count += 1\n",
    "\n",
    "        stretch_average /= count\n",
    "        expiration_average /= count\n",
    "\n",
    "        ratio = stretch[i][2] / stretch_average\n",
    "        neighbor_ratio_stretch_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        ratio = expiration_duration[i][2] / expiration_average\n",
    "        neighbor_ratio_expiration_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "    # Begin assembling datastream for output\n",
    "    inspiration_duration_datastream = np.array(inspiration_duration)[1:-1]\n",
    "\n",
    "    expiration_duration_datastream = np.array(expiration_duration)[1:-1]\n",
    "\n",
    "    respiration_duration_datastream = np.array(respiration_duration)[1:-1]\n",
    "\n",
    "    inspiration_expiration_ratio_datastream = np.array(inspiration_expiration_ratio)[1:-1]\n",
    "\n",
    "    stretch_datastream = np.array(stretch)[1:-1]\n",
    "\n",
    "    delta_previous_inspiration_duration_datastream = np.array(delta_previous_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_expiration_duration_datastream = np.array(delta_previous_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_respiration_duration_datastream = np.array(delta_previous_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_stretch_duration_datastream = np.array(delta_previous_stretch_duration)[1:-1]\n",
    "\n",
    "    delta_next_inspiration_duration_datastream = np.array(delta_next_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_expiration_duration_datastream = np.array(delta_next_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_respiration_duration_datastream = np.array(delta_next_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_stretch_duration_datastream = np.array(delta_next_stretch_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_expiration_datastream = np.array(neighbor_ratio_expiration_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_stretch_datastream = np.array(neighbor_ratio_stretch_duration)[1:-1]\n",
    "\n",
    "    return np.concatenate([inspiration_duration_datastream,\n",
    "                           expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           inspiration_expiration_ratio_datastream[:,2].reshape(-1,1),\n",
    "                           stretch_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_expiration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_stretch_datastream[:,2].reshape(-1,1)],axis=1)\n",
    "stream_name = 'org.md2k.autosense.rip.peak.valley'\n",
    "\n",
    "data = CC.get_stream(stream_name)\n",
    "\n",
    "data_time = data.withColumn('time',1000*F.col('timestamp').cast('double'))\n",
    "\n",
    "# data_time.sort('timestamp').show(5,False)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"inspiration\", DoubleType()),\n",
    "    StructField(\"expiration\", DoubleType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"ieratio\", DoubleType()),\n",
    "    StructField(\"stretch\", DoubleType()),\n",
    "    StructField(\"pinspiration\", DoubleType()),\n",
    "    StructField(\"pexpiration\", DoubleType()),\n",
    "    StructField(\"prespiration\", DoubleType()),\n",
    "    StructField(\"pstretch\", DoubleType()),\n",
    "    StructField(\"ninspiration\", DoubleType()),\n",
    "    StructField(\"nexpiration\", DoubleType()),\n",
    "    StructField(\"nrespiration\", DoubleType()),\n",
    "    StructField(\"nstretch\", DoubleType()),\n",
    "    StructField(\"rexpiration\", DoubleType()),\n",
    "    StructField(\"rstretch\", DoubleType())    \n",
    "])\n",
    "\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(key,data):\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "    if data.shape[0]>8:\n",
    "        index_dict = {}\n",
    "        data = data.sort_values('time').reset_index(drop=True)\n",
    "        for i in range(data.shape[0]):\n",
    "            index_dict[data['time'].loc[i]] = i\n",
    "        peak_data = data[data.indicator.isin(['p'])]\n",
    "        valley_data = data[data.indicator.isin(['v'])]\n",
    "        peaks = peak_data[['time','respiration']].values\n",
    "        valleys = valley_data[['time','respiration']].values\n",
    "        features = rip_cycle_feature_computation(peaks,valleys)\n",
    "        timestamp_col = data['timestamp'].values\n",
    "        localtime_col = data['localtime'].values\n",
    "        user_id = data['user'].values[0]\n",
    "        version_id = data['version'].values[0]\n",
    "        data1 = pd.DataFrame()\n",
    "        ind_col_1 = np.array([index_dict[features[i][0]] for i in range(features.shape[0])])\n",
    "        ind_col_2 = np.array([index_dict[features[i][1]] for i in range(features.shape[0])])\n",
    "        timestamp_col_s = timestamp_col[ind_col_1]\n",
    "        timestamp_col_e = timestamp_col[ind_col_2]\n",
    "        localtime_col = localtime_col[ind_col_1]\n",
    "        data1['timestamp'] = timestamp_col_s\n",
    "        data1['start'] = timestamp_col_s\n",
    "        data1['end'] = timestamp_col_e\n",
    "        data1['localtime'] = localtime_col\n",
    "        data1['user'] = [user_id]*timestamp_col_e.reshape(-1).shape[0]\n",
    "        data1['version'] = [version_id]*timestamp_col_e.reshape(-1).shape[0]\n",
    "        for i in range(2,features.shape[1],1):\n",
    "            data1[columns[i-2]] = list(features[:,i])\n",
    "        return data1\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=columns+['user','version','timestamp','localtime','start','end'])\n",
    "\n",
    "\n",
    "respiration_features = data_time.compute(peak_valley,windowDuration=240,startTime='0 seconds')\n",
    "\n",
    "# respiration_features.sort('timestamp').show(5,False)\n",
    "respiration_features.printSchema()\n",
    "\n",
    "stream_name = 'org.md2k.autosense.rip.cycle.features'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Features from autosense respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"start\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"start time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"end\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"end time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"inspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"expiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"expiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ieratio\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration to expiration ratio\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"stretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pinspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"prespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ninspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nrespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor stretch\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_peak_valley.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','cycle features').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "respiration_features.metadata = stream_metadata\n",
    "CC.save_stream(respiration_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o83.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: FAILED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:240)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.listLeafFiles(InMemoryFileIndex.scala:360)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.$anonfun$bulkListLeafFiles$1(InMemoryFileIndex.scala:195)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:187)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:568)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:406)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-220a85810d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.md2k.autosense.rip.cycle.features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/cerebralcortex/kernel.py\u001b[0m in \u001b[0;36mget_stream\u001b[0;34m(self, stream_name, version, user_id, data_type)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRawData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;31m###########################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/cerebralcortex/core/data_manager/raw/stream_handler.py\u001b[0m in \u001b[0;36mget_stream\u001b[0;34m(self, stream_name, version, user_id, data_type)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;31m#df = df.dropDuplicates(subset=['timestamp'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/cerebralcortex/core/data_manager/raw/filebased_storage.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(self, stream_name, version, user_id)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_storage_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o83.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: FAILED\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:240)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.listLeafFiles(InMemoryFileIndex.scala:360)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.$anonfun$bulkListLeafFiles$1(InMemoryFileIndex.scala:195)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:187)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:135)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:568)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:406)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.cycle.features').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_stream = 'org.md2k.autosense.rip.cycle.features'\n",
    "rr_intervals = CC.get_stream(rr_stream)\n",
    "rr_intervals.show(5,False)\n",
    "rr_intervals = rr_intervals.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_90 = []\n",
    "for c in columns:\n",
    "    percentile_90 = F.expr('percentile_approx('+str(c)+',0.90)')\n",
    "    temp = rr_intervals.groupBy(['user','day']).agg(percentile_90.alias(str(c)))\n",
    "    df_90.append(temp.toPandas())\n",
    "\n",
    "from copy import deepcopy\n",
    "df_90_final = deepcopy(df_90[0])\n",
    "for i in range(1,len(df_90),1):\n",
    "    df_90_final = df_90_final.join(df_90[i].set_index(['user','day']),on=['user','day'], how='left')\n",
    "\n",
    "df_90_final.set_index(['user','day'],inplace=True)\n",
    "\n",
    "df_10 = []\n",
    "for c in columns:\n",
    "    percentile_10 = F.expr('percentile_approx('+str(c)+',0.10)')\n",
    "    temp = rr_intervals.groupBy(['user','day']).agg(percentile_10.alias(str(c)))\n",
    "    df_10.append(temp.toPandas())\n",
    "\n",
    "from copy import deepcopy\n",
    "df_10_final = deepcopy(df_10[0])\n",
    "for i in range(1,len(df_10),1):\n",
    "    df_10_final = df_10_final.join(df_10[i].set_index(['user','day']),on=['user','day'], how='left')\n",
    "\n",
    "df_10_final.set_index(['user','day'],inplace=True)\n",
    "\n",
    "# df_10_final.loc['c64ca471-369e-43fa-a07b-8260fd1c745c','20190612']['inspiration']\n",
    "\n",
    "df_90_final.head()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"inspiration\", DoubleType()),\n",
    "    StructField(\"expiration\", DoubleType()),\n",
    "    StructField(\"respiration\", DoubleType()),\n",
    "    StructField(\"ieratio\", DoubleType()),\n",
    "    StructField(\"stretch\", DoubleType()),\n",
    "    StructField(\"pinspiration\", DoubleType()),\n",
    "    StructField(\"pexpiration\", DoubleType()),\n",
    "    StructField(\"prespiration\", DoubleType()),\n",
    "    StructField(\"pstretch\", DoubleType()),\n",
    "    StructField(\"ninspiration\", DoubleType()),\n",
    "    StructField(\"nexpiration\", DoubleType()),\n",
    "    StructField(\"nrespiration\", DoubleType()),\n",
    "    StructField(\"nstretch\", DoubleType()),\n",
    "    StructField(\"rexpiration\", DoubleType()),\n",
    "    StructField(\"rstretch\", DoubleType()),\n",
    "    StructField(\"day\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def peak_valley(data):\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','prespiration','pstretch',\n",
    "          'ninspiration','nexpiration','nrespiration','nstretch','rexpiration','rstretch']\n",
    "    if data.shape[0]>1:\n",
    "        user = data['user'].loc[0]\n",
    "        day = data['day'].loc[0]\n",
    "        for c in columns:\n",
    "            upper = df_90_final.loc[user,day][c]\n",
    "            lower = df_10_final.loc[user,day][c]\n",
    "            data[c][data[c]>upper] = upper\n",
    "            data[c][data[c]<lower] = lower\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=columns+['user','version','timestamp','localtime','start','end','day'])\n",
    "\n",
    "rr_intervals_winsorized = rr_intervals.groupBy(['user','day']).apply(peak_valley)\n",
    "\n",
    "rr_intervals_winsorized.show(5,False)\n",
    "stream_name = 'org.md2k.autosense.rip.cycle.features.winsorized'\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(stream_name).set_description(\"Features from autosense respiration\") \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"start\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"start time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"end\").set_type(\"timestamp\").set_attribute(\"description\", \\\n",
    "    \"end time of cycle\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"inspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"expiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"expiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"respiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"respiration duration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ieratio\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"inspiration to expiration ratio\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"stretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pinspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"prespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"pstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"previous cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"ninspiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle inspiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nrespiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle respiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"nstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"next cycle stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rexpiration\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor expiration\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"rstretch\").set_type(\"double\").set_attribute(\"description\", \\\n",
    "    \"ratio of neighbor stretch\")) \\\n",
    "    .add_dataDescriptor(\n",
    "    DataDescriptor().set_name(\"day\").set_type(\"string\").set_attribute(\"description\", \\\n",
    "    \"day localtime\")) \\\n",
    "    .add_module(\n",
    "    ModuleMetadata().set_name(\"fourtytwo/mullah/cc3/respiration_percentiles.ipynb\").set_attribute(\"url\", \\\n",
    "    \"http://md2k.org/\").set_attribute('algorithm','cycle features winsorized').set_attribute('unit', \\\n",
    "    'ms').set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "rr_intervals_winsorized_final =DataStream(data=rr_intervals_winsorized,metadata=stream_metadata)\n",
    "CC.save_stream(rr_intervals_winsorized_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.cycle.features.winsorized').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_name = 'org.md2k.autosense.rip.cycle.features.winsorized'\n",
    "df = CC.get_stream(stream_name)\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"m_inspiration\", DoubleType()),\n",
    "    StructField(\"s_inspiration\", DoubleType()),\n",
    "    StructField(\"80_inspiration\", DoubleType()),\n",
    "    StructField(\"20_inspiration\", DoubleType()),\n",
    "    StructField(\"m_expiration\", DoubleType()),\n",
    "    StructField(\"s_expiration\", DoubleType()),\n",
    "    StructField(\"80_expiration\", DoubleType()),\n",
    "    StructField(\"20_expiration\", DoubleType()),\n",
    "    StructField(\"m_respiration\", DoubleType()),\n",
    "    StructField(\"s_respiration\", DoubleType()),\n",
    "    StructField(\"80_respiration\", DoubleType()),\n",
    "    StructField(\"20_respiration\", DoubleType()),\n",
    "    StructField(\"m_ieratio\", DoubleType()),\n",
    "    StructField(\"s_ieratio\", DoubleType()),\n",
    "    StructField(\"80_ieratio\", DoubleType()),\n",
    "    StructField(\"20_ieratio\", DoubleType()),\n",
    "    StructField(\"m_stretch\", DoubleType()),\n",
    "    StructField(\"s_stretch\", DoubleType()),\n",
    "    StructField(\"80_stretch\", DoubleType()),\n",
    "    StructField(\"20_stretch\", DoubleType()),\n",
    "    StructField(\"m_pinspiration\", DoubleType()),\n",
    "    StructField(\"s_pinspiration\", DoubleType()),\n",
    "    StructField(\"80_pinspiration\", DoubleType()),\n",
    "    StructField(\"20_pinspiration\", DoubleType()),\n",
    "    StructField(\"m_pexpiration\", DoubleType()),\n",
    "    StructField(\"s_pexpiration\", DoubleType()),\n",
    "    StructField(\"80_pexpiration\", DoubleType()),\n",
    "    StructField(\"20_pexpiration\", DoubleType()),\n",
    "    StructField(\"m_rexpiration\", DoubleType()),\n",
    "    StructField(\"s_rexpiration\", DoubleType()),\n",
    "    StructField(\"80_rexpiration\", DoubleType()),\n",
    "    StructField(\"20_rexpiration\", DoubleType()),\n",
    "    StructField(\"m_rstretch\", DoubleType()),\n",
    "    StructField(\"s_rstretch\", DoubleType()),\n",
    "    StructField(\"80_rstretch\", DoubleType()),\n",
    "    StructField(\"20_rstretch\", DoubleType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def minute_level_rip_features(key,data):\n",
    "    ms82 = ['m_','s_','80_','20_']\n",
    "    all_columns = ['start','end','timestamp','localtime','user','version','day']\n",
    "    columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "    if data.shape[0]>5:\n",
    "        all_data = []\n",
    "        all_data.extend([key[2]['start'],key[2]['end'],data['timestamp'].loc[0],\n",
    "                        data['localtime'].loc[0],data['user'].loc[0],data['version'].loc[0],\n",
    "                        data['day'].loc[0]])\n",
    "        data = data[columns]\n",
    "        list_col = [list(data.mean()),list(data.std()),\n",
    "                    list(data.quantile(.8)),list(data.quantile(.2))]\n",
    "        for i,c in enumerate(ms82):\n",
    "            all_columns.extend([c+j for j in columns])\n",
    "            all_data.extend(list_col[i])\n",
    "        return pd.DataFrame([all_data],columns=all_columns)\n",
    "    else:\n",
    "        for i,c in enumerate(ms82):\n",
    "            all_columns.extend([c+j for j in columns])\n",
    "        return pd.DataFrame([],columns=all_columns)\n",
    "df_minutewise = df.compute(minute_level_rip_features,windowDuration=60,startTime='0 seconds')\n",
    "ms82 = ['m_','s_','80_','20_']\n",
    "all_columns = [F.struct('start', 'end').alias('window'),'timestamp',\n",
    "               'localtime','user','version','day']\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "      'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "for i,c in enumerate(ms82):\n",
    "    all_columns.extend([c+j for j in columns])\n",
    "df_minutewise = df_minutewise.select(*all_columns)\n",
    "df_minutewise.show(5)\n",
    "schema = df_minutewise._data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.minute.features').set_description(\"Respiration Minute Features\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration Minute Level Features\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "print(stream_metadata.is_valid(),'metadata')\n",
    "ds = DataStream(data=df_minutewise._data,metadata=stream_metadata)\n",
    "ds.printSchema()\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC.get_stream('org.md2k.autosense.rip.minute.features').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rip_data = CC.get_stream('org.md2k.autosense.rip.minute.features')\n",
    "activity_data = CC.get_stream('org.md2k.autosense.accel.activity.60seconds')\n",
    "\n",
    "df = rip_data.join(activity_data.drop(*['timestamp','localtime','version']),on=['user','window'],how='left')\n",
    "df = df.filter(F.col('activity')!=1)\n",
    "df = df.drop(*['activity'])\n",
    "df = df.withColumn('start',F.col('window').start)\n",
    "df = df.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "\n",
    "# df.printSchema()\n",
    "ms82 = ['m_','s_','80_','20_']\n",
    "all_columns = ['start','end','timestamp','localtime','user','version','day']\n",
    "columns = ['inspiration','expiration','respiration','ieratio',\n",
    "          'stretch','pinspiration','pexpiration','rexpiration','rstretch']\n",
    "feature_columns = []\n",
    "for i,c in enumerate(ms82):\n",
    "    feature_columns.extend([c+j for j in columns])\n",
    "    all_columns.extend([c+j for j in columns])\n",
    "\n",
    "df = df.select(*all_columns)\n",
    "# all_columns\n",
    "# df.count()\n",
    "# CC.get_stream('org.md2k.autosense.rip.minute.features').count()\n",
    "\n",
    "feature_columns_action_std = [F.stddev(i).alias(i) for i in feature_columns]\n",
    "feature_columns_action_mean = [F.mean(i).alias(i) for i in feature_columns]\n",
    "\n",
    "std_all = df.groupBy(['user','day']).agg(*feature_columns_action_std)\n",
    "mean_all = df.groupBy(['user','day']).agg(*feature_columns_action_mean)\n",
    "\n",
    "mean_all = mean_all.toPandas()\n",
    "std_all = std_all.toPandas()\n",
    "mean_all.set_index(['user','day'],inplace=True)\n",
    "std_all.set_index(['user','day'],inplace=True)\n",
    "\n",
    "basic_schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType()),\n",
    "            StructField(\"localtime\", TimestampType()),\n",
    "            StructField(\"user\", StringType()),\n",
    "            StructField(\"day\", StringType()),\n",
    "            StructField(\"version\", IntegerType()),\n",
    "            StructField(\"start\", TimestampType()),\n",
    "            StructField(\"end\", TimestampType())\n",
    "])\n",
    "\n",
    "features_list = []\n",
    "for c in feature_columns:\n",
    "    features_list.append(StructField(c, DoubleType(), True))\n",
    "features_schema = StructType(basic_schema.fields + features_list)\n",
    "\n",
    "@pandas_udf(features_schema, PandasUDFType.GROUPED_MAP)\n",
    "def standardize_rip_feature(data):\n",
    "    if data.shape[0]>10:\n",
    "        for c in feature_columns:\n",
    "            m = mean_all.loc[data.loc[0]['user'],data.loc[0]['day']][c]\n",
    "            s = std_all.loc[data.loc[0]['user'],data.loc[0]['day']][c]\n",
    "            data[c] = (data[c] - m)/s\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=all_columns)\n",
    "\n",
    "df_standardized = df.groupBy(['user','day']).apply(standardize_rip_feature)\n",
    "columns = ['m_inspiration', 'm_expiration', 'm_respiration',\n",
    "       'm_ieratio', 'm_stretch', 'm_pinspiration', 'm_pexpiration',\n",
    "       'm_rexpiration', 'm_rstretch', 's_inspiration', 's_expiration',\n",
    "       's_respiration', 's_ieratio', 's_stretch', 's_pinspiration',\n",
    "       's_pexpiration', 's_rexpiration', 's_rstretch', '80_inspiration',\n",
    "       '80_expiration', '80_respiration', '80_ieratio', '80_stretch',\n",
    "       '80_pinspiration', '80_pexpiration', '80_rexpiration', '80_rstretch',\n",
    "       '20_inspiration', '20_expiration', '20_respiration', '20_ieratio',\n",
    "       '20_stretch', '20_pinspiration', '20_pexpiration', '20_rexpiration',\n",
    "       '20_rstretch']\n",
    "print(len(feature_columns),len(columns))\n",
    "df_standardized_array = df_standardized.withColumn('features',F.array(*columns)).drop(*feature_columns)\n",
    "\n",
    "df_standardized_array.show(5,False)\n",
    "\n",
    "df_standardized_array = df_standardized_array.select(\"user\",'version','timestamp','localtime','day',\n",
    "              F.struct('start', 'end').alias('window'),'features')\n",
    "\n",
    "schema = df_standardized_array.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.minute.features.standardized.final').set_description(\"RIP features standardized\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration features standardized\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=df_standardized_array,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CC.get_stream('org.md2k.autosense.rip.minute.features.standardized.final')\n",
    "\n",
    "# df.show(5,False)\n",
    "\n",
    "df = df.withColumn('start',F.col('window').start)\n",
    "df = df.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "\n",
    "schema = StructType([StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"stress_likelihood\", DoubleType())\n",
    "])\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "ecg_model = pickle.load(open('rip_model.p','rb'))\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def ecg_r_peak(key,data):\n",
    "    if data.shape[0]>0:\n",
    "        features = []\n",
    "        for i in range(data.shape[0]):\n",
    "            features.append(np.array(data['features'][i]))\n",
    "        features = np.array(features)\n",
    "        features[features>10] = 10\n",
    "        features[features<-10] = -10\n",
    "        probs = ecg_model.predict_proba(np.nan_to_num(features))[:,1]\n",
    "        data['stress_likelihood'] = probs\n",
    "        data = data[['timestamp','start','end','version','user','day',\n",
    "                     'localtime','stress_likelihood']]\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['timestamp','version','user','day',\n",
    "                                        'localtime','stress_likelihood','start','end'])\n",
    "\n",
    "df_stress = df.compute(ecg_r_peak,windowDuration=6000,startTime='0 seconds')\n",
    "\n",
    "df_stress.show(4,False)\n",
    "\n",
    "df_stress = df_stress.select(\"user\",'version','timestamp','localtime','day',\n",
    "              F.struct('start', 'end').alias('window'),'stress_likelihood')\n",
    "\n",
    "schema = df_stress._data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.rip.stress.likelihood').set_description(\"Stress from Respiration\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration stress likelihood\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=df_stress._data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_data = CC.get_stream('org.md2k.autosense.rip.stress.likelihood')._data.toPandas()\n",
    "# quality_data = CC.get_stream('org.md2k.autosense.rip.quality.60seconds')._data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.plot(stress_data['timestamp'][stress_data.user==stress_data['user'].iloc[20000]],stress_data['stress_likelihood'][stress_data.user==stress_data['user'].iloc[20000]])\n",
    "plt.show()\n",
    "stress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(stress_data,open('./rice_data/stress_rip_only_no_c6_nw.p','wb'))\n",
    "pickle.dump(quality_data,open('./rice_data/rip_quality_60_seconds_nw.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CC.get_stream('org.md2k.motionsensehrv.ppg.left.stress.md2k_aa_rice.imputed.ffill')\n",
    "emas = CC.get_stream(\"perceived.stress.score--org.md2k.ema_scheduler--phone\").drop(*['timestamp',\n",
    "                                                                                    'localtime',\n",
    "                                                                                    'version'])\n",
    "all_stress = ds.join(emas,on=['user','window'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
