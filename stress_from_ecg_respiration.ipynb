{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'dartmouth_sobc', 'default', 'demo', 'jhu_cocaine', 'mars_study', 'mcontain', 'md2k_aa_rice', 'md2k_affsci', 'md2k_labtest', 'md2k_ses_utah', 'md2k_test', 'memphis-test', 'memphis_test_study', 'moffitt', 'moffitt-test', 'moods', 'moods_backup', 'moral', 'mperf', 'mperf-alabsi', 'mperf-buder', 'mperf-mit-ll', 'mperf-test', 'northwestern_smoking', 'nu', 'opioid_study', 'osu', 'rice', 'robas', 'robas_study', 'sobclab', 'test', 'utah', 'utah_p01', 'vermont', 'vermont_smoking', 'wesad']\n"
     ]
    }
   ],
   "source": [
    "from cerebralcortex.util.helper_methods import get_study_names\n",
    "sn = get_study_names(\"/home/jupyter/cc3_conf/\")\n",
    "print(sn)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "ModuleMetadata\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "from cerebralcortex import Kernel\n",
    "CC = Kernel(\"/home/jupyter/cc3_conf/\", study_name='rice')\n",
    "# CC.list_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-----------------------+-------+------------------------------------+--------+--------------------+--------+\n",
      "|timestamp              |start              |end                |localtime              |version|user                                |day     |stress_likelihood   |activity|\n",
      "+-----------------------+-------------------+-------------------+-----------------------+-------+------------------------------------+--------+--------------------+--------+\n",
      "|2017-09-18 16:10:01.369|2017-09-18 16:09:30|2017-09-18 16:10:30|2017-09-18 10:10:01.369|1      |0c155e6b-410c-329e-8a06-66d01424ad53|20170918|0.03436288722751523 |0.0     |\n",
      "|2017-09-18 16:10:01.369|2017-09-18 16:09:25|2017-09-18 16:10:25|2017-09-18 10:10:01.369|1      |0c155e6b-410c-329e-8a06-66d01424ad53|20170918|0.10128116382753467 |0.0     |\n",
      "|2017-09-18 16:10:01.369|2017-09-18 16:09:45|2017-09-18 16:10:45|2017-09-18 10:10:01.369|1      |0c155e6b-410c-329e-8a06-66d01424ad53|20170918|0.01908942377353665 |1.0     |\n",
      "|2017-09-18 16:10:01.369|2017-09-18 16:09:50|2017-09-18 16:10:50|2017-09-18 10:10:01.369|1      |0c155e6b-410c-329e-8a06-66d01424ad53|20170918|0.022652298641998116|1.0     |\n",
      "+-----------------------+-------------------+-------------------+-----------------------+-------+------------------------------------+--------+--------------------+--------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_stream = 'org.md2k.autosense.ecg.features.stndardized.rice.no.activity'\n",
    "ecg_features = CC.get_stream(feature_stream).dropna()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "rip_features = CC.get_stream('org.md2k.autosense.rip.minute.features.standardized.final')\n",
    "\n",
    "rip_features = rip_features.drop('timestamp','localtime','activity','day','version').withColumnRenamed('features','features_rip')\n",
    "\n",
    "features = ecg_features.join(rip_features,on=['window','user'],how='inner')\n",
    "\n",
    "df = features.withColumn('start',F.col('window').start)\n",
    "df = df.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"stress_likelihood\", DoubleType()),\n",
    "    StructField(\"activity\", DoubleType())\n",
    "])\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pickle\n",
    "from scipy import stats\n",
    "ecg_model = pickle.load(open('./models/ecg_rip_model_final.p','rb'))\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def ecg_r_peak(key,data):\n",
    "    data = data.sort_values('timestamp').reset_index(drop=True)\n",
    "    if data.shape[0]>60*12*.5:\n",
    "        features = []\n",
    "        ecg_features = np.array(data['features'])\n",
    "        rip_features = np.array(data['features_rip'])\n",
    "        for i in range(data.shape[0]):\n",
    "            features.append(np.array(list(ecg_features[i])+list(rip_features[i])))\n",
    "        features = np.nan_to_num(np.array(features))\n",
    "#         for c in range(features.shape[1]):\n",
    "#             features[:,c] = smooth(features[:,c],36)\n",
    "        features = StandardScaler().fit_transform(np.nan_to_num(features))\n",
    "#         features[features>=5] = 5\n",
    "#         features[features<=-5] = -5\n",
    "        probs = ecg_model.predict_proba(np.nan_to_num(features))[:,1]\n",
    "        data['stress_likelihood'] = probs\n",
    "        data = data[['timestamp','start','end','version','user','day','activity',\n",
    "                     'localtime','stress_likelihood']]\n",
    "        return data\n",
    "    else:\n",
    "        return pd.DataFrame([],columns=['timestamp','version','user','day','activity',\n",
    "                                        'localtime','stress_likelihood','start','end'])\n",
    "\n",
    "df_stress = df.groupBy(['user','day','version']).apply(ecg_r_peak)\n",
    "\n",
    "df_stress.show(4,False)\n",
    "\n",
    "df_stress = df_stress.select(\"user\",'version','timestamp','localtime','day','activity',\n",
    "              F.struct('start', 'end').alias('window'),'stress_likelihood')\n",
    "\n",
    "schema = df_stress.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name('org.md2k.autosense.ecg.rip.stress.likelihood').set_description(\"Stress from Respiration\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Respiration stress likelihood\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=df_stress,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('sts',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('pca',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=10,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('rf',\n",
       "                 SVC(C=10, break_ties=False, cache_size=2000,\n",
       "                     class_weight={0: 0.4, 1: 0.6}, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma=0.004641588833612777, kernel='rbf', max_iter=-1,\n",
       "                     probability=True, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user: string, version: int, timestamp: timestamp, localtime: timestamp, day: string, activity: double, window: struct<start:timestamp,end:timestamp>, stress_likelihood: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream('org.md2k.autosense.ecg.rip.stress.likelihood')\n",
    "\n",
    "data = data.toPandas()\n",
    "import pickle\n",
    "pickle.dump(data,open('./data/rice_1st_version_ecg_rip1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6072523, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pickle.load(open('./data/rice_1st_version_ecg_rip1.p','rb'))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(data['stress_likelihood'],200)\n",
    "plt.show()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.user.unique()[:10],data.shape\n",
    "\n",
    "data.groupby('user').count().sort_values('timestamp')[::-1]\n",
    "\n",
    "# stress_data = data[data.user=='6c1a560d-85da-3c3b-b655-c46592e70406']\n",
    "stress_data = data\n",
    "count = 0\n",
    "for day in stress_data.day.unique():\n",
    "    plt.figure(figsize=(20,5))\n",
    "    df = stress_data[stress_data.day==day].sort_values('timestamp').reset_index(drop=True)\n",
    "    df['stress_likelihood'] = df['stress_likelihood'].rolling(window=72).mean()\n",
    "    plt.plot(df['localtime'],df['stress_likelihood'],'--*')\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "# stress_data[stress_data.day=='20180913'].shape\n",
    "stress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: double (nullable = true)\n",
      " |-- localtime: double (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- activity: double (nullable = true)\n",
      " |-- stress_likelihood: double (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- version: integer (nullable = false)\n",
      " |-- start: double (nullable = true)\n",
      " |-- end: double (nullable = true)\n",
      " |-- likelihood_mean: integer (nullable = false)\n",
      "\n",
      "+-----------------------+-----------------------+-------+------------------------------------+--------+--------------------+-------+---------------+------------------------------------------+\n",
      "|timestamp              |localtime              |version|user                                |day     |stress_likelihood   |imputed|likelihood_mean|window                                    |\n",
      "+-----------------------+-----------------------+-------+------------------------------------+--------+--------------------+-------+---------------+------------------------------------------+\n",
      "|2017-06-29 09:46:15.945|2017-06-29 03:46:15.945|1      |0dd6b3c7-0314-3e6d-ac60-dde239068a6c|20170629|0.01483406462046723 |0      |1.0            |[2017-06-29 09:46:15, 2017-06-29 09:47:15]|\n",
      "|2017-06-29 09:46:22.103|2017-06-29 03:46:22.103|1      |0dd6b3c7-0314-3e6d-ac60-dde239068a6c|20170629|0.06688838927803237 |0      |1.0            |[2017-06-29 09:46:20, 2017-06-29 09:47:20]|\n",
      "|2017-06-29 09:46:25.729|2017-06-29 03:46:25.729|1      |0dd6b3c7-0314-3e6d-ac60-dde239068a6c|20170629|0.03702018056890881 |0      |1.0            |[2017-06-29 09:46:25, 2017-06-29 09:47:25]|\n",
      "|2017-06-29 09:46:30.002|2017-06-29 03:46:30.002|1      |0dd6b3c7-0314-3e6d-ac60-dde239068a6c|20170629|0.02571089091116567 |0      |1.0            |[2017-06-29 09:46:30, 2017-06-29 09:47:30]|\n",
      "|2017-06-29 09:46:35.403|2017-06-29 03:46:35.403|1      |0dd6b3c7-0314-3e6d-ac60-dde239068a6c|20170629|0.015993411321749734|0      |1.0            |[2017-06-29 09:46:35, 2017-06-29 09:47:35]|\n",
      "+-----------------------+-----------------------+-------+------------------------------------+--------+--------------------+-------+---------------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress_data = CC.get_stream('org.md2k.autosense.ecg.rip.stress.likelihood')\n",
    "stress_data = stress_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "stress_data = stress_data.withColumn('start',F.col('window').start)\n",
    "stress_data = stress_data.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "stress_data = stress_data.withColumn('start',F.col('start').cast('double'))\n",
    "stress_data = stress_data.withColumn('end',F.col('end').cast('double'))\n",
    "stress_data = stress_data.withColumn('localtime',F.col('localtime').cast('double'))\n",
    "stress_data = stress_data.withColumn('timestamp',F.col('timestamp').cast('double'))\n",
    "stress_data  = stress_data.withColumn('likelihood_mean',F.lit(1))\n",
    "stress_data.printSchema()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", DoubleType()),\n",
    "    StructField(\"start\", DoubleType()),\n",
    "    StructField(\"end\", DoubleType()),\n",
    "    StructField(\"localtime\", DoubleType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"stress_likelihood\", DoubleType()),\n",
    "    StructField(\"imputed\", IntegerType()),\n",
    "    StructField(\"likelihood_mean\", DoubleType())\n",
    "])\n",
    "step = 5\n",
    "smoothing = int(60*2/step)\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def impute_forwardfill(data):\n",
    "    data = data[data.activity==0]\n",
    "    data = data.sort_values('start').reset_index(drop=True)\n",
    "#     data['stress_likelihood'] = data['stress_likelihood'].rolling(window=20).mean()\n",
    "    data = data.dropna().sort_values('start').reset_index(drop=True)\n",
    "    if data.shape[0]<.8*60*60/5:\n",
    "        return pd.DataFrame([],columns=['timestamp','localtime','start','end',\n",
    "                                          'version','user','day','stress_likelihood','likelihood_mean','imputed'])\n",
    "    start = data['start'][0]\n",
    "    all_rows = []\n",
    "    median_stress = np.median(data['stress_likelihood'].values)\n",
    "    for i,row in data.iterrows():\n",
    "        if row['start']==start:\n",
    "            all_rows.append([row['timestamp'],row['localtime'],row['start'],row['end'],\n",
    "                             row['version'],row['user'],row['day'],row['stress_likelihood'],\n",
    "                             row['likelihood_mean'],0])\n",
    "            start = row['end']\n",
    "        else:\n",
    "            k = 1\n",
    "            while (start+k*step)<=row['start']:\n",
    "                all_rows.append([data.loc[i-1]['timestamp']+k*step,data.loc[i-1]['localtime']+k*step,\n",
    "                                 data.loc[i-1]['start']+k*step,data.loc[i-1]['end']+k*step,\n",
    "                                 row['version'],row['user'],row['day'],median_stress,data.loc[i-1]['likelihood_mean'],1])\n",
    "                k+=1\n",
    "            all_rows.append([row['timestamp'],row['localtime'],row['start'],row['end'],\n",
    "                             row['version'],row['user'],row['day'],row['stress_likelihood'],\n",
    "                             row['likelihood_mean'],0])\n",
    "            start = row['end']    \n",
    "    return pd.DataFrame(all_rows,columns=['timestamp','localtime','start','end',\n",
    "                                          'version','user','day','stress_likelihood','likelihood_mean','imputed'])\n",
    "\n",
    "stress_imputed_data = stress_data.groupBy(['user','day']).apply(impute_forwardfill)\n",
    "stress_imputed_data = stress_imputed_data.withColumn('start',F.col('start').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('end',F.col('end').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('localtime',F.col('localtime').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('timestamp',F.col('timestamp').cast('timestamp'))\n",
    "cols = list(stress_imputed_data.columns)\n",
    "cols.append(F.struct('start', 'end').alias('window'))\n",
    "cols.remove('start')\n",
    "cols.remove('end')\n",
    "stress_imputed_data = stress_imputed_data.select(*cols)\n",
    "stress_imputed_data.show(5,False)\n",
    "schema = stress_imputed_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.autosense.ecg.rip.stress.likelihood.imputed\").set_description(\"rip stress imputed\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"stress forward fill imputed\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=stress_imputed_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn import ensemble\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,GridSearchCV,KFold,train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "def best_fit_slope(ys):\n",
    "    return np.mean(np.diff(ys))\n",
    "\n",
    "def get_features(y):\n",
    "    tmp = y[-1,0]\n",
    "    return [tmp,\n",
    "#             np.median(y),\n",
    "#             np.std(y),\n",
    "#             np.percentile(y,80),\n",
    "#             np.percentile(y,20),\n",
    "            best_fit_slope(y[:,0])]\n",
    "\n",
    "def get_trained_model(X_train,y_train):\n",
    "    paramGrid = {'rf__n_neighbors':[3,4,5,6,7,8,9],\n",
    "                 }\n",
    "    clf = Pipeline([('rf',KNeighborsRegressor())])\n",
    "    gkf = KFold(n_splits=5)\n",
    "    grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=gkf.split(X_train),\n",
    "                               scoring='r2',verbose=5)\n",
    "    grid_search.fit(X_train,y_train)\n",
    "    clf = grid_search.best_estimator_\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf\n",
    "\n",
    "weekday_dict = {'Wednesday':5,\n",
    "                'Saturday':1,\n",
    "                'Thursday':6,\n",
    "                'Tuesday':4,\n",
    "                'Friday':0,\n",
    "                'Sunday':2,\n",
    "                'Monday':3}\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", DoubleType()),\n",
    "    StructField(\"start\", DoubleType()),\n",
    "    StructField(\"end\", DoubleType()),\n",
    "    StructField(\"localtime\", DoubleType()),\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"weekday\", StringType()),\n",
    "    StructField(\"hour\", IntegerType()),\n",
    "    StructField(\"stress_probability\", DoubleType()),\n",
    "    StructField(\"imputed\", IntegerType()),\n",
    "])\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def fillup_imputation(data):\n",
    "    data = data.sort_values('start').reset_index(drop=True)\n",
    "#     data = data.dropna().sort_values('start').reset_index(drop=True)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i,row in data.iterrows():\n",
    "        if i<lookback:\n",
    "            continue\n",
    "        if row['imputed'] in [1]:\n",
    "            continue\n",
    "        prev_imputed = list(data['imputed'].iloc[i-lookback:i].values)\n",
    "        if prev_imputed.count(1)<=.33*lookback:\n",
    "            feature = data[['stress_probability','imputed','timestamp']].iloc[i-lookback:i]\n",
    "#             .sort_values('timestamp').reset_index(drop=True)\n",
    "            feature = feature[feature.imputed.isin([0,2])][['stress_probability','timestamp']].values\n",
    "            X.append(np.array(get_features(feature)+[row['hour'],weekday_dict[row['weekday']]]))\n",
    "            y.append(row['stress_probability'])\n",
    "    if len(X)<100:\n",
    "        return data\n",
    "    X = np.array(X)\n",
    "    X_s = X[:,:-2]\n",
    "    X_hour = X[:,-2:-1].reshape(-1,1)\n",
    "    X_weekday = X[:,-1:].reshape(-1,1)\n",
    "    clf_hour = OneHotEncoder().fit(np.arange(0,25,1).reshape(-1,1))\n",
    "    clf_week_day = OneHotEncoder().fit(np.arange(0,7,1).reshape(-1,1))\n",
    "    X_hour = clf_hour.transform(X_hour).todense().reshape(X.shape[0],-1)\n",
    "    X_weekday = clf_week_day.transform(X_weekday).todense().reshape(X.shape[0],-1)\n",
    "    X = np.concatenate([X_s,X_hour,X_weekday],axis=1)\n",
    "    y = np.array(y)\n",
    "    print(X.shape)\n",
    "    clf = get_trained_model(X,y)\n",
    "    start = data[data.imputed==1].shape[0]+1\n",
    "#     return data\n",
    "    count = 0\n",
    "    while data[data.imputed==1].shape[0]<start and data[data.imputed==1].shape[0]>0:\n",
    "        start = data[data.imputed==1].shape[0]\n",
    "        print(start,count)\n",
    "        X = []\n",
    "        y = []\n",
    "        index = []\n",
    "        for i,row in data.iterrows():\n",
    "            if i<lookback:\n",
    "                continue\n",
    "            if row['imputed'] in [0,2]:\n",
    "                continue\n",
    "            prev_imputed = list(data['imputed'].iloc[i-lookback:i].values)\n",
    "            if prev_imputed.count(1)<=.33*lookback:\n",
    "                feature = data[['stress_probability','imputed','timestamp']].iloc[i-lookback:i]\n",
    "#                 .sort_values('timestamp').reset_index(drop=True)\n",
    "                feature = feature[feature.imputed.isin([0,2])][['stress_probability','timestamp']].values\n",
    "                X.append(np.array(get_features(feature)+[row['hour'],weekday_dict[row['weekday']]]))\n",
    "                y.append(row['stress_probability'])\n",
    "                index.append(i)\n",
    "        count+=1\n",
    "        if len(X)==0:\n",
    "            break\n",
    "        X = np.array(X)\n",
    "        X_s = X[:,:-2]\n",
    "        X_hour = X[:,-2:-1].reshape(-1,1)\n",
    "        X_weekday = X[:,-1:].reshape(-1,1)\n",
    "        X_hour = clf_hour.transform(X_hour).todense().reshape(X.shape[0],-1)\n",
    "        X_weekday = clf_week_day.transform(X_weekday).todense().reshape(X.shape[0],-1)\n",
    "        X = np.concatenate([X_s,X_hour,X_weekday],axis=1)\n",
    "#         break\n",
    "        data.loc[index,'stress_probability'] = clf.predict(X).reshape(-1)\n",
    "        data.loc[index,'imputed'] = 2\n",
    "    return data\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "# def fillup_imputation_simple(data):\n",
    "# #     data = data.sort_values('start').reset_index(drop=True)\n",
    "# #     data['stress_probability'] = data['stress_probability'].rolling(window=lookback).mean()\n",
    "#     data = data.dropna().sort_values('start').reset_index(drop=True)\n",
    "#     if data.shape[0]<1000:\n",
    "#         return data\n",
    "#     Xs = []\n",
    "#     Xweekday = []\n",
    "#     Xhour = []\n",
    "#     index = []\n",
    "#     for i,row in data.iterrows():\n",
    "#         if row['imputed'] in [1]:\n",
    "#             Xs.append(np.nan)\n",
    "#             index.append(i)\n",
    "#         else:\n",
    "#             Xs.append(row['stress_probability'])\n",
    "#         Xweekday.append(weekday_dict[row['weekday']])\n",
    "#         Xhour.append(row['hour'])\n",
    "#     X_s = np.array(Xs).reshape(-1,1)\n",
    "#     clf_hour = OneHotEncoder().fit(np.arange(0,25,1).reshape(-1,1))\n",
    "#     clf_week_day = OneHotEncoder().fit(np.arange(0,7,1).reshape(-1,1))\n",
    "#     X_hour = clf_hour.transform(np.array(Xhour).reshape(-1,1)).todense().reshape(X_s.shape[0],-1)\n",
    "#     X_weekday = clf_week_day.transform(np.array(Xweekday).reshape(-1,1)).todense().reshape(X_s.shape[0],-1)\n",
    "#     X = np.concatenate([X_hour,X_weekday,X_s],axis=1)\n",
    "#     imputer = KNNImputer(n_neighbors=10)\n",
    "#     X = imputer.fit_transform(X)[np.array(index)]\n",
    "#     data.loc[index,'stress_probability'] = X[:,-1]\n",
    "#     data.loc[index,'imputed'] = 2\n",
    "#     return data\n",
    "\n",
    "step = 5\n",
    "lookback = int(3*(60/5))\n",
    "stream_name = \"org.md2k.autosense.ecg.rip.stress.likelihood.imputed\"\n",
    "stress_data = CC.get_stream(stream_name)\n",
    "stress_data = stress_data.withColumnRenamed('stress_likelihood','stress_probability').drop('likelihood_mean')\n",
    "# stress_data = stress_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "stress_data = stress_data.withColumn('start',F.col('window').start)\n",
    "stress_data = stress_data.withColumn('end',F.col('window').end).drop(*['window'])\n",
    "stress_data = stress_data.withColumn('start',F.col('start').cast('double'))\n",
    "stress_data = stress_data.withColumn('end',F.col('end').cast('double'))\n",
    "stress_data = stress_data.withColumn('hour',F.hour('localtime'))\n",
    "stress_data = stress_data.withColumn('weekday',F.date_format('localtime','EEEE'))\n",
    "stress_data = stress_data.withColumn('localtime',F.col('localtime').cast('double'))\n",
    "stress_data = stress_data.withColumn('timestamp',F.col('timestamp').cast('double'))\n",
    "stress_data.show(5,False)\n",
    "# data_1 = stress_data._data.toPandas()\n",
    "stress_imputed_data = stress_data.groupBy(['user','version']).apply(fillup_imputation)\n",
    "stress_imputed_data = stress_imputed_data.withColumn('start',F.col('start').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('end',F.col('end').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('localtime',F.col('localtime').cast('timestamp'))\n",
    "stress_imputed_data = stress_imputed_data.withColumn('timestamp',F.col('timestamp').cast('timestamp'))\n",
    "cols = list(stress_imputed_data.columns)\n",
    "cols.append(F.struct('start', 'end').alias('window'))\n",
    "cols.remove('start')\n",
    "cols.remove('end')\n",
    "cols.remove('hour')\n",
    "# cols.remove('weekday')\n",
    "# cols.remove('day')\n",
    "stress_imputed_data = stress_imputed_data.select(*cols)\n",
    "stress_imputed_data = stress_imputed_data.withColumn('localtime_str',F.col('localtime').cast('string'))\n",
    "stress_imputed_data.show(10,False) \n",
    "schema = stress_imputed_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.autosense.ecg.rip.stress.likelihood.imputed.all\").set_description(\"stress imputed\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"stress imputed in KNN\") \\\n",
    "    .set_attribute(\"url\", \"hhtps://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=stress_imputed_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)\n",
    "# stream_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream(\"org.md2k.autosense.ecg.rip.stress.likelihood.imputed\")\n",
    "data = data.toPandas()\n",
    "import pickle\n",
    "pickle.dump(data,open('./data/rice_1st_version_ecg_rip_imputed_all.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pickle.load(open('./data/rice_1st_version_ecg_rip_imputed.p','rb'))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.hist(data['stress_likelihood'][data['imputed']==0],200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.user.unique()[:10],data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_data = data[data.user=='cd55ae15-ee0e-3527-b7f5-d5a4897d5b8e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for day in stress_data.day.unique():\n",
    "    plt.figure(figsize=(20,10))\n",
    "    df = stress_data[stress_data.day==day].sort_values('timestamp').reset_index(drop=True)\n",
    "    df['stress_likelihood'] = df['stress_likelihood'].rolling(window=72).mean()\n",
    "    plt.plot(df['localtime'],df['stress_likelihood'],'--*')\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stress_data[stress_data.day=='20180913'].shape\n",
    "# stress_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macd_param_fast = 7*12\n",
    "macd_param_slow = 19*12\n",
    "macd_param_signal = 2*12\n",
    "smoothing_window = 3*12 # FIXME - 3 minutes\n",
    "threshold_stressed = 0.36\n",
    "threshold_not_stressed= 0.36\n",
    "NOTSTRESS = \"NOTSTRESSED\"\n",
    "UNSURE = 'UNSURE'\n",
    "YESSTRESS = 'STRESSED'\n",
    "UNKNOWN = 'UNKNOWN'\n",
    "UNCLASSIFIED = 'UNCLASSIFIED'\n",
    "stream_name = \"org.md2k.autosense.ecg.stress_episodes\"\n",
    "\n",
    "def stress_episodes_estimation(stress_data: object) -> object:\n",
    "    stress_data = stress_data.sort_values('timestamp').reset_index(drop=True)\n",
    "    \"\"\"\n",
    "    smooth stress probabilities and use MACD to label stress episodes\n",
    "    Args:\n",
    "        stress_data (pandas.df):\n",
    "    Returns:\n",
    "        pandas.df\n",
    "    \"\"\"\n",
    "    \n",
    "    user_id = stress_data.iloc[0].user\n",
    "    # Smooth the stress values\n",
    "    stress_smoothed_list = []\n",
    "\n",
    "    for indx in range(len(stress_data['stress_probability'].values)):\n",
    "        if indx<2:\n",
    "            smoothed_stress = stress_data['stress_probability'].values[indx]\n",
    "        else:\n",
    "            smoothed_stress = (stress_data['stress_probability'].values[indx] + \\\n",
    "                             stress_data['stress_probability'].values[indx-1] + \\\n",
    "                             stress_data['stress_probability'].values[indx-2]) / smoothing_window\n",
    "\n",
    "        stress_smoothed_list.append((stress_data['timestamp'].values[indx], smoothed_stress, stress_data['localtime'].values[indx]))\n",
    "\n",
    "    # EMA = Exponential Moving Average\n",
    "    ema_fast_list = []\n",
    "    ema_fast_list.append(stress_smoothed_list[0])\n",
    "    ema_slow_list = []\n",
    "    ema_slow_list.append(stress_smoothed_list[0])\n",
    "    ema_signal_list = []\n",
    "    ema_signal_list.append((0,0))\n",
    "    histogram_list = []\n",
    "\n",
    "    stress_episode_start = []\n",
    "    stress_episode_peak = []\n",
    "    stress_episode_classification = []\n",
    "    stress_episode_intervals = []\n",
    "\n",
    "    for indx in range(len(stress_smoothed_list)):\n",
    "        if indx%100==0:\n",
    "            print(indx)\n",
    "        ema_fast_prev = ema_fast_list[-1][1]\n",
    "        ema_fast_current = stress_smoothed_list[indx][1]\n",
    "        ema_fast = ewma(ema_fast_current, ema_fast_prev, 2.0/(macd_param_fast + 1))\n",
    "        ema_fast_list.append((stress_smoothed_list[indx][0], ema_fast))\n",
    "\n",
    "        ema_slow_prev = ema_slow_list[-1][1]\n",
    "        ema_slow_current = stress_smoothed_list[indx][1]\n",
    "        ema_slow = ewma(ema_slow_current, ema_slow_prev, 2.0/(macd_param_slow + 1))\n",
    "        ema_slow_list.append((stress_smoothed_list[indx][0], ema_slow))\n",
    "\n",
    "        macd_prev = ema_fast_prev - ema_slow_prev\n",
    "        macd_current = ema_fast_current - ema_slow_current\n",
    "        ema_signal_prev = ema_signal_list[-1][1]\n",
    "        ema_signal = ewma(macd_current, macd_prev, 2.0/(macd_param_signal + 1))\n",
    "        ema_signal_list.append((stress_smoothed_list[indx][0], ema_signal))\n",
    "\n",
    "        histogram_prev =  macd_prev - ema_signal_prev\n",
    "        histogram = macd_current - ema_signal\n",
    "        histogram_list.append((stress_smoothed_list[indx][0], histogram))\n",
    "\n",
    "        if histogram_prev <=0 and histogram > 0:\n",
    "            # Episode has ended, started increasing again\n",
    "            start_timestamp = None;\n",
    "            peak_timestamp = None;\n",
    "            end_timestamp = stress_smoothed_list[indx][0]\n",
    "            stress_class = None\n",
    "            if len(stress_episode_start):\n",
    "                start_timestamp = stress_episode_start[-1][0]\n",
    "\n",
    "            if len(stress_episode_classification):\n",
    "                peak_timestamp = stress_episode_classification[-1][0]\n",
    "                stress_class = stress_episode_classification[-1][1]\n",
    "\n",
    "            if stress_class:\n",
    "                #TODO - Handle this?????\n",
    "                stress_episode_timestamps = []\n",
    "                stress_episode_timestamps.append(start_timestamp)\n",
    "                stress_episode_timestamps.append(peak_timestamp)\n",
    "                stress_episode_timestamps.append(end_timestamp)\n",
    "                stress_episode_timestamps.append(stress_class)\n",
    "                stress_episode_intervals.append(stress_episode_timestamps)\n",
    "\n",
    "        if histogram_prev >=0 and histogram < 0:\n",
    "            # Episode is in the middle, started decreasing\n",
    "            episode_start_timestamp = get_episode_start_timestamp(stress_episode_classification, histogram_list, stress_smoothed_list[indx][0])\n",
    "            if episode_start_timestamp is None:\n",
    "                stress_episode_start.append((episode_start_timestamp, UNCLASSIFIED))\n",
    "                stress_episode_peak.append((stress_smoothed_list[indx][0], UNCLASSIFIED))\n",
    "                stress_episode_classification.append([stress_smoothed_list[indx][0], stress_smoothed_list[indx][2], stress_smoothed_list[indx][1], UNCLASSIFIED])\n",
    "            else:\n",
    "                proportion_available = get_proportion_available(stress_data, episode_start_timestamp, stress_smoothed_list[indx][0])\n",
    "                if proportion_available < 0.5:\n",
    "                    stress_episode_start.append((episode_start_timestamp, UNKNOWN))\n",
    "                    stress_episode_peak.append((stress_smoothed_list[indx][0], UNKNOWN))\n",
    "                    stress_episode_classification.append([stress_smoothed_list[indx][0], stress_smoothed_list[indx][2], stress_smoothed_list[indx][1], UNKNOWN])\n",
    "                else:\n",
    "                    historical_stress = get_historical_values_timestamp_based(stress_smoothed_list, episode_start_timestamp, stress_smoothed_list[indx][0])\n",
    "                    if not len(historical_stress):\n",
    "                        stress_episode_start.append((episode_start_timestamp, UNKNOWN))\n",
    "                        stress_episode_peak.append((stress_smoothed_list[indx][0], UNKNOWN))\n",
    "                        stress_episode_classification.append([stress_smoothed_list[indx][0], stress_smoothed_list[indx][2], stress_smoothed_list[indx][1], UNKNOWN])\n",
    "                    else:\n",
    "                        cumu_sum = 0.0\n",
    "                        for hs in historical_stress:\n",
    "                            cumu_sum += hs[1]\n",
    "                        stress_density = cumu_sum / len(historical_stress)\n",
    "                        if stress_density >= threshold_stressed:\n",
    "                            stress_episode_start.append((episode_start_timestamp, YESSTRESS))\n",
    "                            stress_episode_peak.append((stress_smoothed_list[indx][0], YESSTRESS))\n",
    "                            stress_episode_classification.append([stress_smoothed_list[indx][0], stress_smoothed_list[indx][2], stress_smoothed_list[indx][1], YESSTRESS])\n",
    "                        elif stress_density <= threshold_not_stressed:\n",
    "                            stress_episode_start.append((episode_start_timestamp, NOTSTRESS))\n",
    "                            stress_episode_peak.append((stress_smoothed_list[indx][0], NOTSTRESS))\n",
    "                            stress_episode_classification.append([stress_smoothed_list[indx][0], stress_smoothed_list[indx][2], stress_smoothed_list[indx][1], NOTSTRESS])\n",
    "                        else:\n",
    "                            stress_episode_start.append((episode_start_timestamp, UNSURE))\n",
    "                            stress_episode_peak.append((stress_smoothed_list[indx][0], UNSURE))\n",
    "                            stress_episode_classification.append([stress_smoothed_list[indx][0], stress_smoothed_list[indx][2], stress_smoothed_list[indx][1], UNSURE])\n",
    "\n",
    "    df = pd.DataFrame(stress_episode_classification, columns=['timestamp', 'localtime', 'stress_probability', 'stress_episode'])\n",
    "    df['user'] = user_id\n",
    "    df['version'] = 1\n",
    "    return df\n",
    "\n",
    "def ewma(current:int, previous:int, alpha:int)->float:\n",
    "    \"\"\"\n",
    "    compute exponential weighted moving average\n",
    "    Args:\n",
    "        current (int): current stress probability value\n",
    "        previous (int): previous stress probability value\n",
    "        alpha (int):\n",
    "    Returns:\n",
    "        float\n",
    "    \"\"\"\n",
    "    return alpha * current + (1 - alpha) * previous\n",
    "\n",
    "def get_episode_start_timestamp(stress_episode_classification, histogram_list, currenttime):\n",
    "    \"\"\"\n",
    "    Get start time of a stress episode\n",
    "    Args:\n",
    "        stress_episode_classification:\n",
    "        histogram_list:\n",
    "        currenttime:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    timestamp_prev = None\n",
    "    if len(stress_episode_classification) >= 3:\n",
    "        timestamp_prev = stress_episode_classification[-3][0]\n",
    "    elif len(stress_episode_classification) == 2:\n",
    "        timestamp_prev = stress_episode_classification[-2][0]\n",
    "    elif len(stress_episode_classification) == 1:\n",
    "        timestamp_prev = stress_episode_classification[-1][0]\n",
    "\n",
    "    histogram_history =  get_historical_values_timestamp_based(histogram_list, timestamp_prev, currenttime)\n",
    "\n",
    "    if len(histogram_history) <= 1:\n",
    "        return None\n",
    "\n",
    "    for x in range(len(histogram_history)-2 , -1, -1):\n",
    "        if histogram_history[x][1] <= 0:\n",
    "            return histogram_history[x+1][0]\n",
    "\n",
    "    return histogram_history[0][0]\n",
    "\n",
    "\n",
    "def get_historical_values_timestamp_based(data, start_timestamp, currenttime):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data:\n",
    "        start_timestamp:\n",
    "        currenttime:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    toreturn = []\n",
    "    starttime = start_timestamp\n",
    "    if starttime == None:\n",
    "        starttime = currenttime - np.timedelta64(100*365*24*3600, 's') # approx 100 year to approximate -1\n",
    "    for c in data:\n",
    "        if c[0] >= starttime and c[0] <= currenttime:\n",
    "            toreturn.append(c)\n",
    "        if c[0] > currenttime:\n",
    "            break\n",
    "\n",
    "    return toreturn\n",
    "\n",
    "\n",
    "def get_proportion_available(data, st, current_timestamp):\n",
    "    \"\"\"\n",
    "    compute the ratio of (detected + imputed stress episodes)/detected stress episodes\n",
    "    Args:\n",
    "        data:\n",
    "        st:\n",
    "        current_timestamp:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    available = 0\n",
    "    start_timestamp = st\n",
    "    if start_timestamp is None:\n",
    "        start_timestamp = current_timestamp\n",
    "    for x in range(len(data)):\n",
    "        row_time = data.iloc[x]['timestamp']\n",
    "        if row_time >= start_timestamp and row_time <= current_timestamp:\n",
    "            available += data.iloc[x]['imputed']\n",
    "            count +=1\n",
    "        if row_time > current_timestamp:\n",
    "            break\n",
    "\n",
    "    if count:\n",
    "        return available/count\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open('./data/rice_1st_version_ecg_rip_imputed.p','rb'))\n",
    "\n",
    "data['stress_probability'] = data['stress_likelihood']\n",
    "\n",
    "ecg_stress_probability = data[(data.user=='cd55ae15-ee0e-3527-b7f5-d5a4897d5b8e')&(data.day=='20180419')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_stress_probability.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ecg_stress_probability.groupby(['user','day','version'],as_index=False).apply(stress_episodes_estimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.vlines(data[data.stress_probability>.01]['timestamp'],0,1,'k')\n",
    "plt.plot(ecg_stress_probability['timestamp'],ecg_stress_probability['stress_probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
